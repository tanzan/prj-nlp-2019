{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Завантажуємо спочатку тестові данні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/vseloved/prj-nlp-2019/master/tasks/07-language-as-sequence/run-on-test.json --output run-on-test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('run-on-test.json') as f:\n",
    "    run_on_js = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набираємо тренувальних данних з с корпусів NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import abc\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "\n",
    "source = []\n",
    "\n",
    "for fileid in reuters.fileids():\n",
    "    source.append(reuters.sents(fileid))\n",
    "    \n",
    "for fileid in gutenberg.fileids():\n",
    "    source.append(gutenberg.sents(fileid))    \n",
    "    \n",
    "for fileid in brown.fileids():\n",
    "    source.append(brown.sents(fileid))    \n",
    "    \n",
    "for fileid in abc.fileids():\n",
    "    source.append(abc.sents(fileid))        \n",
    "    \n",
    "for fileid in treebank.fileids():\n",
    "    source.append(treebank.sents(fileid))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі генеруємо фейкові run-on речення на основі зібраного корпусу і розставляємо теги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable = ['ner','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "# визначає випадковим способом скільки реченнь склеювати, \n",
    "# пропорції приблизні як у наданних вище тестових данних\n",
    "def rand_run_on_num():\n",
    "    x = random.random()\n",
    "    if x < 0.25:\n",
    "        return 0\n",
    "    elif 0.25 <= x < 0.97:\n",
    "        return 1\n",
    "    \n",
    "    return 2\n",
    "\n",
    "Token = namedtuple('Token', 'text point_after pos tag lemma dep token_')\n",
    "\n",
    "random.seed(273757)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "def tag_tokens(tokens):   \n",
    "    \n",
    "    doc = nlp(' '.join(tokens))\n",
    "    \n",
    "    return [Token(token.text, False, token.pos_, token.tag_, token.lemma_, token.dep_, token) for token in doc]\n",
    "\n",
    "\n",
    "for sents in source:        \n",
    "    \n",
    "    run_on_num = rand_run_on_num()\n",
    "    buff = []\n",
    "    prev_offset = 0   \n",
    "    \n",
    "    for sent in sents[1:]:\n",
    "        if len(sent) < 5:\n",
    "            continue\n",
    "\n",
    "        exclude = False\n",
    "        for word in sent:\n",
    "            if word == '?' or word == ';' or word == '...':\n",
    "                exclude = True\n",
    "                break\n",
    "        if exclude:\n",
    "            continue\n",
    "\n",
    "        tokens = tag_tokens(sent)\n",
    "        buff.extend(tokens)          \n",
    "\n",
    "        if prev_offset > 0:                \n",
    "            if buff[prev_offset - 1].text == '.':\n",
    "                del buff[prev_offset - 1]\n",
    "                buff[prev_offset - 2] = buff[prev_offset - 2]._replace(point_after = True)                \n",
    "\n",
    "                if not buff[prev_offset - 1].text.isupper():                \n",
    "                    lwr = random.random()                \n",
    "                    if lwr < 0.93: #псуємо сase\n",
    "                        buff[prev_offset - 1] = buff[prev_offset -1].\\\n",
    "                        _replace(text = buff[prev_offset - 1].text.lower())                                            \n",
    "\n",
    "        prev_offset = len(buff)            \n",
    "\n",
    "        if run_on_num == 0:            \n",
    "            corpus.append(buff)\n",
    "            run_on_num = rand_run_on_num()\n",
    "            prev_offset = 0\n",
    "            buff = []                               \n",
    "\n",
    "        run_on_num -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18701"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемішуємо корпус та розділяємо на тренувальні і тестові датасети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    random.shuffle(corpus)\n",
    "\n",
    "train_index = int(0.7 * len(corpus))\n",
    "\n",
    "train_data = corpus[: train_index]\n",
    "test_data = corpus[train_index: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збераємо біграми та триграми на тестових данних, які також включають крапки на кінці речення. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = defaultdict(float)\n",
    "trigrams = defaultdict(float)\n",
    "\n",
    "all_tokens = [token for sent in train_data for token in map(lambda x: x.text.lower(),sent)]\n",
    "\n",
    "for g in nltk.ngrams(all_tokens, 2):        \n",
    "    bigrams[g] += 1.0\n",
    "                \n",
    "for g in nltk.ngrams(all_tokens, 3):    \n",
    "    trigrams[g] += 1.0\n",
    "\n",
    "all_tokens = []\n",
    "\n",
    "def update_freq(ngrams):\n",
    "    n = sum(ngrams.values())\n",
    "    for k, v in ngrams.items():\n",
    "        ngrams[k] = v/n \n",
    "\n",
    "update_freq(bigrams)\n",
    "update_freq(trigrams)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', 'the'), 0.00598490423501983),\n",
       " (('of', 'the'), 0.0055116691522362525),\n",
       " ((\"'\", 's'), 0.00521861072941156),\n",
       " (('in', 'the'), 0.004758400465420191),\n",
       " (('said', '.'), 0.003377769673446085),\n",
       " ((',', 'the'), 0.003186738997827026),\n",
       " (('said', 'the'), 0.0029913667159438976),\n",
       " ((',', '000'), 0.0029544630626993067),\n",
       " (('mln', 'dlrs'), 0.0028524353154936732),\n",
       " (('u', '.'), 0.0026852834743269966),\n",
       " (('the', 'company'), 0.002631013396026128),\n",
       " (('.', '\"'), 0.002572401711461189),\n",
       " (('.', 's'), 0.002500765208104042),\n",
       " (('s', '.'), 0.002405249870294513),\n",
       " ((',', '\"'), 0.002403079067162478),\n",
       " (('for', 'the'), 0.0021121914474698206),\n",
       " (('to', 'the'), 0.001901623543662449),\n",
       " ((',', 'and'), 0.0018191330246451279),\n",
       " (('said', 'it'), 0.0017995957964568152),\n",
       " (('1', '.'), 0.0017995957964568152)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigrams.items(), key=lambda kv: kv[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('u', '.', 's'), 0.0024942582132514795),\n",
       " (('.', 's', '.'), 0.0023640097425856057),\n",
       " (('the', 'company', 'said'), 0.0011678946203039998),\n",
       " (('the', 'u', '.'), 0.0011244784634153755),\n",
       " (('.', 'the', 'company'), 0.0008704939456169219),\n",
       " (('said', '.', 'the'), 0.0008031989024395538),\n",
       " ((',', '000', 'dlrs'), 0.0007098541651290112),\n",
       " ((',', '000', 'tonnes'), 0.0006664380082403866),\n",
       " (('he', 'said', '.'), 0.0006382175062627807),\n",
       " (('.', 'it', 'said'), 0.0006186802356628996),\n",
       " ((',', 'it', 'said'), 0.0005839473101519999),\n",
       " ((',', '\"', 'he'), 0.0005665808473965502),\n",
       " ((',', 'he', 'said'), 0.0004927733806858885),\n",
       " (('mln', 'dlrs', 'in'), 0.0004819193414637323),\n",
       " (('mln', 'dlrs', ','), 0.0004797485336193011),\n",
       " (('.', '\"', 'the'), 0.0004558696473305576),\n",
       " ((',', 'the', 'company'), 0.0004493572237972639),\n",
       " (('.', '5', 'pct'), 0.00042547833750852045),\n",
       " (('\"', 'he', 'said'), 0.00041896591397522676),\n",
       " (('it', 'said', '.'), 0.0004059410669086394)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(trigrams.items(), key=lambda kv: kv[1], reverse = True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Намагаємось побудувати простий безлайн на правилах, та перевіямо якість на тестових датасетах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def baseline(data):\n",
    "    result_data = []\n",
    "    for sent in data:\n",
    "        result_sent = []\n",
    "        last_point = 0\n",
    "        for i, word in enumerate(sent):            \n",
    "            if (i - last_point) > 3 and i < (len(sent) - 1):\n",
    "                pbigram = (word[0].lower(), '.')                \n",
    "                bigram = (word[0].lower(), sent[i+1][0].lower())                                    \n",
    "                if (sent[i + 1][0][0:1].isupper()):\n",
    "                    result_sent.append([word[0], True])\n",
    "                elif math.log(bigrams[pbigram] + 0.000000000001) > math.log(bigrams[bigram] + 0.001):                        \n",
    "                    result_sent.append([word[0], True])\n",
    "                    last_point = i\n",
    "                else:                       \n",
    "                    result_sent.append([word[0], False])                                    \n",
    "            else:                \n",
    "                result_sent.append([word[0], False])\n",
    "            \n",
    "        result_data.append(result_sent)   \n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = baseline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# виділяє вектор лейблів з розміченних данних\n",
    "def labels_vec(data):\n",
    "    return [word[1] for sent in data for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.91      0.95    196267\n",
      "        True       0.02      0.18      0.03      1655\n",
      "\n",
      "   micro avg       0.91      0.91      0.91    197922\n",
      "   macro avg       0.51      0.55      0.49    197922\n",
      "weighted avg       0.98      0.91      0.94    197922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(labels_vec(test_data), labels_vec(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.96      0.97      4542\n",
      "        True       0.26      0.45      0.33       155\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      4697\n",
      "   macro avg       0.62      0.70      0.65      4697\n",
      "weighted avg       0.96      0.94      0.95      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(run_on_js), labels_vec(baseline(run_on_js))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Далі будуємо і тренуємо модель базовану на логістичній регресії, порівнюємо 2 варіанта за n-грамами та без них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def bigram_log_freq(word1, word2):\n",
    "    return math.log((0.0 if (word1 is None) or (word2 is None)\\\n",
    "                    else bigrams[(word1.lower(), word2.lower())]) + 0.000001)\n",
    "\n",
    "def trigram_log_freq(word1, word2, word3):\n",
    "    return math.log((0.0 if (word1 is None) or (word2 is None) or (word3 is None)\\\n",
    "                    else trigrams[(word1, word2, word3)]) + 0.000001)\n",
    "\n",
    "def get_prop(token, name, default):\n",
    "    return default if token is None else getattr(token, name)\n",
    "\n",
    "def extract_sent_features(sent, use_ngrams):\n",
    "    features = []\n",
    "    next1_token = sent[1] if len(sent) > 1 else None\n",
    "    next2_token = sent[2] if len(sent) > 2 else None\n",
    "    prev1_token = None\n",
    "    prev2_token = None    \n",
    "    for i, token in enumerate(sent):\n",
    "        fdic = {}                \n",
    "        fdic['next_case1'] = get_prop(next1_token, 'text', '')[:1].isupper()        \n",
    "        fdic['case'] = token.text[:1].isupper()\n",
    "        \n",
    "        fdic['tag'] = token.tag\n",
    "        fdic['tag_prev1'] = get_prop(prev1_token, 'tag', 'NONE_TAG')\n",
    "        fdic['tag_next1'] = get_prop(next1_token, 'tag', 'NONE_TAG')\n",
    "        fdic['tag_prev2'] = get_prop(prev2_token, 'tag', 'NONE_TAG')\n",
    "        fdic['tag_next2'] = get_prop(next2_token, 'tag', 'NONE_TAG')\n",
    "        \n",
    "        fdic['index1'] = len(sent)/(i + 1)\n",
    "        fdic['index2'] = len(sent)/(len(sent) - i +  1)\n",
    "        \n",
    "        fdic['word'] = token.text\n",
    "        fdic['prev_word1'] = get_prop(prev1_token, 'text', '')\n",
    "        fdic['next_word1'] = get_prop(next1_token, 'text', '')\n",
    "        fdic['prev_word2'] = get_prop(prev2_token, 'text', '')\n",
    "        fdic['next_word2'] = get_prop(next2_token, 'text', '')\n",
    "                \n",
    "        if use_ngrams:\n",
    "            fdic['bigram'] = bigram_log_freq(token.text, get_prop(next1_token, 'text', None))\n",
    "            fdic['pbigram'] = bigram_log_freq(token.text, '.')        \n",
    "            fdic['pbigram2'] = bigram_log_freq('.', get_prop(next1_token, 'text', None))        \n",
    "            fdic['ptrigram'] = trigram_log_freq(token.text, '.', get_prop(next1_token, 'text', None))         \n",
    "            fdic['ptrigram2'] = trigram_log_freq('.', get_prop(next1_token, 'text', None), get_prop(next2_token, 'text', None)) \n",
    "            fdic['ptrigram3'] = trigram_log_freq(get_prop(prev2_token, 'text', None), get_prop(prev1_token, 'text', None), '.')        \n",
    "        \n",
    "        features.append(fdic)\n",
    "        \n",
    "        prev2_token = prev1_token\n",
    "        prev1_token = token\n",
    "        \n",
    "        next1_token = next2_token\n",
    "        next2_token = sent[i+3] if (i+3) < len(sent) else None\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features(sents, use_ngrams):\n",
    "    features = []\n",
    "    for sent in sents:\n",
    "        features.extend(extract_sent_features(sent, use_ngrams))\n",
    "    return features     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "class RunOnModel:\n",
    "    def __init__(self, use_ngrams):\n",
    "        self.use_ngrams = use_ngrams\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.logreg = LogisticRegression(random_state=26, solver='liblinear', multi_class='ovr', max_iter=3000)\n",
    "        \n",
    "    def train(self, data):\n",
    "        features = extract_features(data, self.use_ngrams)\n",
    "        self.vectorizer.fit(features)\n",
    "        feature_vecs = self.vectorizer.transform(features)\n",
    "        self.logreg.fit(feature_vecs, labels_vec(data))\n",
    "        \n",
    "    def predict(self, data):\n",
    "        vec = self.vectorizer.transform(extract_features(data, self.use_ngrams))\n",
    "        return self.logreg.predict(vec)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_mod = RunOnModel(use_ngrams = False)\n",
    "simple_mod.train(train_data)\n",
    "\n",
    "ngram_mod = RunOnModel(use_ngrams = True)\n",
    "ngram_mod.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    196267\n",
      "        True       0.84      0.45      0.59      1655\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    197922\n",
      "   macro avg       0.92      0.73      0.79    197922\n",
      "weighted avg       0.99      0.99      0.99    197922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(test_data), simple_mod.predict(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00    196267\n",
      "        True       0.69      0.65      0.67      1655\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    197922\n",
      "   macro avg       0.85      0.82      0.83    197922\n",
      "weighted avg       0.99      0.99      0.99    197922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(test_data), ngram_mod.predict(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі перевіряемо натреновану модель на наданих тестових данних, але перед цим тегаємо данні та виправяемо лейбли, оскліьки токенізація може відрізнятись."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tokens(data):\n",
    "    tagged_sents = []\n",
    "    \n",
    "    for sent in data:\n",
    "        tagged_sent = [] \n",
    "        tokens = tag_tokens(list(map(lambda w: w[0], sent)))\n",
    "        compound = None\n",
    "        merged = 0\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while i < len(tokens) or j < len(sent): \n",
    "            orig_token = sent[i] if i < len(sent) else sent[len(sent) - 1]\n",
    "            token = tokens[j] if j < len(tokens) else tokens[len(tokens) - 1]           \n",
    "            if orig_token[0] == token.text:                \n",
    "                tagged_sent.append(token._replace(point_after = orig_token[1]))\n",
    "                j += 1\n",
    "                i += 1\n",
    "            elif orig_token[0] in token.text:                \n",
    "                if not compound:\n",
    "                    compound = token\n",
    "                else:\n",
    "                    compound = compound._replace(point_after = orig_token[1])\n",
    "                merged += len(orig_token[0])    \n",
    "                if merged == len(compound.text):                     \n",
    "                    j += 1\n",
    "                    tagged_sent.append(compound)\n",
    "                    compound = None                \n",
    "                    merged = 0\n",
    "                i += 1    \n",
    "            elif token.text in orig_token[0]:                \n",
    "                if not compound:\n",
    "                    compound = orig_token[0]\n",
    "                    \n",
    "                merged += len(token.text)\n",
    "                if merged == len(compound): \n",
    "                    i += 1\n",
    "                    tagged_sent.append(token._replace(point_after = orig_token[1]))                    \n",
    "                    compound = None\n",
    "                    merged = 0\n",
    "                else:\n",
    "                    tagged_sent.append(token)\n",
    "                j += 1\n",
    "            else:                              \n",
    "                assert False                                    \n",
    "        \n",
    "        tagged_sents.append(tagged_sent)\n",
    "    \n",
    "    return tagged_sents\n",
    "\n",
    "run_on_js_tokenized = convert_to_tokens(run_on_js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "print(sum([w[1] for sent in run_on_js for w in sent]))\n",
    "print(sum([t.point_after for sent in run_on_js_tokenized for t in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      1.00      0.99      4618\n",
      "        True       0.78      0.14      0.23       155\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      4773\n",
      "   macro avg       0.87      0.57      0.61      4773\n",
      "weighted avg       0.97      0.97      0.96      4773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(run_on_js_tokenized), simple_mod.predict(run_on_js_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      1.00      0.99      4618\n",
      "        True       0.74      0.37      0.50       155\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      4773\n",
      "   macro avg       0.86      0.68      0.74      4773\n",
      "weighted avg       0.97      0.98      0.97      4773\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_vec(run_on_js_tokenized), ngram_mod.predict(run_on_js_tokenized)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
