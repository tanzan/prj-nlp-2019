{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://raw.githubusercontent.com/vseloved/prj-nlp-2019/master/tasks/07-language-as-sequence/run-on-test.json --output run-on-test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('run-on-test.json') as f:\n",
    "    run_on_js = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rand_run_on_num():\n",
    "    x = random.random()\n",
    "    if x < 0.25:\n",
    "        return 0\n",
    "    elif 0.25 <= x < 0.95:\n",
    "        return 1\n",
    "    \n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import gutenberg\n",
    "#from nltk.corpus import webtext\n",
    "from nltk.corpus import abc\n",
    "from nltk.corpus import treebank\n",
    "#from nltk.corpus import dependency_treebank\n",
    "#from nltk.corpus import conll2000\n",
    "\n",
    "\n",
    "source = []\n",
    "\n",
    "for fileid in reuters.fileids():\n",
    "    source.append(reuters.sents(fileid))\n",
    "    \n",
    "#for fileid in webtext.fileids():\n",
    "#    source.append(webtext.sents(fileid))\n",
    "    \n",
    "#for fileid in gutenberg.fileids():\n",
    "#    source.append(gutenberg.sents(fileid))    \n",
    "    \n",
    "#for fileid in abc.fileids():\n",
    "#    source.append(abc.sents(fileid))       \n",
    "    \n",
    "#for fileid in brown.fileids():\n",
    "#    source.append(brown.sents(fileid)) \n",
    "    \n",
    "#for fileid in dependency_treebank.fileids():\n",
    "#    source.append(dependency_treebank.sents(fileid)) \n",
    "\n",
    "#for fileid in treebank.fileids():\n",
    "#    source.append(treebank.sents(fileid)) \n",
    "\n",
    "#for fileid in conll2000.fileids():\n",
    "#    source.append(conll2000.sents(fileid))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"ner\", 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Token = namedtuple('Token', 'text point_after pos tag lemma')\n",
    "\n",
    "def parse(sent):    \n",
    "    doc = nlp(' '.join(list(map(lambda w: w[0], sent))))\n",
    "    return [Token(token.text, False, token.pos_, token.tag_, token.lemma_) \\\n",
    "            for token in doc]\n",
    "\n",
    "def convert_to_tokens(sent):\n",
    "            \n",
    "    tagged_sent = []         \n",
    "    tokens = parse(sent)\n",
    "    compound = None\n",
    "    merged = 0\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(tokens) or j < len(sent): \n",
    "        orig_token = sent[i] if i < len(sent) else sent[len(sent) - 1]\n",
    "        token = tokens[j] if j < len(tokens) else tokens[len(tokens) - 1]           \n",
    "        if orig_token[0] == token.text:                \n",
    "            tagged_sent.append(token._replace(point_after = orig_token[1]))\n",
    "            j += 1\n",
    "            i += 1\n",
    "        elif orig_token[0] in token.text:                \n",
    "            if not compound:\n",
    "                compound = token\n",
    "            else:\n",
    "                compound = compound._replace(point_after = orig_token[1])\n",
    "            merged += len(orig_token[0])    \n",
    "            if merged == len(compound.text):                     \n",
    "                j += 1\n",
    "                tagged_sent.append(compound)\n",
    "                compound = None                \n",
    "                merged = 0\n",
    "            i += 1    \n",
    "        elif token.text in orig_token[0]:                \n",
    "            if not compound:\n",
    "                compound = orig_token[0]\n",
    "\n",
    "            merged += len(token.text)\n",
    "            if merged == len(compound): \n",
    "                i += 1\n",
    "                tagged_sent.append(token._replace(point_after = orig_token[1]))                    \n",
    "                compound = None\n",
    "                merged = 0\n",
    "            else:\n",
    "                tagged_sent.append(token)\n",
    "            j += 1\n",
    "        else:                \n",
    "            print(orig_token, token.text)\n",
    "            print(list(map(lambda w: w[0], tagged_sent)))\n",
    "            print(list(map(lambda w: w[0], sent)))\n",
    "            assert False                                    \n",
    "\n",
    "    return tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(273757)\n",
    "corpus = []\n",
    "\n",
    "def tag_tokens(tokens):\n",
    "    #doc = nlp(' '.join(tokens))\n",
    "    \n",
    "    return [[token, False] for token in tokens]\n",
    "\n",
    "with open('run-on-corpus.txt', 'w') as f_corpus:\n",
    "    for sents in source:        \n",
    "        run_on_num = rand_run_on_num()\n",
    "        buff = []\n",
    "        prev_offset = 0      \n",
    "        for sent in sents[1:]:\n",
    "            if len(sent) < 5:\n",
    "                continue\n",
    "                \n",
    "            buff.extend(tag_tokens(sent))            \n",
    "                \n",
    "            if prev_offset > 0:                \n",
    "                if buff[prev_offset - 1][0] in '.?;!':\n",
    "                    del buff[prev_offset - 1]\n",
    "                    buff[prev_offset - 2][1] = True                \n",
    "                   \n",
    "                    if not buff[prev_offset - 1][0].isupper():                \n",
    "                        lwr = random.random()                \n",
    "                        if lwr < 0.93:\n",
    "                            buff[prev_offset - 1][0] = buff[prev_offset - 1][0].lower()\n",
    "            \n",
    "            prev_offset = len(buff)            \n",
    "            \n",
    "            if run_on_num == 0:\n",
    "                print(buff, file = f_corpus, end = \"\\n\")                \n",
    "                corpus.append(convert_to_tokens(buff))\n",
    "                run_on_num = rand_run_on_num()\n",
    "                prev_offset = 0\n",
    "                buff = []                               \n",
    "                       \n",
    "            run_on_num -= 1       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = defaultdict(float)\n",
    "trigrams = defaultdict(float)\n",
    "\n",
    "for g in nltk.ngrams([w.lower() for sents in source for sent in sents for w in sent], 2):        \n",
    "    bigrams[g] += 1.0\n",
    "                \n",
    "for g in nltk.ngrams([w.lower() for sents in source for sent in sents for w in sent], 3):\n",
    "    trigrams[g] += 1.0    \n",
    "\n",
    "#for g in nltk.ngrams([w.text.lower() for sent in convert_to_tokens(run_on_js) for w in sent], 2):        \n",
    "   # bigrams[g] += 1.0\n",
    "    \n",
    "\n",
    "#for g in nltk.ngrams([w.text.lower() for sent in convert_to_tokens(run_on_js) for w in sent], 3):\n",
    "#    trigrams[g] += 1.0      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_freq(ngrams):\n",
    "    n = sum(ngrams.values())\n",
    "    for k, v in ngrams.items():\n",
    "        ngrams[k] = v/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_freq(bigrams)\n",
    "update_freq(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(bigrams.items(), key=lambda kv: kv[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(corpus)\n",
    "random.shuffle(corpus)\n",
    "random.shuffle(corpus)\n",
    "\n",
    "train_index = int(0.7 * len(corpus))\n",
    "\n",
    "train_data = corpus[: train_index]\n",
    "test_data = corpus[train_index: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def baseline(data):\n",
    "    result_data = []\n",
    "    for sent in data:\n",
    "        result_sent = []\n",
    "        last_point = 0\n",
    "        for i, word in enumerate(sent):            \n",
    "            if (i - last_point) > 3 and i < (len(sent) - 1):\n",
    "                pbigram = (word[0].lower(), '.')                \n",
    "                bigram = (word[0].lower(), sent[i+1][0].lower())                                    \n",
    "                if (sent[i + 1][0][0:1].isupper()):\n",
    "                    result_sent.append([word[0], True])\n",
    "                elif math.log(bigrams[pbigram] + 0.000000000001) > math.log(bigrams[bigram] + 0.001):    \n",
    "                    #if not word[1]:\n",
    "                         #print('false positive', pbigram, bigram, bigrams[pbigram], bigrams[bigram], word, sent[i + 1])\n",
    "                    result_sent.append([word[0], True])\n",
    "                    last_point = i\n",
    "                else:   \n",
    "                    #if word[1]:\n",
    "                    #     print('false negative', pbigram, bigram, bigrams[pbigram], bigrams[bigram], word, sent[i + 1])\n",
    "                    result_sent.append([word[0], False])                                    \n",
    "            else:\n",
    "                #if word[1]:\n",
    "                #    print('false negative', word, sent[i + 1])\n",
    "                result_sent.append([word[0], False])\n",
    "            \n",
    "        result_data.append(result_sent)   \n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = baseline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_vec(data):\n",
    "    return [word[1] for sent in data for word in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(labels_vec(test_data), labels_vec(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_vec(run_on_js), labels_vec(baseline(run_on_js))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def token_features(title, token, fdic):\n",
    "    #fdic[title + 'is_upper'] = token.text.isupper() if not (token is None) else False\n",
    "    fdic[title + 'is_title'] = token.text.istitle() if not (token is None) else False\n",
    "    #fdic[title + 'is_punct'] = token.token_.is_punct if token else False\n",
    "    fdic[title + 'tag'] = token.tag_ if not (token is None) else ''\n",
    "    fdic[title + 'word']  = token.text.lower() if not (token is None) else ''\n",
    "    #fdic[title + 'lemma']  = token.token_.lemma_ if token else ''\n",
    "    #fdic[title + 'dep'] = token.token_.dep_ if not (token is None) else ''\n",
    "    #fdic[title + 'head'] = token.token_.head.text.lower() if not (token is None) else ''\n",
    "    #fdic[title + 'head_tag'] = token.token_.head.tag_ if not (token is None) else ''\n",
    "\n",
    "\n",
    "def extract_sent_features(sent):\n",
    "    features = []\n",
    "    prev1_token = None\n",
    "    prev2_token = None\n",
    "    prev3_token = None\n",
    "    next1_token = sent[1] #sent is always grater than 5\n",
    "    next2_token = sent[2]\n",
    "    next3_token = sent[3]\n",
    "    for i, token in enumerate(sent):\n",
    "        fdic = {}        \n",
    "        token_features('curr', token, fdic)\n",
    "        token_features('prev1', prev1_token, fdic)\n",
    "        token_features('prev2', prev2_token, fdic)\n",
    "        token_features('prev3', prev3_token, fdic)\n",
    "        token_features('next1', next1_token, fdic)\n",
    "        token_features('next2', next1_token, fdic)\n",
    "        token_features('next3', next1_token, fdic)\n",
    "        \n",
    "        fdic['passed'] = len(sent)/(i+1)\n",
    "        fdic['left'] = len(sent)/(len(sent) - i +1)\n",
    "        \n",
    "        fdic['bigram'] = math.log((0.0 if i == (len(sent) - 1) else bigrams[(token.text.lower(), sent[i+1].text.lower())]) +  sys.float_info.epsilon * 1000000)\n",
    "        fdic['pbigram'] = math.log((0.0 if i == (len(sent) - 1) else bigrams[(token.text.lower(), '.')]) +  sys.float_info.epsilon * 1000000)        \n",
    "        fdic['pbigram2'] = math.log((0.0 if i == (len(sent) - 1) else bigrams[('.', sent[i+1].text.lower())]) +  sys.float_info.epsilon * 1000000)      \n",
    "        fdic['trigram'] = math.log((0.0 if i > (len(sent) - 3) else trigrams[(token.text.lower(), sent[i+1].text.lower(), sent[i+2].text.lower())]) +  sys.float_info.epsilon * 1000000)\n",
    "        fdic['ptrigram'] = math.log((0.0 if i == (len(sent) - 1) else trigrams[(token.text.lower(), '.', sent[i+1].text.lower())]) + sys.float_info.epsilon * 1000000)\n",
    "        fdic['ptrigram2'] = math.log((0.0 if i > (len(sent) - 3) else trigrams[('.', sent[i+1].text.lower(), sent[i+2].text.lower())]) +  sys.float_info.epsilon * 1000000)\n",
    "        fdic['ptrigram3'] = math.log((trigrams[(sent[i-1].text.lower(), token.text.lower(), '.')] if i > 0 else 0.0) + sys.float_info.epsilon * 1000000)         \n",
    "        \n",
    "        features.append(fdic)\n",
    "                \n",
    "        prev3_token = prev2_token\n",
    "        prev2_token = prev1_token\n",
    "        prev1_token = token            \n",
    "        \n",
    "        next1_token = next2_token\n",
    "        next2_token = next3_token\n",
    "        next3_token = sent[i + 1] if i < (len(sent) - 2) else None\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features(sents):\n",
    "    features = []\n",
    "    for sent in sents:\n",
    "        features.extend(extract_sent_features(sent))\n",
    "    return features \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "vectorizer.fit(features)\n",
    "\n",
    "feature_vecs = vectorizer.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "#logreg = LogisticRegression(random_state=26,  solver='lbfgs', multi_class=\"multinomial\", max_iter=2000)\n",
    "logreg = LogisticRegression(random_state=23466, solver='lbfgs', max_iter=3000)\n",
    "\n",
    "                               \n",
    "\n",
    "logreg.fit(feature_vecs, labels_vec(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    vec = vectorizer.transform(extract_features(data))\n",
    "    return logreg.predict(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_vec(test_data), test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_js_tokenized = [convert_to_tokens(sent) for sent in run_on_js]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([w[1] for sent in run_on_js for w in sent]))\n",
    "print(sum([t.point_after for sent in run_on_js_tokenized for t in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict([run_on_js_tokenized[0]]))\n",
    "print(run_on_js_tokenized[0][-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_on_js[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels_vec(run_on_js_tokenized), predict(run_on_js_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip([t.point_after for sent in run_on_js_tokenized for t in sent], predict(run_on_js_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
