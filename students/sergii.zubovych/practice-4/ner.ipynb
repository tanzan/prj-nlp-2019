{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/Users/serg/projects/ner-uk' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lang-uk/ner-uk.git ~/projects/ner-uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path '/Users/serg/projects/ua-gazetters' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lang-uk/ua-gazetteers.git ~/projects/ua-gazetters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 73\n"
     ]
    }
   ],
   "source": [
    "PATH = \"/Users/serg/projects/ner-uk/\"\n",
    "\n",
    "# Read tokens and positions of tokens from a file\n",
    "\n",
    "def read_tokens(filename):\n",
    "    tokens = []\n",
    "    pos = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read().split(\"\\n\")\n",
    "        for line in text:\n",
    "            if len(line) == 0:\n",
    "                pos += 1\n",
    "            else:\n",
    "                tokens.append((\"<S>\", pos, pos))\n",
    "                for token in line.split(\" \"):\n",
    "                    tokens.append((token, pos, pos + len(token)))\n",
    "                    pos += len(token) + 1\n",
    "                tokens.append((\"</S>\", pos, pos))\n",
    "    return tokens\n",
    "\n",
    "# Read annotations and positions of annotations from a file\n",
    "\n",
    "def read_annotations(filename):\n",
    "    anno = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            annotations = line.split()\n",
    "            anno.append((annotations[1], int(annotations[2]), int(annotations[3])))\n",
    "    return anno\n",
    "\n",
    "# Using positions of tokens and annotations, extract a list of token labels\n",
    "\n",
    "def extract_labels(anno, tokens):\n",
    "    labels = []\n",
    "    ann_id = 0\n",
    "    for token in tokens:\n",
    "        if ann_id < len(anno):\n",
    "            label, beg, end = anno[ann_id]\n",
    "            if token[0] in [\"<S>\", \"</S>\"]:\n",
    "                labels.append(\"--\")\n",
    "            elif token[1] < beg:\n",
    "                labels.append(\"--\")\n",
    "            else:\n",
    "                if token[1] == beg:\n",
    "                    labels.append(\"B-\" + label)\n",
    "                else:\n",
    "                    labels.append(\"I-\" + label)\n",
    "                if token[2] == end:\n",
    "                    ann_id += 1\n",
    "        else:\n",
    "            labels.append(\"--\")    \n",
    "    return labels\n",
    "\n",
    "# tokens = read_tokens(PATH + \"data/A_alumni.krok.edu.ua_Prokopenko_Vidrodzhennia_velotreku(5).tok.txt\")\n",
    "# anno = read_annotations(PATH + \"data/A_alumni.krok.edu.ua_Prokopenko_Vidrodzhennia_velotreku(5).tok.ann\")\n",
    "# labels = extract_labels(anno, tokens)\n",
    "\n",
    "# for i, j in zip(tokens, labels):\n",
    "#     print(i[0], j)\n",
    "\n",
    "# Extract list of files for training and testing\n",
    "\n",
    "dev_test = {\"dev\": [], \"test\": []}\n",
    "category = \"\"\n",
    "with open(PATH + \"doc/dev-test-split.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line in [\"DEV\", \"TEST\"]:\n",
    "            category = line.lower()\n",
    "        elif len(line) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dev_test[category].append(line)\n",
    "\n",
    "print(len(dev_test[\"dev\"]), len(dev_test[\"test\"]))\n",
    "\n",
    "# Get train and test data and labels\n",
    "\n",
    "train_tokens, test_tokens, train_labels, test_labels = [], [], [], []\n",
    "\n",
    "for filename in dev_test[\"dev\"]:\n",
    "    try:\n",
    "        tokens = read_tokens(PATH + \"data/\" + filename + \".txt\")\n",
    "        train_tokens += [token[0] for token in tokens]\n",
    "        train_labels += extract_labels(read_annotations(PATH + \"data/\" + filename + \".ann\"), tokens)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for filename in dev_test[\"test\"]:\n",
    "    try:\n",
    "        tokens = read_tokens(PATH + \"data/\" + filename + \".txt\")\n",
    "        test_tokens += [token[0] for token in tokens]\n",
    "        test_labels += extract_labels(read_annotations(PATH + \"data/\" + filename + \".ann\"), tokens)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('–', '--'),\n",
       " ('повідомляють', '--'),\n",
       " ('Першій', 'B-ОРГ'),\n",
       " ('електронній', 'I-ОРГ'),\n",
       " ('в', '--'),\n",
       " ('прес-службі', '--'),\n",
       " ('УДМС', 'B-ОРГ'),\n",
       " ('України', 'I-ОРГ'),\n",
       " ('в', 'I-ОРГ'),\n",
       " ('Кіровоградській', 'I-ОРГ'),\n",
       " ('області', 'I-ОРГ'),\n",
       " ('.', '--'),\n",
       " ('</S>', '--'),\n",
       " ('<S>', '--'),\n",
       " ('Розцінки', '--'),\n",
       " ('на', '--'),\n",
       " ('послуги', '--'),\n",
       " ('таких', '--'),\n",
       " ('посередників', '--'),\n",
       " ('починаються', '--'),\n",
       " ('від', '--'),\n",
       " ('1000', '--'),\n",
       " ('грн', '--'),\n",
       " ('.', '--'),\n",
       " ('</S>', '--'),\n",
       " ('<S>', '--'),\n",
       " ('Закінчується', '--'),\n",
       " ('«', '--'),\n",
       " ('біометрична', '--'),\n",
       " ('афера', '--'),\n",
       " ('»', '--'),\n",
       " ('в', '--'),\n",
       " ('кращому', '--'),\n",
       " ('випадку', '--'),\n",
       " ('звичайним', '--'),\n",
       " ('оформленням', '--'),\n",
       " ('документа', '--'),\n",
       " ('у', '--'),\n",
       " ('міграційній', 'B-ОРГ'),\n",
       " ('службі', 'I-ОРГ'),\n",
       " ('у', '--'),\n",
       " ('встановлений', '--'),\n",
       " ('законодавством', '--'),\n",
       " ('строк', '--'),\n",
       " (',', '--'),\n",
       " ('у', '--'),\n",
       " ('гіршому', '--'),\n",
       " ('–', '--'),\n",
       " ('втратою', '--'),\n",
       " ('коштів', '--')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_tokens, train_labels))[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<S>', '--'),\n",
       " ('В', '--'),\n",
       " ('Україні', 'B-ЛОК'),\n",
       " ('спостерігається', '--'),\n",
       " ('істотне', '--'),\n",
       " ('погіршення', '--'),\n",
       " ('погодних', '--'),\n",
       " ('умов', '--'),\n",
       " (':', '--'),\n",
       " ('сильний', '--')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(test_tokens, test_labels))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('спостерігається')[0].tag.POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Token = namedtuple('Token', 'text iob_tag pos lemma')\n",
    "\n",
    "def extract_sent_features(sent):\n",
    "    features = []\n",
    "    for i, token in enumerate(sent):\n",
    "        fdic = {}\n",
    "        #fdic['len'] = len(token.text)\n",
    "        fdic['case'] = token.text[:1].isupper() \n",
    "        fdic['pos'] = str(token.pos)\n",
    "        fdic['pos_prev'] = str(None) if i == 0 else str(sent[i-1].pos)\n",
    "        fdic['pos_next'] = str(None) if i == (len(sent) - 1) else str(sent[i+1].pos)\n",
    "        fdic['prev_lemma'] = str(None) if i == 0 else sent[i-1].lemma\n",
    "        fdic['next_lemma'] = str(None) if i == (len(sent) - 1) else sent[i+1].lemma\n",
    "        fdic['lemma'] = token.lemma\n",
    "        features.append(fdic)\n",
    "    return features    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(tokens, labels):\n",
    "    sent = []\n",
    "    sents = []\n",
    "    for token,label in zip(tokens, labels):\n",
    "        pm = morph.parse(token)[0]\n",
    "        sent.append(Token(text = token, iob_tag = label, pos = pm.tag.POS, lemma = pm.normal_form))\n",
    "        if token == '</S>':\n",
    "            sents.append(sent)\n",
    "            sent = []\n",
    "    return sents\n",
    "\n",
    "sents = prepare(train_tokens, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='На', iob_tag='--', pos='INTJ', lemma='на'),\n",
       "  Token(text='довірливих', iob_tag='--', pos='ADJF', lemma='довірливий'),\n",
       "  Token(text='кіровоградців', iob_tag='--', pos='NOUN', lemma='кіровоградець'),\n",
       "  Token(text='полюють', iob_tag='--', pos='VERB', lemma='полювати'),\n",
       "  Token(text='шахраї', iob_tag='--', pos='NOUN', lemma='шахрай'),\n",
       "  Token(text='та', iob_tag='--', pos='CONJ', lemma='та'),\n",
       "  Token(text='фірми-посередники', iob_tag='--', pos='NOUN', lemma='фірма-посередник'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='які', iob_tag='--', pos='NPRO', lemma='який'),\n",
       "  Token(text='за', iob_tag='--', pos='ADVB', lemma='за'),\n",
       "  Token(text='1000', iob_tag='--', pos=None, lemma='1000'),\n",
       "  Token(text='грн', iob_tag='--', pos='NOUN', lemma='грн'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='готові', iob_tag='--', pos='ADJF', lemma='готовий'),\n",
       "  Token(text='«', iob_tag='--', pos=None, lemma='«'),\n",
       "  Token(text='виготовити', iob_tag='--', pos='VERB', lemma='виготовити'),\n",
       "  Token(text='»', iob_tag='--', pos=None, lemma='»'),\n",
       "  Token(text='біометричний', iob_tag='--', pos='ADJF', lemma='біометричний'),\n",
       "  Token(text='паспорт', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='який', iob_tag='--', pos='NPRO', lemma='який'),\n",
       "  Token(text='коштує', iob_tag='--', pos='VERB', lemma='коштувати'),\n",
       "  Token(text='518', iob_tag='--', pos=None, lemma='518'),\n",
       "  Token(text='грн', iob_tag='--', pos='NOUN', lemma='грн'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Із', iob_tag='--', pos='PREP', lemma='із'),\n",
       "  Token(text='запровадженням', iob_tag='--', pos='NOUN', lemma='запровадження'),\n",
       "  Token(text='біометричних', iob_tag='--', pos='ADJF', lemma='біометричний'),\n",
       "  Token(text='паспортів', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text='активізувалися', iob_tag='--', pos='VERB', lemma='активізуватися'),\n",
       "  Token(text='шахраї', iob_tag='--', pos='NOUN', lemma='шахрай'),\n",
       "  Token(text='та', iob_tag='--', pos='CONJ', lemma='та'),\n",
       "  Token(text='фірми-посередники', iob_tag='--', pos='NOUN', lemma='фірма-посередник'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='які', iob_tag='--', pos='NPRO', lemma='який'),\n",
       "  Token(text='пропонують', iob_tag='--', pos='VERB', lemma='пропонувати'),\n",
       "  Token(text='«', iob_tag='--', pos=None, lemma='«'),\n",
       "  Token(text='прискорити', iob_tag='--', pos='VERB', lemma='прискорити'),\n",
       "  Token(text='»', iob_tag='--', pos=None, lemma='»'),\n",
       "  Token(text='оформлення', iob_tag='--', pos='NOUN', lemma='оформлення'),\n",
       "  Token(text='біометричного', iob_tag='--', pos='ADJF', lemma='біометричний'),\n",
       "  Token(text='паспорта', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text='або', iob_tag='--', pos='CONJ', lemma='або'),\n",
       "  Token(text='просто', iob_tag='--', pos='PRCL', lemma='просто'),\n",
       "  Token(text='оформити', iob_tag='--', pos='VERB', lemma='оформити'),\n",
       "  Token(text='цей', iob_tag='--', pos='NPRO', lemma='цей'),\n",
       "  Token(text='документ', iob_tag='--', pos='NOUN', lemma='документ'),\n",
       "  Token(text='–', iob_tag='--', pos=None, lemma='–'),\n",
       "  Token(text='повідомляють', iob_tag='--', pos='VERB', lemma='повідомляти'),\n",
       "  Token(text='Першій', iob_tag='B-ОРГ', pos='ADJF', lemma='перший'),\n",
       "  Token(text='електронній', iob_tag='I-ОРГ', pos='ADJF', lemma='електронний'),\n",
       "  Token(text='в', iob_tag='--', pos='PREP', lemma='в'),\n",
       "  Token(text='прес-службі', iob_tag='--', pos='NOUN', lemma='прес-служба'),\n",
       "  Token(text='УДМС', iob_tag='B-ОРГ', pos='NOUN', lemma='удмс'),\n",
       "  Token(text='України', iob_tag='I-ОРГ', pos='NOUN', lemma='україна'),\n",
       "  Token(text='в', iob_tag='I-ОРГ', pos='PREP', lemma='в'),\n",
       "  Token(text='Кіровоградській', iob_tag='I-ОРГ', pos='ADJF', lemma='кіровоградський'),\n",
       "  Token(text='області', iob_tag='I-ОРГ', pos='NOUN', lemma='область'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Розцінки', iob_tag='--', pos='NOUN', lemma='розцінка'),\n",
       "  Token(text='на', iob_tag='--', pos='INTJ', lemma='на'),\n",
       "  Token(text='послуги', iob_tag='--', pos='NOUN', lemma='послуга'),\n",
       "  Token(text='таких', iob_tag='--', pos='NPRO', lemma='такий'),\n",
       "  Token(text='посередників', iob_tag='--', pos='NOUN', lemma='посередник'),\n",
       "  Token(text='починаються', iob_tag='--', pos='VERB', lemma='починатися'),\n",
       "  Token(text='від', iob_tag='--', pos='PREP', lemma='від'),\n",
       "  Token(text='1000', iob_tag='--', pos=None, lemma='1000'),\n",
       "  Token(text='грн', iob_tag='--', pos='NOUN', lemma='грн'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Закінчується', iob_tag='--', pos='VERB', lemma='закінчуватися'),\n",
       "  Token(text='«', iob_tag='--', pos=None, lemma='«'),\n",
       "  Token(text='біометрична', iob_tag='--', pos='ADJF', lemma='біометричний'),\n",
       "  Token(text='афера', iob_tag='--', pos='NOUN', lemma='афера'),\n",
       "  Token(text='»', iob_tag='--', pos=None, lemma='»'),\n",
       "  Token(text='в', iob_tag='--', pos='PREP', lemma='в'),\n",
       "  Token(text='кращому', iob_tag='--', pos='ADJF', lemma='кращий'),\n",
       "  Token(text='випадку', iob_tag='--', pos='NOUN', lemma='випадок'),\n",
       "  Token(text='звичайним', iob_tag='--', pos='ADJF', lemma='звичайний'),\n",
       "  Token(text='оформленням', iob_tag='--', pos='NOUN', lemma='оформлення'),\n",
       "  Token(text='документа', iob_tag='--', pos='NOUN', lemma='документ'),\n",
       "  Token(text='у', iob_tag='--', pos='PREP', lemma='у'),\n",
       "  Token(text='міграційній', iob_tag='B-ОРГ', pos='ADJF', lemma='міграційний'),\n",
       "  Token(text='службі', iob_tag='I-ОРГ', pos='NOUN', lemma='служба'),\n",
       "  Token(text='у', iob_tag='--', pos='PREP', lemma='у'),\n",
       "  Token(text='встановлений', iob_tag='--', pos='ADJF', lemma='встановлений'),\n",
       "  Token(text='законодавством', iob_tag='--', pos='NOUN', lemma='законодавство'),\n",
       "  Token(text='строк', iob_tag='--', pos='NOUN', lemma='строк'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='у', iob_tag='--', pos='PREP', lemma='у'),\n",
       "  Token(text='гіршому', iob_tag='--', pos='ADJF', lemma='гірший'),\n",
       "  Token(text='–', iob_tag='--', pos=None, lemma='–'),\n",
       "  Token(text='втратою', iob_tag='--', pos='NOUN', lemma='втрата'),\n",
       "  Token(text='коштів', iob_tag='--', pos='NOUN', lemma='кошт'),\n",
       "  Token(text='і', iob_tag='--', pos='CONJ', lemma='і'),\n",
       "  Token(text='неотриманням', iob_tag='--', pos='NOUN', lemma='неотримання'),\n",
       "  Token(text='документа', iob_tag='--', pos='NOUN', lemma='документ'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Інформація', iob_tag='--', pos='NOUN', lemma='інформація'),\n",
       "  Token(text='про', iob_tag='--', pos='NOUN', lemma='про'),\n",
       "  Token(text='вже', iob_tag='--', pos='ADVB', lemma='вже'),\n",
       "  Token(text='виявлених', iob_tag='--', pos='ADJF', lemma='виявлений'),\n",
       "  Token(text='«', iob_tag='--', pos=None, lemma='«'),\n",
       "  Token(text='посередників', iob_tag='--', pos='NOUN', lemma='посередник'),\n",
       "  Token(text='»', iob_tag='--', pos=None, lemma='»'),\n",
       "  Token(text='передана', iob_tag='--', pos='ADJF', lemma='переданий'),\n",
       "  Token(text='до', iob_tag='--', pos='NOUN', lemma='до'),\n",
       "  Token(text='правоохоронних', iob_tag='--', pos='ADJF', lemma='правоохоронний'),\n",
       "  Token(text='органів', iob_tag='--', pos='NOUN', lemma='орган'),\n",
       "  Token(text='для', iob_tag='--', pos='PREP', lemma='для'),\n",
       "  Token(text='відповідного', iob_tag='--', pos='ADJF', lemma='відповідний'),\n",
       "  Token(text='реагування', iob_tag='--', pos='NOUN', lemma='реагування'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Для', iob_tag='--', pos='PREP', lemma='для'),\n",
       "  Token(text='оформлення', iob_tag='--', pos='NOUN', lemma='оформлення'),\n",
       "  Token(text='біометричного', iob_tag='--', pos='ADJF', lemma='біометричний'),\n",
       "  Token(text='паспорта', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text='громадянам', iob_tag='--', pos='NOUN', lemma='громадянин'),\n",
       "  Token(text='необхідно', iob_tag='--', pos='ADVB', lemma='необхідно'),\n",
       "  Token(text='звертатися', iob_tag='--', pos='VERB', lemma='звертатися'),\n",
       "  Token(text='до', iob_tag='--', pos='NOUN', lemma='до'),\n",
       "  Token(text='відповідного', iob_tag='--', pos='ADJF', lemma='відповідний'),\n",
       "  Token(text='найближчого', iob_tag='--', pos='ADJF', lemma='найближчий'),\n",
       "  Token(text='підрозділу', iob_tag='--', pos='NOUN', lemma='підрозділ'),\n",
       "  Token(text='міграційної', iob_tag='B-ОРГ', pos='ADJF', lemma='міграційний'),\n",
       "  Token(text='служби', iob_tag='I-ОРГ', pos='NOUN', lemma='служба'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='і', iob_tag='--', pos='CONJ', lemma='і'),\n",
       "  Token(text='нікуди', iob_tag='--', pos='NOUN', lemma='нікуда'),\n",
       "  Token(text='більше', iob_tag='--', pos='ADVB', lemma='більше'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Тільки', iob_tag='--', pos='PRCL', lemma='тільки'),\n",
       "  Token(text='працівники', iob_tag='--', pos='NOUN', lemma='працівник'),\n",
       "  Token(text='міграційної', iob_tag='B-ОРГ', pos='ADJF', lemma='міграційний'),\n",
       "  Token(text='служби', iob_tag='I-ОРГ', pos='NOUN', lemma='служба'),\n",
       "  Token(text='здійснюють', iob_tag='--', pos='VERB', lemma='здійснювати'),\n",
       "  Token(text='фотографування', iob_tag='--', pos='NOUN', lemma='фотографування'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='сканування', iob_tag='--', pos='NOUN', lemma='сканування'),\n",
       "  Token(text='підпису', iob_tag='--', pos='NOUN', lemma='підпис'),\n",
       "  Token(text='та', iob_tag='--', pos='CONJ', lemma='та'),\n",
       "  Token(text='відбитків', iob_tag='--', pos='NOUN', lemma='відбиток'),\n",
       "  Token(text='пальців', iob_tag='--', pos='NOUN', lemma='палець'),\n",
       "  Token(text='рук', iob_tag='--', pos='NOUN', lemma='рука'),\n",
       "  Token(text='особи', iob_tag='--', pos='NOUN', lemma='особа'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='і', iob_tag='--', pos='CONJ', lemma='і'),\n",
       "  Token(text='без', iob_tag='--', pos='PREP', lemma='без'),\n",
       "  Token(text='особистої', iob_tag='--', pos='ADJF', lemma='особистий'),\n",
       "  Token(text='присутності', iob_tag='--', pos='NOUN', lemma='присутність'),\n",
       "  Token(text='людини', iob_tag='--', pos='NOUN', lemma='людина'),\n",
       "  Token(text='у', iob_tag='--', pos='PREP', lemma='у'),\n",
       "  Token(text='підрозділі', iob_tag='--', pos='NOUN', lemma='підрозділ'),\n",
       "  Token(text='ДМС', iob_tag='B-ОРГ', pos=None, lemma='дмс'),\n",
       "  Token(text='ці', iob_tag='--', pos='NPRO', lemma='цей'),\n",
       "  Token(text='процедури', iob_tag='--', pos='NOUN', lemma='процедура'),\n",
       "  Token(text='провести', iob_tag='--', pos='VERB', lemma='провести'),\n",
       "  Token(text='неможливо', iob_tag='--', pos='ADVB', lemma='неможливо'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='Система', iob_tag='--', pos='NOUN', lemma='система'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='на', iob_tag='--', pos='INTJ', lemma='на'),\n",
       "  Token(text='основі', iob_tag='--', pos='NOUN', lemma='основа'),\n",
       "  Token(text='якої', iob_tag='--', pos='NPRO', lemma='який'),\n",
       "  Token(text='оформлюються', iob_tag='--', pos='VERB', lemma='оформлюватися'),\n",
       "  Token(text='паспорти', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='не', iob_tag='--', pos='PRCL', lemma='не'),\n",
       "  Token(text='приймає', iob_tag='--', pos='VERB', lemma='приймати'),\n",
       "  Token(text='інформацію', iob_tag='--', pos='NOUN', lemma='інформація'),\n",
       "  Token(text='з', iob_tag='--', pos='PREP', lemma='з'),\n",
       "  Token(text='жодних', iob_tag='--', pos='NPRO', lemma='жодний'),\n",
       "  Token(text='носіїв', iob_tag='--', pos='NOUN', lemma='носій'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='На', iob_tag='--', pos='INTJ', lemma='на'),\n",
       "  Token(text='своїх', iob_tag='--', pos='NPRO', lemma='свій'),\n",
       "  Token(text='сайтах', iob_tag='--', pos='NOUN', lemma='сайт'),\n",
       "  Token(text='шахраї', iob_tag='--', pos='NOUN', lemma='шахрай'),\n",
       "  Token(text='пишуть', iob_tag='--', pos='VERB', lemma='писати'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='що', iob_tag='--', pos='CONJ', lemma='що'),\n",
       "  Token(text='нібито', iob_tag='--', pos='PRCL', lemma='нібито'),\n",
       "  Token(text='є', iob_tag='--', pos='VERB', lemma='бути'),\n",
       "  Token(text='офіційними', iob_tag='--', pos='ADJF', lemma='офіційний'),\n",
       "  Token(text='партнерами', iob_tag='--', pos='NOUN', lemma='партнер'),\n",
       "  Token(text='Державної', iob_tag='B-ОРГ', pos='ADJF', lemma='державний'),\n",
       "  Token(text='міграційної', iob_tag='I-ОРГ', pos='ADJF', lemma='міграційний'),\n",
       "  Token(text='служби', iob_tag='I-ОРГ', pos='NOUN', lemma='служба'),\n",
       "  Token(text='України', iob_tag='I-ОРГ', pos='NOUN', lemma='україна'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='або', iob_tag='--', pos='CONJ', lemma='або'),\n",
       "  Token(text='ж', iob_tag='--', pos='CONJ', lemma='ж'),\n",
       "  Token(text='Державного', iob_tag='B-ОРГ', pos='ADJF', lemma='державний'),\n",
       "  Token(text='підприємства', iob_tag='I-ОРГ', pos='NOUN', lemma='підприємство'),\n",
       "  Token(text='«', iob_tag='I-ОРГ', pos=None, lemma='«'),\n",
       "  Token(text='Документ', iob_tag='I-ОРГ', pos='NOUN', lemma='документ'),\n",
       "  Token(text='»', iob_tag='I-ОРГ', pos=None, lemma='»'),\n",
       "  Token(text='у', iob_tag='--', pos='PREP', lemma='у'),\n",
       "  Token(text='галузі', iob_tag='--', pos='NOUN', lemma='галуза'),\n",
       "  Token(text='оформлення', iob_tag='--', pos='NOUN', lemma='оформлення'),\n",
       "  Token(text='документів', iob_tag='--', pos='NOUN', lemma='документ'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='однак', iob_tag='--', pos='CONJ', lemma='однак'),\n",
       "  Token(text='ані', iob_tag='--', pos='CONJ', lemma='ані'),\n",
       "  Token(text='міграційна', iob_tag='B-ОРГ', pos='ADJF', lemma='міграційний'),\n",
       "  Token(text='служба', iob_tag='I-ОРГ', pos='NOUN', lemma='служба'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='ані', iob_tag='--', pos='CONJ', lemma='ані'),\n",
       "  Token(text='ДП', iob_tag='B-ОРГ', pos='NOUN', lemma='дп'),\n",
       "  Token(text='«', iob_tag='I-ОРГ', pos=None, lemma='«'),\n",
       "  Token(text='Документ', iob_tag='I-ОРГ', pos='NOUN', lemma='документ'),\n",
       "  Token(text='»', iob_tag='I-ОРГ', pos=None, lemma='»'),\n",
       "  Token(text='не', iob_tag='--', pos='PRCL', lemma='не'),\n",
       "  Token(text='співпрацюють', iob_tag='--', pos='VERB', lemma='співпрацювати'),\n",
       "  Token(text='з', iob_tag='--', pos='PREP', lemma='з'),\n",
       "  Token(text='питань', iob_tag='--', pos='NOUN', lemma='питання'),\n",
       "  Token(text='оформлення', iob_tag='--', pos='NOUN', lemma='оформлення'),\n",
       "  Token(text='паспорта', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text='для', iob_tag='--', pos='PREP', lemma='для'),\n",
       "  Token(text='виїзду', iob_tag='--', pos='NOUN', lemma='виїзд'),\n",
       "  Token(text='за', iob_tag='--', pos='ADVB', lemma='за'),\n",
       "  Token(text='кордон', iob_tag='--', pos='NOUN', lemma='кордон'),\n",
       "  Token(text='з', iob_tag='--', pos='PREP', lemma='з'),\n",
       "  Token(text='жодними', iob_tag='--', pos='NPRO', lemma='жодний'),\n",
       "  Token(text='приватними', iob_tag='--', pos='ADJF', lemma='приватний'),\n",
       "  Token(text='компаніями', iob_tag='--', pos='NOUN', lemma='компанія'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')],\n",
       " [Token(text='<S>', iob_tag='--', pos=None, lemma='<s>'),\n",
       "  Token(text='У', iob_tag='--', pos='PREP', lemma='у'),\n",
       "  Token(text='разі', iob_tag='--', pos='NOUN', lemma='раз'),\n",
       "  Token(text='виявлення', iob_tag='--', pos='NOUN', lemma='виявлення'),\n",
       "  Token(text='приватних', iob_tag='--', pos='ADJF', lemma='приватний'),\n",
       "  Token(text='організацій', iob_tag='--', pos='NOUN', lemma='організація'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='які', iob_tag='--', pos='NPRO', lemma='який'),\n",
       "  Token(text='пропонують', iob_tag='--', pos='VERB', lemma='пропонувати'),\n",
       "  Token(text='послуги', iob_tag='--', pos='NOUN', lemma='послуга'),\n",
       "  Token(text='з', iob_tag='--', pos='PREP', lemma='з'),\n",
       "  Token(text='оформлення', iob_tag='--', pos='NOUN', lemma='оформлення'),\n",
       "  Token(text='біометричного', iob_tag='--', pos='ADJF', lemma='біометричний'),\n",
       "  Token(text='паспорта', iob_tag='--', pos='NOUN', lemma='паспорт'),\n",
       "  Token(text=',', iob_tag='--', pos=None, lemma=','),\n",
       "  Token(text='громадяни', iob_tag='--', pos='NOUN', lemma='громадянин'),\n",
       "  Token(text='мають', iob_tag='--', pos='VERB', lemma='мати'),\n",
       "  Token(text='інформувати', iob_tag='--', pos='VERB', lemma='інформувати'),\n",
       "  Token(text='правоохоронні', iob_tag='--', pos='ADJF', lemma='правоохоронний'),\n",
       "  Token(text='органи', iob_tag='--', pos='NOUN', lemma='орган'),\n",
       "  Token(text='або', iob_tag='--', pos='CONJ', lemma='або'),\n",
       "  Token(text='міграційну', iob_tag='B-ОРГ', pos='ADJF', lemma='міграційний'),\n",
       "  Token(text='службу', iob_tag='I-ОРГ', pos='NOUN', lemma='служба'),\n",
       "  Token(text='.', iob_tag='--', pos=None, lemma='.'),\n",
       "  Token(text='</S>', iob_tag='--', pos=None, lemma='</s>')]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sents):\n",
    "    features = []\n",
    "    for sent in sents:\n",
    "        features.extend(extract_sent_features(sent))\n",
    "    return features \n",
    "\n",
    "features = extract_features(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'case': False,\n",
       "  'pos': 'None',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'INTJ',\n",
       "  'prev_lemma': 'None',\n",
       "  'next_lemma': 'на',\n",
       "  'lemma': '<s>'},\n",
       " {'case': True,\n",
       "  'pos': 'INTJ',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'ADJF',\n",
       "  'prev_lemma': '<s>',\n",
       "  'next_lemma': 'довірливий',\n",
       "  'lemma': 'на'},\n",
       " {'case': False,\n",
       "  'pos': 'ADJF',\n",
       "  'pos_prev': 'INTJ',\n",
       "  'pos_next': 'NOUN',\n",
       "  'prev_lemma': 'на',\n",
       "  'next_lemma': 'кіровоградець',\n",
       "  'lemma': 'довірливий'},\n",
       " {'case': False,\n",
       "  'pos': 'NOUN',\n",
       "  'pos_prev': 'ADJF',\n",
       "  'pos_next': 'VERB',\n",
       "  'prev_lemma': 'довірливий',\n",
       "  'next_lemma': 'полювати',\n",
       "  'lemma': 'кіровоградець'},\n",
       " {'case': False,\n",
       "  'pos': 'VERB',\n",
       "  'pos_prev': 'NOUN',\n",
       "  'pos_next': 'NOUN',\n",
       "  'prev_lemma': 'кіровоградець',\n",
       "  'next_lemma': 'шахрай',\n",
       "  'lemma': 'полювати'},\n",
       " {'case': False,\n",
       "  'pos': 'NOUN',\n",
       "  'pos_prev': 'VERB',\n",
       "  'pos_next': 'CONJ',\n",
       "  'prev_lemma': 'полювати',\n",
       "  'next_lemma': 'та',\n",
       "  'lemma': 'шахрай'},\n",
       " {'case': False,\n",
       "  'pos': 'CONJ',\n",
       "  'pos_prev': 'NOUN',\n",
       "  'pos_next': 'NOUN',\n",
       "  'prev_lemma': 'шахрай',\n",
       "  'next_lemma': 'фірма-посередник',\n",
       "  'lemma': 'та'},\n",
       " {'case': False,\n",
       "  'pos': 'NOUN',\n",
       "  'pos_prev': 'CONJ',\n",
       "  'pos_next': 'None',\n",
       "  'prev_lemma': 'та',\n",
       "  'next_lemma': ',',\n",
       "  'lemma': 'фірма-посередник'},\n",
       " {'case': False,\n",
       "  'pos': 'None',\n",
       "  'pos_prev': 'NOUN',\n",
       "  'pos_next': 'NPRO',\n",
       "  'prev_lemma': 'фірма-посередник',\n",
       "  'next_lemma': 'який',\n",
       "  'lemma': ','},\n",
       " {'case': False,\n",
       "  'pos': 'NPRO',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'ADVB',\n",
       "  'prev_lemma': ',',\n",
       "  'next_lemma': 'за',\n",
       "  'lemma': 'який'},\n",
       " {'case': False,\n",
       "  'pos': 'ADVB',\n",
       "  'pos_prev': 'NPRO',\n",
       "  'pos_next': 'None',\n",
       "  'prev_lemma': 'який',\n",
       "  'next_lemma': '1000',\n",
       "  'lemma': 'за'},\n",
       " {'case': False,\n",
       "  'pos': 'None',\n",
       "  'pos_prev': 'ADVB',\n",
       "  'pos_next': 'NOUN',\n",
       "  'prev_lemma': 'за',\n",
       "  'next_lemma': 'грн',\n",
       "  'lemma': '1000'},\n",
       " {'case': False,\n",
       "  'pos': 'NOUN',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'None',\n",
       "  'prev_lemma': '1000',\n",
       "  'next_lemma': '.',\n",
       "  'lemma': 'грн'},\n",
       " {'case': False,\n",
       "  'pos': 'None',\n",
       "  'pos_prev': 'NOUN',\n",
       "  'pos_next': 'ADJF',\n",
       "  'prev_lemma': 'грн',\n",
       "  'next_lemma': 'готовий',\n",
       "  'lemma': '.'},\n",
       " {'case': False,\n",
       "  'pos': 'ADJF',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'None',\n",
       "  'prev_lemma': '.',\n",
       "  'next_lemma': '«',\n",
       "  'lemma': 'готовий'},\n",
       " {'case': False,\n",
       "  'pos': 'None',\n",
       "  'pos_prev': 'ADJF',\n",
       "  'pos_next': 'VERB',\n",
       "  'prev_lemma': 'готовий',\n",
       "  'next_lemma': 'виготовити',\n",
       "  'lemma': '«'},\n",
       " {'case': False,\n",
       "  'pos': 'VERB',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'None',\n",
       "  'prev_lemma': '«',\n",
       "  'next_lemma': '»',\n",
       "  'lemma': 'виготовити'},\n",
       " {'case': False,\n",
       "  'pos': 'None',\n",
       "  'pos_prev': 'VERB',\n",
       "  'pos_next': 'ADJF',\n",
       "  'prev_lemma': 'виготовити',\n",
       "  'next_lemma': 'біометричний',\n",
       "  'lemma': '»'},\n",
       " {'case': False,\n",
       "  'pos': 'ADJF',\n",
       "  'pos_prev': 'None',\n",
       "  'pos_next': 'NOUN',\n",
       "  'prev_lemma': '»',\n",
       "  'next_lemma': 'паспорт',\n",
       "  'lemma': 'біометричний'},\n",
       " {'case': False,\n",
       "  'pos': 'NOUN',\n",
       "  'pos_prev': 'ADJF',\n",
       "  'pos_next': 'None',\n",
       "  'prev_lemma': 'біометричний',\n",
       "  'next_lemma': ',',\n",
       "  'lemma': 'паспорт'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "vectorizer.fit(features)\n",
    "\n",
    "feature_vecs = vectorizer.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vecs.toarray()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def norm_labels(raw_labels):\n",
    "    return [re.sub(r'[^\\-]\\-','',label) for label in raw_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=2000, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "logreg = LogisticRegression(random_state=42,  solver='lbfgs', multi_class=\"multinomial\", max_iter=2000)\n",
    "\n",
    "logreg.fit(feature_vecs, norm_labels(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = prepare(test_tokens, test_labels)\n",
    "\n",
    "test_features_vec = vectorizer.transform(extract_features(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = logreg.predict(test_features_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          --       0.92      1.00      0.96     69817\n",
      "         ЛОК       0.67      0.19      0.30      1485\n",
      "         ОРГ       0.78      0.07      0.12      2188\n",
      "        ПЕРС       0.74      0.34      0.47      3998\n",
      "        РІЗН       0.36      0.11      0.17       555\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     78043\n",
      "   macro avg       0.70      0.34      0.40     78043\n",
      "weighted avg       0.90      0.92      0.89     78043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rep = classification_report(norm_labels(test_labels), forecast)\n",
    "\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
