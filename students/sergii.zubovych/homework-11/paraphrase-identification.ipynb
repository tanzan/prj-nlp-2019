{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz --output numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gunzip numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "numberbatch = KeyedVectors.load_word2vec_format(\"numberbatch-en-17.06.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contravariant_functor', 0.9674091935157776),\n",
       " ('forgetful_functor', 0.9666687250137329),\n",
       " ('yoneda_embedding', 0.9497337937355042),\n",
       " ('endofunctor', 0.9360368847846985),\n",
       " ('representable_functor', 0.9314213991165161),\n",
       " ('cofunctor', 0.9296932220458984),\n",
       " ('natural_transformation', 0.9164144992828369),\n",
       " ('yoneda_lemma', 0.9022417664527893),\n",
       " ('coaugmentation', 0.8871707320213318),\n",
       " ('category_theory', 0.8751257658004761)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberbatch.most_similar(['functor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Phrase = namedtuple('Phrase', 'original candidate label')\n",
    "Token = namedtuple('Token', 'text tags')\n",
    "\n",
    "def split_tokens(sent):\n",
    "    tokens = []\n",
    "    for token in sent.split():\n",
    "        tags = token.split('/')\n",
    "        tokens.append(Token(tags[0].lower(), tuple(tags[1:])))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def readData(filename, eval_label, ignoreNone):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) == 7:\n",
    "                (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = fields\n",
    "            else:\n",
    "                continue\n",
    "            label = eval_label(judge)\n",
    "            if ((label is None) and ignoreNone):\n",
    "                continue\n",
    "            data.append(Phrase(split_tokens(origsenttag), split_tokens(candsenttag), label))\n",
    "    \n",
    "    return data\n",
    "                \n",
    "def eval_amt_label(label):\n",
    "    nYes = eval(label)[0]            \n",
    "    \n",
    "    if nYes >= 3:\n",
    "        return True\n",
    "    elif nYes <= 1:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "def eval_expert_label(label):\n",
    "    nYes = int(label[0])\n",
    "    \n",
    "    if nYes >= 4:\n",
    "        return True\n",
    "    elif nYes <= 2:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def readTrainData(filename):\n",
    "    return readData(filename, eval_amt_label, True)\n",
    "\n",
    "def readTestData(filename):\n",
    "    return readData(filename, eval_expert_label, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readTrainData(\"SemEval-PIT2015-py3/data/train.data\")\n",
    "dev_data = readTrainData(\"SemEval-PIT2015-py3/data/dev.data\")\n",
    "test_data = [p for p in readTestData(\"SemEval-PIT2015-py3/data/test.data\") if p.label is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='bro', tags=('O', 'NN', 'I-NP', 'O')), Token(text='from', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='757', tags=('O', 'CD', 'I-NP', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'I-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='gone', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='can', tags=('O', 'MD', 'B-VP', 'O')), Token(text='believe', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='went', tags=('O', 'VBD', 'B-VP', 'O')), Token(text='as', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='what', tags=('O', 'WP', 'I-NP', 'O'))], label=True)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='8', tags=('O', 'NN', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='thats', tags=('O', 'NNS', 'B-NP', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='movie', tags=('O', 'NN', 'I-NP', 'B-EVENT'))], label=None),\n",
       " Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='last', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='nevr', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='old', tags=('O', 'JJ', 'B-NP', 'O')), Token(text='ahah', tags=('O', 'JJ', 'I-NP', 'O'))], label=False),\n",
       " Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'O')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='at', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='end', tags=('O', 'NN', 'I-NP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='me', tags=('O', 'PRP', 'B-NP', 'O')), Token(text='so', tags=('O', 'RB', 'B-ADVP', 'O')), Token(text='hype', tags=('O', 'JJ', 'I-ADVP', 'O'))], label=False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_sent(sent):\n",
    "    new_sent = []\n",
    "    for token in sent:\n",
    "        if token.tags[0].startswith('B-'):\n",
    "            new_sent.append(token.tags[0].split('-')[1])\n",
    "            continue\n",
    "        if token.tags[0].startswith('I-') or token.text in stopwords.words('english'):\n",
    "            continue\n",
    "        new_sent.append(token.text)\n",
    "                            \n",
    "    return new_sent\n",
    "\n",
    "def clean_data(data):\n",
    "    return [Phrase(clean_sent(phrase.original), clean_sent(phrase.candidate), phrase.label) for phrase in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = clean_data(train_data)\n",
    "clean_dev_data = clean_data(train_data)\n",
    "clean_test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=['person', '1st', 'qb', 'go', 'draft'], candidate=['bro', '757', 'person', '1st', 'qb', 'gone'], label=True),\n",
       " Phrase(original=['person', '1st', 'qb', 'go', 'draft'], candidate=['believe', 'person', 'went', '1st', 'qb', 'draft'], label=True),\n",
       " Phrase(original=['person', '1st', 'qb', 'go', 'draft'], candidate=['person', '1st', 'qb'], label=True)]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7577"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def phrase_tokens(phrase):\n",
    "    return [token for token in (phrase.original + phrase.candidate)]\n",
    "    \n",
    "\n",
    "vocab = Dictionary([phrase_tokens(p) for p in clean_train_data + clean_dev_data + clean_test_data])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7578\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab) + 1 # +1 for padding\n",
    "\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11530\n",
      "11530\n",
      "11530\n"
     ]
    }
   ],
   "source": [
    "def data_to_sequences(data):\n",
    "    \n",
    "    encoder_seqs = []\n",
    "    decoder_seqs = []\n",
    "    labels = []\n",
    "    \n",
    "    for phrase in data:\n",
    "        encoder_seqs.append(text_to_sequence([t for t in phrase.original]))\n",
    "        decoder_seqs.append(text_to_sequence([t for t in phrase.candidate]))\n",
    "        labels.append(phrase.label)\n",
    "        \n",
    "    return encoder_seqs, decoder_seqs, labels \n",
    "\n",
    "train_encoder_seqs, train_decoder_seqs, train_labels = data_to_sequences(clean_train_data)\n",
    "\n",
    "print(len(train_encoder_seqs))\n",
    "print(len(train_decoder_seqs))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', '1st', 'qb', 'go', 'draft']\n",
      "['bro', '757', 'person', '1st', 'qb', 'gone']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_encoder_seqs[0]))\n",
    "print(sequence_to_text(train_decoder_seqs[0]))\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoder_seqs, dev_decoder_seqs, dev_labels = data_to_sequences(clean_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_seqs, test_decoder_seqs, test_labels = data_to_sequences(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max([len(seq) for seq in (train_encoder_seqs + train_decoder_seqs + \\\n",
    "                                       dev_encoder_seqs + dev_decoder_seqs + \\\n",
    "                                       test_decoder_seqs + test_encoder_seqs)])\n",
    "\n",
    "print(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(sequences):\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LEN, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1506\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_MATRIX = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "  \n",
    "missed = []\n",
    "for word, i in vocab.token2id.items():\n",
    "    try:\n",
    "        EMBEDDING_MATRIX[i] = numberbatch[word]\n",
    "    except KeyError:\n",
    "        missed.append(word)\n",
    "\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['757',\n",
       " 'fsu',\n",
       " 'sportsteam',\n",
       " '2013',\n",
       " 'noles',\n",
       " 'cbaire1',\n",
       " '17th',\n",
       " 'nfldraft',\n",
       " 'NONE',\n",
       " 'wasnt',\n",
       " '16',\n",
       " 'buffalobills',\n",
       " 'didnt',\n",
       " '50',\n",
       " 'tvshow',\n",
       " '2009',\n",
       " 'asf',\n",
       " '59th',\n",
       " 'mrsh',\n",
       " '59',\n",
       " 'thunderingherd',\n",
       " 'patriotsnation',\n",
       " 'weswelker',\n",
       " 'onehanded',\n",
       " '110m',\n",
       " 'isnt',\n",
       " '110mill',\n",
       " '110',\n",
       " '40',\n",
       " '5yr110',\n",
       " '5years',\n",
       " '40m',\n",
       " '40million',\n",
       " 'highestpaid',\n",
       " '666k',\n",
       " 'gezwxm87',\n",
       " '2325',\n",
       " '2325million',\n",
       " '22',\n",
       " '5year',\n",
       " '110million',\n",
       " 'baaaad',\n",
       " 'maaan',\n",
       " 'stephenasmith',\n",
       " 'bigmoney',\n",
       " 'espnnfcnblog',\n",
       " 'shiting',\n",
       " 'butler182',\n",
       " 'abbeyview',\n",
       " 'chicagobears',\n",
       " 'transportredoing',\n",
       " 'idealware',\n",
       " 'winemakers',\n",
       " 'alidoee',\n",
       " 'bmthofficial',\n",
       " 'aaaaaaaaaaaaaahahaha',\n",
       " 'raaaiiiiders',\n",
       " 'audiance',\n",
       " 'liveview',\n",
       " 'line2',\n",
       " 'functionalities',\n",
       " 'exok',\n",
       " 'n9',\n",
       " 'lutzenkirchen',\n",
       " 'americanidol',\n",
       " '17',\n",
       " '14',\n",
       " '9inning',\n",
       " '16th',\n",
       " '100',\n",
       " '9inn',\n",
       " 'alltime',\n",
       " 'careerhigh',\n",
       " '15ks',\n",
       " '12',\n",
       " '15',\n",
       " 'sbjsvsjwjw',\n",
       " 'fergies',\n",
       " 'ravensnation',\n",
       " 'kstate',\n",
       " 'avs',\n",
       " 'wildavs',\n",
       " '2000',\n",
       " 'sakic',\n",
       " 'blackstrapbbq',\n",
       " 'bbqbrolaws',\n",
       " 'nacwc',\n",
       " '21',\n",
       " 'againts',\n",
       " '7atit',\n",
       " 'lewandowski',\n",
       " 'barkevious',\n",
       " 'sweeeeeet',\n",
       " '37th',\n",
       " 'awwwww',\n",
       " '25th',\n",
       " 'eamaddennfl',\n",
       " 'ughhhhhhh',\n",
       " 'ayeee',\n",
       " '25',\n",
       " 'shouldnt',\n",
       " '13',\n",
       " 'yeaaaaaaaaaah',\n",
       " 'madden25',\n",
       " 'ayyee',\n",
       " 'babyyyyyyy',\n",
       " 'bisping',\n",
       " 'benzino',\n",
       " 'ugghe',\n",
       " '764',\n",
       " 'aint',\n",
       " 'punkd',\n",
       " 'bispingbelcher',\n",
       " 'frfr',\n",
       " 'shouldve',\n",
       " '10',\n",
       " 'griffinzach',\n",
       " 'zbo',\n",
       " 'breh',\n",
       " 'gasol',\n",
       " 'ohhhhhhh',\n",
       " 'turiaf',\n",
       " 'aaawww',\n",
       " 'fukk',\n",
       " 'doublefoul',\n",
       " 'margella',\n",
       " 'cuuuuuuuuute',\n",
       " 'unfollowed',\n",
       " 'doesnt',\n",
       " 'oiii',\n",
       " 'lordd',\n",
       " 'rewtardid',\n",
       " 'guarddddd',\n",
       " 'offfff',\n",
       " 'oomf',\n",
       " 'reallly',\n",
       " 'youuuuuu',\n",
       " 'thts',\n",
       " 'grindin',\n",
       " 'poppppin',\n",
       " 'tooo',\n",
       " 'acapella',\n",
       " 'alterego',\n",
       " '106',\n",
       " 'oomgggg',\n",
       " 'shittttt',\n",
       " 'looong',\n",
       " 'raunchiness',\n",
       " 'sleeppppp',\n",
       " '20',\n",
       " 'bogut',\n",
       " 'ssfootball',\n",
       " '34',\n",
       " 'ucl',\n",
       " 'borussia',\n",
       " '138',\n",
       " '97',\n",
       " 'hasnt',\n",
       " '00',\n",
       " '75',\n",
       " '10000',\n",
       " '40000',\n",
       " 'championsleague',\n",
       " 'bvb',\n",
       " 'klopp',\n",
       " '8000',\n",
       " '2014',\n",
       " '3pointer',\n",
       " 'nofloorseats',\n",
       " 'outchea',\n",
       " 'goooo',\n",
       " 'maddym17',\n",
       " 'heatbucks',\n",
       " 'watchn',\n",
       " 'kaykay032',\n",
       " 'turnt',\n",
       " 'zzzzzzzzzzzzzzzz',\n",
       " 'fantards',\n",
       " '62',\n",
       " '67',\n",
       " 'areana',\n",
       " 'miamibucks',\n",
       " 'twitchyteam',\n",
       " '30',\n",
       " 'naterobinson',\n",
       " 'itd',\n",
       " 'ashtonirwow',\n",
       " 'hiiiii',\n",
       " 'ilysm',\n",
       " 'fkn',\n",
       " 'x48',\n",
       " 'sososo',\n",
       " 'nerly',\n",
       " 'folllow',\n",
       " 'babycake',\n",
       " '04',\n",
       " 'youuuu',\n",
       " 'calum5sos',\n",
       " 'cutiecalum',\n",
       " 'heyyyitslizz',\n",
       " 'pleaseeeee',\n",
       " '5sosupdate',\n",
       " 'x39',\n",
       " 'almondnarry',\n",
       " 'calummmm',\n",
       " 'theirgroupie',\n",
       " 'x70',\n",
       " 'leahmclaren',\n",
       " 'togethermoms',\n",
       " '10th',\n",
       " 'sotheresthat',\n",
       " '3er',\n",
       " '444',\n",
       " 'derrota',\n",
       " 'sumisin',\n",
       " 'guilitine',\n",
       " '120',\n",
       " 'yammed',\n",
       " 'yamming',\n",
       " 'woaaaaah',\n",
       " 'lmfaooo',\n",
       " 'connorclark15',\n",
       " 'yammin',\n",
       " 'postered',\n",
       " 'boofed',\n",
       " 'dooed',\n",
       " 'duuuunked',\n",
       " 'podiumgame',\n",
       " 'and1',\n",
       " 'posterized',\n",
       " '35',\n",
       " '1035',\n",
       " '36',\n",
       " '24',\n",
       " 'dzhokhar',\n",
       " '28',\n",
       " 'nfldraftthat',\n",
       " 'kcchiefs',\n",
       " 'chael',\n",
       " 'sonnen',\n",
       " 'whopped',\n",
       " 'jonnybones',\n",
       " 'joneschael',\n",
       " '401',\n",
       " '2nyt',\n",
       " 'gotze',\n",
       " 'germansa',\n",
       " 'warmack',\n",
       " 'shmedium',\n",
       " '350',\n",
       " 'sumnerpaschke',\n",
       " '549',\n",
       " 'gaurd',\n",
       " 'rtr',\n",
       " 'bruhhhhhhhhh',\n",
       " 'dawgg',\n",
       " 'pittcon',\n",
       " 'subtchicago',\n",
       " 'aileybayrae',\n",
       " 'justinbieber',\n",
       " 'gdragon',\n",
       " 'pinkmartini',\n",
       " 'gds',\n",
       " '2011',\n",
       " 'analogie',\n",
       " 'timtebow',\n",
       " '1500espnreusse',\n",
       " 'startribune',\n",
       " '19th',\n",
       " '62nd',\n",
       " 'bowwowymcmb',\n",
       " 'babydaddy',\n",
       " 'perolawiberg',\n",
       " 'sexxyy',\n",
       " 'badd',\n",
       " '1future',\n",
       " 'mairs',\n",
       " 'hollldddd',\n",
       " 'knw',\n",
       " 'hitmansteviej',\n",
       " 'lawwwwdddd',\n",
       " 'famm',\n",
       " 'cinddyyrelaa',\n",
       " 'kingdont',\n",
       " 'niallofficial',\n",
       " 'abcfamily',\n",
       " '19',\n",
       " '1950',\n",
       " 'orginal',\n",
       " 'hotcole',\n",
       " 'snappin',\n",
       " 'samepage',\n",
       " 'rulegoverence',\n",
       " 'congressgovt',\n",
       " 'cordarelle',\n",
       " '29',\n",
       " 'dantoni',\n",
       " 'wantknows',\n",
       " 'gonn',\n",
       " '12th',\n",
       " 'raidernation',\n",
       " 'coog',\n",
       " 'uofh',\n",
       " 'wthe',\n",
       " 'dignit',\n",
       " 'bushcenter',\n",
       " '81st',\n",
       " '90',\n",
       " 'damontre',\n",
       " 'nyg',\n",
       " 'datone',\n",
       " 'jonesgreen',\n",
       " '51',\n",
       " 'amerson',\n",
       " 'bihhh',\n",
       " '51st',\n",
       " 'cbnc',\n",
       " 'joeyorck',\n",
       " 'httr',\n",
       " 'seleciona',\n",
       " 'redskinsnation',\n",
       " 'davidarchie',\n",
       " 'fangirls',\n",
       " 'livestream',\n",
       " '1110',\n",
       " 'davidsbackpack',\n",
       " '1100',\n",
       " '26',\n",
       " 'nebtd',\n",
       " 'wwtt',\n",
       " 'relyin',\n",
       " 'ayee',\n",
       " 'trufant',\n",
       " '22nd',\n",
       " 'nfldraft2013',\n",
       " 'forreal',\n",
       " 'westsidehell',\n",
       " 'dblock',\n",
       " '365',\n",
       " 'jobaint',\n",
       " 'gundogan',\n",
       " 'illkay',\n",
       " 'bmt',\n",
       " 'supposely',\n",
       " 'iker',\n",
       " 'goalkeepers',\n",
       " 'worldie',\n",
       " 'casilas',\n",
       " 'tht',\n",
       " 'qualitty',\n",
       " 'really2',\n",
       " 'awzum',\n",
       " 'todayhell',\n",
       " 'gundagun',\n",
       " 'wft',\n",
       " 'awww',\n",
       " 'checc',\n",
       " 'fangirling',\n",
       " 'tbt',\n",
       " 'omgggggggggggffhkeekneeieirbrvejriiehw',\n",
       " 'shoutout',\n",
       " 'lizziemcguire',\n",
       " 'teennick',\n",
       " 'throwbackthursday',\n",
       " 'nowwww',\n",
       " '122',\n",
       " 'schumaker',\n",
       " 'hahahahaha',\n",
       " 'dortmunds',\n",
       " '31',\n",
       " '41',\n",
       " 'humilate',\n",
       " 'oneoff',\n",
       " '90quid',\n",
       " 'bayerndortmund',\n",
       " 'dortmundreal',\n",
       " 'leandowski',\n",
       " 'shwd',\n",
       " 'dortmunddestruction',\n",
       " 'unbealivable',\n",
       " 'tattaz',\n",
       " '4pt',\n",
       " 'bullsvsnets',\n",
       " '127127',\n",
       " '123123',\n",
       " '55',\n",
       " 'at127',\n",
       " 'chitown',\n",
       " 'nbaplayoffs',\n",
       " 'netsbulls',\n",
       " 'fiya',\n",
       " 'loooove',\n",
       " 'dejando',\n",
       " 'pillo',\n",
       " 'nbaontnt',\n",
       " 'pourjeinventoryblowout',\n",
       " 'taureandirect',\n",
       " 'todoautosurplus',\n",
       " 'wayyyyy',\n",
       " 'harbaugh',\n",
       " '49ers',\n",
       " 'couldve',\n",
       " 'goldsons',\n",
       " 'ninersnation',\n",
       " 'superbound',\n",
       " '49erfam',\n",
       " 'lensfree',\n",
       " 'ansah',\n",
       " 'wout',\n",
       " 'real3d',\n",
       " 'lenseless',\n",
       " 'reald3d',\n",
       " 'reald',\n",
       " 'ahahahha',\n",
       " 'lmfaoooooooooo',\n",
       " '11',\n",
       " '130',\n",
       " 'meeeee',\n",
       " 'starwars',\n",
       " 'yellowbastard',\n",
       " 'realsummerwwe',\n",
       " '27',\n",
       " 'lobbin',\n",
       " 'alleyoop',\n",
       " 'rhythem',\n",
       " 'shumperts',\n",
       " 'foles',\n",
       " 'qbs',\n",
       " 'harrystyles',\n",
       " 'tmht',\n",
       " '1839',\n",
       " 'lattimore',\n",
       " 'lamicheal',\n",
       " '1133',\n",
       " 'cryy',\n",
       " 'spx',\n",
       " '23',\n",
       " 'protags',\n",
       " 'tde',\n",
       " 'gwb',\n",
       " 'liebary',\n",
       " 'hooha',\n",
       " 'thebushcenter',\n",
       " 'tearyeyed',\n",
       " '68',\n",
       " 'fromyale',\n",
       " 'noshow',\n",
       " 'michaelmoore',\n",
       " 'wowww',\n",
       " 'spursvslakers',\n",
       " 'celticsknicks',\n",
       " 'foul2',\n",
       " 'smmfh',\n",
       " 'tmrw',\n",
       " 'therealjrsmith',\n",
       " 'whhaaattt',\n",
       " 'ayeeee',\n",
       " 'bulllshit',\n",
       " 'recieving',\n",
       " 'fga',\n",
       " 'overunder',\n",
       " 'dammm',\n",
       " 'jrsmith',\n",
       " 'knicksboston',\n",
       " 'boutta',\n",
       " 'heatnationthat',\n",
       " 'dukies',\n",
       " 'thibs',\n",
       " 'whatevz',\n",
       " '11091',\n",
       " 'thecross87',\n",
       " 'soooooo',\n",
       " 'brooklynnets',\n",
       " 'seered',\n",
       " 'calisi',\n",
       " 'mtn',\n",
       " 'squeeeee',\n",
       " '1299',\n",
       " 'shaunwkeaveny',\n",
       " 'ohdont',\n",
       " 'nunggu',\n",
       " '1040',\n",
       " 'thoooouuugh',\n",
       " 'buttcentric',\n",
       " 'timeeee',\n",
       " 'ahahahaha',\n",
       " 'garrard',\n",
       " 'lmaoo',\n",
       " 'sanchize',\n",
       " 'btwn',\n",
       " 'competish',\n",
       " 'teraz',\n",
       " 'mccelroy',\n",
       " 'dellorto',\n",
       " '20m',\n",
       " 'defuk',\n",
       " 'kaepernick',\n",
       " '33',\n",
       " 'coolin',\n",
       " '39',\n",
       " '5050',\n",
       " 'ppl',\n",
       " 'heeeeeated',\n",
       " 'toinght',\n",
       " 'ahahah',\n",
       " '85',\n",
       " 'wowwww',\n",
       " 'boomd',\n",
       " 'deadass',\n",
       " 'sheeesssh',\n",
       " 'dirtyyyyyy',\n",
       " '2ndq',\n",
       " 'top10',\n",
       " 'ouu',\n",
       " 'haaaaaard',\n",
       " 'lawddd',\n",
       " 'dunkfaced',\n",
       " 'damnn',\n",
       " 'posturized',\n",
       " 'thefieldhouse',\n",
       " 'ooowee',\n",
       " 'iamshake',\n",
       " 'sportcenter',\n",
       " 'teamnovapetey',\n",
       " 'sumtin',\n",
       " 'steds',\n",
       " 'ww3',\n",
       " 'kachidgameboi',\n",
       " 'wnna',\n",
       " '81',\n",
       " '1937',\n",
       " '166',\n",
       " 'wgitmo',\n",
       " '2016',\n",
       " 'icloud',\n",
       " 'sciencegoogle',\n",
       " 'iphoneipad',\n",
       " 'disponible',\n",
       " 'iphone5',\n",
       " 'iphonehacks',\n",
       " 'searchs',\n",
       " 'zdnetgoogle',\n",
       " 'compell',\n",
       " 'goudelock',\n",
       " 'cofc',\n",
       " 'walkoff',\n",
       " 'beyonc',\n",
       " 'terminou',\n",
       " 'choreo',\n",
       " 'harvick',\n",
       " 'wellid',\n",
       " 'rcr',\n",
       " '400',\n",
       " 'laptimes',\n",
       " 'jpm',\n",
       " 'kb78',\n",
       " 'toyotaowners400',\n",
       " 'wowwwww',\n",
       " 'jpmontoya',\n",
       " 'wannnaaaa',\n",
       " 'fightshoving',\n",
       " 'nickiminaj',\n",
       " 'frikken',\n",
       " 'parkerighile',\n",
       " 'hollywoodcelebrity',\n",
       " 'chocolatevanilla',\n",
       " '752',\n",
       " 'askscandal',\n",
       " 'sosighit',\n",
       " 'higuain',\n",
       " 'phuck',\n",
       " 'leftfoot',\n",
       " 'rightfooted',\n",
       " 'poten',\n",
       " 'ccccrrrrraaaaazzzzzzyyyyyy',\n",
       " '3dganci',\n",
       " 'hihi',\n",
       " '175m',\n",
       " 'hellionvladimir',\n",
       " '1953',\n",
       " 'craaaazy',\n",
       " 'aaaand',\n",
       " 'farried',\n",
       " '21st',\n",
       " 'javale',\n",
       " 'sht',\n",
       " 'shaqtin',\n",
       " 'andrewbogut',\n",
       " 'tripledouble',\n",
       " 'mozgovd',\n",
       " 'warriorsnuggets',\n",
       " 'fbis',\n",
       " 'sctop10',\n",
       " '0o',\n",
       " '82',\n",
       " 'ilovecheese',\n",
       " '45',\n",
       " 'grandelaughs',\n",
       " 'tweetlimit',\n",
       " 'x36',\n",
       " '54',\n",
       " 'muchh',\n",
       " 'x1',\n",
       " '53',\n",
       " 'hayjdbieber',\n",
       " 'xx20',\n",
       " '243jaibrooks1',\n",
       " '05',\n",
       " 'jaibrooks1',\n",
       " 'slothanator',\n",
       " 'plase',\n",
       " 'trsrdy787876',\n",
       " 'mallys',\n",
       " '58',\n",
       " '76',\n",
       " 'pleeeeease',\n",
       " 'lyfe',\n",
       " 'plazzzzzzz',\n",
       " 'tomlinshiit',\n",
       " 'xxoxoxo',\n",
       " 'iilovecheese',\n",
       " 'pleasee',\n",
       " 'follw',\n",
       " 'kakam',\n",
       " 'anyssaayala',\n",
       " 'itzjessicahoex3',\n",
       " 'x32',\n",
       " 'rted',\n",
       " 'x73',\n",
       " 'eole',\n",
       " 'patrickquirky',\n",
       " 'ttlyteala',\n",
       " '70',\n",
       " '86',\n",
       " '52',\n",
       " '52nd',\n",
       " 'olb',\n",
       " '60th',\n",
       " '65',\n",
       " 'selecionou',\n",
       " 'usm',\n",
       " 'nepick',\n",
       " 'cstodd89',\n",
       " 'studentprintz',\n",
       " 'usmalumni',\n",
       " 'mikereiss',\n",
       " 'pissin',\n",
       " '2001',\n",
       " 'lmfaoo',\n",
       " 'shiited',\n",
       " 'hmmmmmmmmmmm',\n",
       " 'mhmm',\n",
       " 'hawtest',\n",
       " 'sidehug',\n",
       " 'foxyashell',\n",
       " 'yeen',\n",
       " 'joaning',\n",
       " 'athome',\n",
       " 'joanrivers',\n",
       " 'celebrityapprentice',\n",
       " 'halfassed',\n",
       " 'acually',\n",
       " 'dh2',\n",
       " '11s',\n",
       " 'getglue',\n",
       " 'rg3',\n",
       " '85th',\n",
       " '3pts',\n",
       " 'rentre',\n",
       " 'sigb',\n",
       " 'nxt',\n",
       " 'seau',\n",
       " 'sandiego',\n",
       " 'sdchargers',\n",
       " 'syas',\n",
       " 'vols',\n",
       " 'fantasyfootball',\n",
       " 'ayyy',\n",
       " 'wahoooo',\n",
       " 'tenessee',\n",
       " '29th',\n",
       " '34th',\n",
       " 'nygiants',\n",
       " 'trex',\n",
       " 'otog',\n",
       " 'olineman',\n",
       " 'beattheblock',\n",
       " 'khedndiby',\n",
       " 'whysomad',\n",
       " 'whodatnation',\n",
       " 'dreamchasin23',\n",
       " '15th',\n",
       " '1961',\n",
       " 'ibaka',\n",
       " 'khali',\n",
       " 'blakegriffin32',\n",
       " 'sooooooo',\n",
       " 'killllllll',\n",
       " 'lolll',\n",
       " 'fuggin',\n",
       " 'griffens',\n",
       " 'uuuuup',\n",
       " 'hahahah',\n",
       " 'commericals',\n",
       " 'commericials',\n",
       " 'any1',\n",
       " 'dayummmmmm',\n",
       " 'looooool',\n",
       " '20112012',\n",
       " 'sactown',\n",
       " 'purplearmy',\n",
       " 'ziggler',\n",
       " 'roynelsonmma',\n",
       " 'cheik',\n",
       " 'fukked',\n",
       " 'kuntry',\n",
       " '203',\n",
       " 'r1',\n",
       " 'fck',\n",
       " 'cheick',\n",
       " 'chieck',\n",
       " 'wowwwwww',\n",
       " 'knocced',\n",
       " 'fugg',\n",
       " 'outtttt',\n",
       " 'knockes',\n",
       " 'kod',\n",
       " 'uo',\n",
       " '115',\n",
       " '115th',\n",
       " 'dallascowboy',\n",
       " 'jptheasshole',\n",
       " '48th',\n",
       " 'steelernation',\n",
       " '44',\n",
       " 'coentrao',\n",
       " 'acabo',\n",
       " 'buscar',\n",
       " 'utilizar',\n",
       " 'reggaetton',\n",
       " 'duuude',\n",
       " 'devaneykk',\n",
       " 'watchung',\n",
       " 'poppycorn',\n",
       " '1810',\n",
       " 'yday',\n",
       " 'haaaaang',\n",
       " '11th',\n",
       " 'lucic',\n",
       " 'dagauvins',\n",
       " 'turris',\n",
       " 'mainevent',\n",
       " 'loveeee',\n",
       " 'knicksnation',\n",
       " 'aite',\n",
       " 'barackobama',\n",
       " 'yannah',\n",
       " 'twaz',\n",
       " 'yesto',\n",
       " 'bpl',\n",
       " '101',\n",
       " '1h',\n",
       " 'fucktheotherteams',\n",
       " 'awal',\n",
       " 'baru',\n",
       " 'babak1',\n",
       " 'louding',\n",
       " '24m',\n",
       " 'ggmu',\n",
       " 'nonton',\n",
       " 'drez4prez',\n",
       " '864',\n",
       " 'niceeeee',\n",
       " 'glab',\n",
       " '9ers',\n",
       " '131st',\n",
       " 'lemonier',\n",
       " 'dpbrugler',\n",
       " 'sheamus',\n",
       " '32nd',\n",
       " 'ravensplus',\n",
       " 'untouchablejay4',\n",
       " 'firstdraft',\n",
       " 'hardhitting',\n",
       " 'burgandy',\n",
       " 'bestfriend',\n",
       " 'trynna',\n",
       " 'pancackes',\n",
       " 'saramcmann',\n",
       " 'congratrs',\n",
       " 'frankcaliendo',\n",
       " 'kiper',\n",
       " 'caliendo',\n",
       " 'espns',\n",
       " 'caliendos',\n",
       " 'lmaoooooooo',\n",
       " 'fittz',\n",
       " 'ilysfmluke',\n",
       " 'replyfollow',\n",
       " 'ammieee94',\n",
       " 'indirecting',\n",
       " 'legendim',\n",
       " 'popsbeadles',\n",
       " 'michael5sos',\n",
       " 'burritolirry',\n",
       " 'sumthin',\n",
       " 'nbahall',\n",
       " 'hoez',\n",
       " 'bandwagoners',\n",
       " '4541',\n",
       " 'montaged',\n",
       " 'fukin',\n",
       " 'mitad',\n",
       " 'nyknicks',\n",
       " 'vakation',\n",
       " 'uglass',\n",
       " 'hiphop',\n",
       " 'homewreaker',\n",
       " 'womancrushwednesday',\n",
       " 'thaat',\n",
       " 'thoe',\n",
       " 'kingdomlion',\n",
       " 'sewins',\n",
       " 'tuneinradio',\n",
       " 'bkst',\n",
       " 'chivettes',\n",
       " 'shittest',\n",
       " 'sweeped',\n",
       " 'taemin',\n",
       " 'delaplaine',\n",
       " 'goodasgold',\n",
       " 'alskiiii',\n",
       " 'righttt',\n",
       " 'txt',\n",
       " 'hienas',\n",
       " '10874468753',\n",
       " 'tearyeyes',\n",
       " '10x',\n",
       " 'sportsnation',\n",
       " '2010',\n",
       " 'elliedelancey',\n",
       " 'zoovie',\n",
       " 'runnerrrgirl1',\n",
       " '18th',\n",
       " '1x1',\n",
       " 'anddd',\n",
       " '1981',\n",
       " '2004',\n",
       " 'comcast',\n",
       " 'netflixinstant',\n",
       " 'okk',\n",
       " 'cawfee',\n",
       " 'absolutley',\n",
       " 'dbz',\n",
       " 'whhhhaaaat',\n",
       " 'redbox',\n",
       " 'bobsaget',\n",
       " 'hby',\n",
       " 'surfthechannel',\n",
       " 'rustyrockets',\n",
       " 'tomgreenlive',\n",
       " 'thexx',\n",
       " 'brookelyn',\n",
       " 'sumall',\n",
       " 'bloomsburg',\n",
       " 'oneman',\n",
       " 'bakk',\n",
       " 'concievec',\n",
       " 'wmy',\n",
       " 'appli',\n",
       " 'yorkbased',\n",
       " 'theoph',\n",
       " 'twizy',\n",
       " 'safirah',\n",
       " 'yishun',\n",
       " 'rbatto',\n",
       " 'liamlbdr',\n",
       " 'therock',\n",
       " 'tradi',\n",
       " 'comjng',\n",
       " 'lnow',\n",
       " '125',\n",
       " 'sputtered',\n",
       " '4500',\n",
       " 'dieit',\n",
       " 'rabie',\n",
       " '1988',\n",
       " 'mickjagger',\n",
       " 'dism',\n",
       " 'latenight',\n",
       " 'yorka',\n",
       " 'nialler',\n",
       " 'twitcam',\n",
       " 'anyhtjjng',\n",
       " 'couldshould',\n",
       " 'respusha',\n",
       " 'llf',\n",
       " 'wudda',\n",
       " '3ball',\n",
       " 'gigure',\n",
       " 'shittin',\n",
       " 'okc',\n",
       " 'eeeeverything',\n",
       " 'solaveiregionals',\n",
       " 'prespinning',\n",
       " 'mosnter',\n",
       " 'amaazinggg',\n",
       " 'wondercon',\n",
       " 'rimjob',\n",
       " 'poopyourtrousers',\n",
       " 'bitsoftheworldwhichsoundfunny',\n",
       " 'jox',\n",
       " '123',\n",
       " 'reli',\n",
       " 'yestrdy',\n",
       " 'tyrann',\n",
       " 'singledigit',\n",
       " 'samething',\n",
       " 'wrf',\n",
       " 'glenperkins',\n",
       " 'ugliests',\n",
       " '7370',\n",
       " 'itsstephsoricex',\n",
       " '630',\n",
       " 'knowitall',\n",
       " 'backtoback',\n",
       " 'watchibg',\n",
       " 'oldschool',\n",
       " 'magalhaes',\n",
       " '205',\n",
       " 'bacarri',\n",
       " '980',\n",
       " '119th',\n",
       " 'sfresno',\n",
       " 'aize',\n",
       " 'livng',\n",
       " 'tweetgrubes',\n",
       " 'comparingcontrasting',\n",
       " 'expresidents',\n",
       " 'saaanging',\n",
       " 'ceremon',\n",
       " '128th',\n",
       " '128',\n",
       " '31st',\n",
       " '2ndround',\n",
       " 'lavergne',\n",
       " '49',\n",
       " 'fcked',\n",
       " '131',\n",
       " 'rvp',\n",
       " 'lewondoski',\n",
       " 'wilshere',\n",
       " 'shiii',\n",
       " '03',\n",
       " 'thoo',\n",
       " 'thrist',\n",
       " '2day',\n",
       " '745',\n",
       " 'jux',\n",
       " 'madriddortmund',\n",
       " 'borrusia',\n",
       " 'dourtmound',\n",
       " 'semfinal',\n",
       " 'kena',\n",
       " 'dortmundcant',\n",
       " 'bernabeu',\n",
       " '175',\n",
       " 'asik',\n",
       " 'darkskin',\n",
       " 'eeeeeevvvviiiilll',\n",
       " 'reginasnow',\n",
       " 'ughhh',\n",
       " 'varane',\n",
       " 'nahh',\n",
       " 'thurmanthomas',\n",
       " 'ahahaha',\n",
       " '41st',\n",
       " 'cr7',\n",
       " 'modric',\n",
       " 'miscontrol',\n",
       " 'outshit',\n",
       " 'balotelli',\n",
       " 'ozil',\n",
       " 'hopegrace10',\n",
       " 'trys',\n",
       " 'mesut',\n",
       " 'hahahahah',\n",
       " 'hellthat',\n",
       " 'someonesomething',\n",
       " 'stacey79',\n",
       " 'wooooooooow',\n",
       " 'mcl',\n",
       " 'muniscis',\n",
       " 'russwest44',\n",
       " 'playofs',\n",
       " 'nbaguru',\n",
       " 'mensiscus',\n",
       " 'fuuuuun',\n",
       " 'indirects',\n",
       " 'nassib',\n",
       " ...]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1987594034578329"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missed)/len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_missed = [p for p in train_data for t in (p.original + p.candidate) if t.text in missed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83642671292281"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_missed)/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                            output_dim = EMBEDDING_SIZE,\n",
    "                            input_length = MAX_SEQ_LEN,\n",
    "                            weights = [EMBEDDING_MATRIX], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 13, 300)      2273400     input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  [(None, 300), (None, 721200      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  [(None, 300), (None, 721200      embedding_4[1][0]                \n",
      "                                                                 lstm_36[0][1]                    \n",
      "                                                                 lstm_36[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            301         lstm_37[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,716,101\n",
      "Trainable params: 1,442,701\n",
      "Non-trainable params: 2,273,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, TimeDistributed, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "encoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "encoder_embedding = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "decoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "decoder_embedding = embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "outputs = Dense(1, activation='sigmoid')(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10377 samples, validate on 1153 samples\n",
      "Epoch 1/5\n",
      "10377/10377 [==============================] - 20s 2ms/step - loss: 0.6383 - acc: 0.6517 - val_loss: 0.6427 - val_acc: 0.6314\n",
      "Epoch 2/5\n",
      "10377/10377 [==============================] - 13s 1ms/step - loss: 0.6035 - acc: 0.6815 - val_loss: 0.6334 - val_acc: 0.6314\n",
      "Epoch 3/5\n",
      "10377/10377 [==============================] - 13s 1ms/step - loss: 0.5748 - acc: 0.7104 - val_loss: 0.6560 - val_acc: 0.6383\n",
      "Epoch 4/5\n",
      "10377/10377 [==============================] - 13s 1ms/step - loss: 0.5515 - acc: 0.7305 - val_loss: 0.6855 - val_acc: 0.6349\n",
      "Epoch 5/5\n",
      "10377/10377 [==============================] - 13s 1ms/step - loss: 0.5394 - acc: 0.7394 - val_loss: 0.6512 - val_acc: 0.6392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159630208>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 5, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predicted = [prob > 0.5 for prob in model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.89      0.82      7534\n",
      "        True       0.69      0.47      0.56      3996\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     11530\n",
      "   macro avg       0.72      0.68      0.69     11530\n",
      "weighted avg       0.73      0.74      0.73     11530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dev_labels, labels_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import LSTM, Lambda, concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "HIDDEN_DIM=100\n",
    "\n",
    "def exponent_neg_manhattan_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    return K.exp(-K.sum(K.abs(x[:,:hidden_size] - x[:,hidden_size:]), axis=1, keepdims=True))\n",
    "\n",
    "def exponent_neg_cosine_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    leftNorm = K.l2_normalize(x[:,:hidden_size], axis=-1)\n",
    "    rightNorm = K.l2_normalize(x[:,hidden_size:], axis=-1)\n",
    "    return K.exp(K.sum(K.prod([leftNorm, rightNorm], axis=0), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence1 (InputLayer)          (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence2 (InputLayer)          (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 13, 300)      2273400     sequence1[0][0]                  \n",
      "                                                                 sequence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 100)          160400      embedding_4[12][0]               \n",
      "                                                                 embedding_4[13][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 200)          0           lstm_43[0][0]                    \n",
      "                                                                 lstm_43[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 1)            0           concatenate_27[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,433,800\n",
      "Trainable params: 160,400\n",
      "Non-trainable params: 2,273,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_1 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence1')\n",
    "seq_2 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence2')\n",
    "\n",
    "input_1 = embedding_layer(seq_1)\n",
    "input_2 = embedding_layer(seq_2)\n",
    "\n",
    "l1 = LSTM(units=HIDDEN_DIM)\n",
    "\n",
    "l1_out = l1(input_1)\n",
    "l2_out = l1(input_2)\n",
    "\n",
    "concats = concatenate([l1_out, l2_out], axis=-1)\n",
    "\n",
    "#main_output = Lambda(exponent_neg_cosine_distance, output_shape=(1,))(concats)\n",
    "main_output = Lambda(exponent_neg_manhattan_distance, output_shape=(1,))(concats)\n",
    "#dense_ouput = Dense(1024, activation=\"relu\")(concats)\n",
    "#main_output = Dense(1, activation=\"sigmoid\")(dense_ouput)\n",
    "\n",
    "model = Model(inputs=[seq_1, seq_2], outputs=[main_output])\n",
    "\n",
    "opt = keras.optimizers.Adadelta(lr = 0.1, clipnorm=1.25)\n",
    "\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=opt,loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10377 samples, validate on 1153 samples\n",
      "Epoch 1/10\n",
      "10377/10377 [==============================] - 12s 1ms/step - loss: 0.3134 - acc: 0.4860 - val_loss: 0.3088 - val_acc: 0.5421\n",
      "Epoch 2/10\n",
      "10377/10377 [==============================] - 4s 353us/step - loss: 0.2743 - acc: 0.5694 - val_loss: 0.2785 - val_acc: 0.5872\n",
      "Epoch 3/10\n",
      "10377/10377 [==============================] - 4s 351us/step - loss: 0.2560 - acc: 0.5976 - val_loss: 0.2649 - val_acc: 0.6028\n",
      "Epoch 4/10\n",
      "10377/10377 [==============================] - 4s 350us/step - loss: 0.2455 - acc: 0.6133 - val_loss: 0.2580 - val_acc: 0.6088\n",
      "Epoch 5/10\n",
      "10377/10377 [==============================] - 4s 349us/step - loss: 0.2364 - acc: 0.6257 - val_loss: 0.2499 - val_acc: 0.6054\n",
      "Epoch 6/10\n",
      "10377/10377 [==============================] - 4s 351us/step - loss: 0.2287 - acc: 0.6370 - val_loss: 0.2465 - val_acc: 0.6132\n",
      "Epoch 7/10\n",
      "10377/10377 [==============================] - 4s 365us/step - loss: 0.2226 - acc: 0.6503 - val_loss: 0.2447 - val_acc: 0.6141\n",
      "Epoch 8/10\n",
      "10377/10377 [==============================] - 4s 343us/step - loss: 0.2173 - acc: 0.6585 - val_loss: 0.2439 - val_acc: 0.6279\n",
      "Epoch 9/10\n",
      "10377/10377 [==============================] - 4s 352us/step - loss: 0.2125 - acc: 0.6659 - val_loss: 0.2446 - val_acc: 0.6236\n",
      "Epoch 10/10\n",
      "10377/10377 [==============================] - 4s 353us/step - loss: 0.2079 - acc: 0.6754 - val_loss: 0.2464 - val_acc: 0.6236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc26b550>"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 10, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_similarity = model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.70      0.90      0.79      7534\n",
      "        True       0.59      0.26      0.36      3996\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     11530\n",
      "   macro avg       0.64      0.58      0.57     11530\n",
      "weighted avg       0.66      0.68      0.64     11530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_labels, [prob > 0.5 for prob in predicted_similarity]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7424978317432784"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(dev_labels, [prob > 0.5 for prob in predicted_similarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
