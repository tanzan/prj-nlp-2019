{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz --output numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gunzip numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "numberbatch = KeyedVectors.load_word2vec_format(\"numberbatch-en-17.06.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contravariant_functor', 0.9674091935157776),\n",
       " ('forgetful_functor', 0.9666687250137329),\n",
       " ('yoneda_embedding', 0.9497337937355042),\n",
       " ('endofunctor', 0.9360368847846985),\n",
       " ('representable_functor', 0.9314213991165161),\n",
       " ('cofunctor', 0.9296932220458984),\n",
       " ('natural_transformation', 0.9164144992828369),\n",
       " ('yoneda_lemma', 0.9022417664527893),\n",
       " ('coaugmentation', 0.8871707320213318),\n",
       " ('category_theory', 0.8751257658004761)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberbatch.most_similar(['functor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Phrase = namedtuple('Phrase', 'original candidate label')\n",
    "Token = namedtuple('Token', 'text tags')\n",
    "\n",
    "def split_tokens(sent):\n",
    "    tokens = []\n",
    "    for token in sent.split():\n",
    "        tags = token.split('/')\n",
    "        tokens.append(Token(tags[0].lower(), tuple(tags[1:])))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def readData(filename, eval_label, ignoreNone):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) == 7:\n",
    "                (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = fields\n",
    "            else:\n",
    "                continue\n",
    "            label = eval_label(judge)\n",
    "            if ((label is None) and ignoreNone):\n",
    "                continue\n",
    "            data.append(Phrase(split_tokens(origsenttag), split_tokens(candsenttag), label))\n",
    "    \n",
    "    return data\n",
    "                \n",
    "def eval_amt_label(label):\n",
    "    nYes = eval(label)[0]            \n",
    "    \n",
    "    if nYes >= 3:\n",
    "        return True\n",
    "    elif nYes <= 1:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "def eval_expert_label(label):\n",
    "    nYes = int(label[0])\n",
    "    \n",
    "    if nYes >= 4:\n",
    "        return True\n",
    "    elif nYes <= 2:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def readTrainData(filename):\n",
    "    return readData(filename, eval_amt_label, True)\n",
    "\n",
    "def readTestData(filename):\n",
    "    return readData(filename, eval_expert_label, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readTrainData(\"SemEval-PIT2015-py3/data/train.data\")\n",
    "dev_data = readTrainData(\"SemEval-PIT2015-py3/data/dev.data\")\n",
    "test_data = [p for p in readTestData(\"SemEval-PIT2015-py3/data/test.data\") if p.label is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='bro', tags=('O', 'NN', 'I-NP', 'O')), Token(text='from', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='757', tags=('O', 'CD', 'I-NP', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'I-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='gone', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='can', tags=('O', 'MD', 'B-VP', 'O')), Token(text='believe', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='went', tags=('O', 'VBD', 'B-VP', 'O')), Token(text='as', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='what', tags=('O', 'WP', 'I-NP', 'O'))], label=True)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='8', tags=('O', 'NN', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='thats', tags=('O', 'NNS', 'B-NP', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='movie', tags=('O', 'NN', 'I-NP', 'B-EVENT'))], label=None),\n",
       " Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='last', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='nevr', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='old', tags=('O', 'JJ', 'B-NP', 'O')), Token(text='ahah', tags=('O', 'JJ', 'I-NP', 'O'))], label=False),\n",
       " Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'O')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='at', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='end', tags=('O', 'NN', 'I-NP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='me', tags=('O', 'PRP', 'B-NP', 'O')), Token(text='so', tags=('O', 'RB', 'B-ADVP', 'O')), Token(text='hype', tags=('O', 'JJ', 'I-ADVP', 'O'))], label=False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "\n",
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10841"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def phrase_tokens(phrase):\n",
    "    return [token.text for token in (phrase.original + phrase.candidate)]\n",
    "    \n",
    "\n",
    "vocab = Dictionary([phrase_tokens(p) for p in train_data + dev_data + test_data])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10842\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab) + 1 # +1 for padding\n",
    "\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11530\n",
      "11530\n",
      "11530\n"
     ]
    }
   ],
   "source": [
    "def data_to_sequences(data):\n",
    "    \n",
    "    encoder_seqs = []\n",
    "    decoder_seqs = []\n",
    "    labels = []\n",
    "    \n",
    "    for phrase in data:\n",
    "        encoder_seqs.append(text_to_sequence([t.text for t in phrase.original]))\n",
    "        decoder_seqs.append(text_to_sequence([t.text for t in phrase.candidate]))\n",
    "        labels.append(phrase.label)\n",
    "        \n",
    "    return encoder_seqs, decoder_seqs, labels \n",
    "\n",
    "train_encoder_seqs, train_decoder_seqs, train_labels = data_to_sequences(train_data)\n",
    "\n",
    "print(len(train_encoder_seqs))\n",
    "print(len(train_decoder_seqs))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ej', 'manuel', 'the', '1st', 'qb', 'to', 'go', 'in', 'this', 'draft']\n",
      "['but', 'my', 'bro', 'from', 'the', '757', 'ej', 'manuel', 'is', 'the', '1st', 'qb', 'gone']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_encoder_seqs[0]))\n",
    "print(sequence_to_text(train_decoder_seqs[0]))\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoder_seqs, dev_decoder_seqs, dev_labels = data_to_sequences(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_seqs, test_decoder_seqs, test_labels = data_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max([len(seq) for seq in (train_encoder_seqs + train_decoder_seqs + \\\n",
    "                                       dev_encoder_seqs + dev_decoder_seqs + \\\n",
    "                                       test_decoder_seqs + test_encoder_seqs)])\n",
    "\n",
    "print(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(sequences):\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LEN, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2446\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_MATRIX = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "  \n",
    "missed = []\n",
    "for word, i in vocab.token2id.items():\n",
    "    try:\n",
    "        EMBEDDING_MATRIX[i] = numberbatch[word]\n",
    "    except KeyError:\n",
    "        missed.append(word)\n",
    "\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['757',\n",
       " 'fsu',\n",
       " 'qbflorida',\n",
       " '2013',\n",
       " 'noles',\n",
       " 'cbaire1',\n",
       " '17th',\n",
       " 'nfldraft',\n",
       " 'wasnt',\n",
       " '16',\n",
       " 'buffalobills',\n",
       " 'didnt',\n",
       " 'yoooooo',\n",
       " '50',\n",
       " '2009',\n",
       " 'asf',\n",
       " 'randymoss',\n",
       " '59th',\n",
       " 'mrsh',\n",
       " '59',\n",
       " 'kiper',\n",
       " 'mcshay',\n",
       " 'wrmarshall',\n",
       " 'thunderingherd',\n",
       " 'patriotsnation',\n",
       " 'weswelker',\n",
       " 'onehanded',\n",
       " '110m',\n",
       " 'isnt',\n",
       " '110mill',\n",
       " '110',\n",
       " '40',\n",
       " '5yr110',\n",
       " '5years',\n",
       " '40m',\n",
       " '40million',\n",
       " 'highestpaid',\n",
       " '666k',\n",
       " 'gezwxm87',\n",
       " '2325',\n",
       " '2325million',\n",
       " '22',\n",
       " '5year',\n",
       " '110million',\n",
       " 'baaaad',\n",
       " 'maaan',\n",
       " 'stephenasmith',\n",
       " 'bigmoney',\n",
       " 'espnnfcnblog',\n",
       " 'shiting',\n",
       " 'butler182',\n",
       " 'abbeyview',\n",
       " 'chicagobears',\n",
       " 'swearinger',\n",
       " 'transportredoing',\n",
       " 'idealware',\n",
       " 'winemakers',\n",
       " 'alidoee',\n",
       " 'bmthofficial',\n",
       " 'aaaaaaaaaaaaaahahaha',\n",
       " 'raaaiiiiders',\n",
       " 'nickiminaj',\n",
       " 'audiance',\n",
       " 'audley',\n",
       " 'liveview',\n",
       " 'line2',\n",
       " 'functionalities',\n",
       " 'exok',\n",
       " 'n9',\n",
       " 'lutzenkirchen',\n",
       " 'americanidol',\n",
       " '17',\n",
       " '14',\n",
       " '9inning',\n",
       " '16th',\n",
       " 'comerica',\n",
       " '10',\n",
       " 'kkkkkkkkkkkkkkkkk',\n",
       " '100',\n",
       " '9inn',\n",
       " 'alltime',\n",
       " 'careerhigh',\n",
       " 'ezmode',\n",
       " '15ks',\n",
       " '12',\n",
       " '15',\n",
       " 'sbjsvsjwjw',\n",
       " 'fergies',\n",
       " 'ravensnation',\n",
       " 'kstate',\n",
       " 'avs',\n",
       " 'wildavs',\n",
       " '2000',\n",
       " 'sakic',\n",
       " 'blackstrapbbq',\n",
       " 'bbqbrolaws',\n",
       " 'nacwc',\n",
       " '21',\n",
       " 'againts',\n",
       " '7atit',\n",
       " 'lewandowski',\n",
       " 'barkevious',\n",
       " 'sweeeeeet',\n",
       " '37th',\n",
       " 'awwwww',\n",
       " '25th',\n",
       " 'eamaddennfl',\n",
       " 'ughhhhhhh',\n",
       " '2014',\n",
       " 'ayeee',\n",
       " '25',\n",
       " 'shouldnt',\n",
       " '13',\n",
       " 'yeaaaaaaaaaah',\n",
       " 'madden25',\n",
       " 'ayyee',\n",
       " 'babyyyyyyy',\n",
       " 'bisping',\n",
       " 'benzino',\n",
       " 'ugghe',\n",
       " '764',\n",
       " 'aint',\n",
       " 'punkd',\n",
       " 'cheick',\n",
       " 'bispingbelcher',\n",
       " 'frfr',\n",
       " 'shouldve',\n",
       " 'griffinzach',\n",
       " 'zbo',\n",
       " 'gasol',\n",
       " 'breh',\n",
       " 'ohhhhhhh',\n",
       " 'turiaf',\n",
       " 'aaawww',\n",
       " 'macbo50',\n",
       " 'fukk',\n",
       " 'doublefoul',\n",
       " 'margella',\n",
       " 'awwwwwwwwwwwww',\n",
       " 'cuuuuuuuuute',\n",
       " 'unfollowed',\n",
       " 'doesnt',\n",
       " 'oiii',\n",
       " 'lordd',\n",
       " 'rewtardid',\n",
       " 'guarddddd',\n",
       " 'offfff',\n",
       " 'jazze',\n",
       " 'oomf',\n",
       " 'reallly',\n",
       " 'youuuuuu',\n",
       " 'joseline',\n",
       " 'thts',\n",
       " 'grindin',\n",
       " 'poppppin',\n",
       " 'tooo',\n",
       " 'acapella',\n",
       " 'alterego',\n",
       " '106',\n",
       " 'oomgggg',\n",
       " 'shittttt',\n",
       " 'looong',\n",
       " 'raunchiness',\n",
       " 'lawdddd',\n",
       " 'mannnn',\n",
       " 'sleeppppp',\n",
       " 'okayla',\n",
       " '20',\n",
       " 'bogut',\n",
       " 'borussia',\n",
       " 'ssfootball',\n",
       " '34',\n",
       " 'ucl',\n",
       " '138',\n",
       " '97',\n",
       " 'hasnt',\n",
       " '00',\n",
       " '75',\n",
       " 'bvb',\n",
       " '10000',\n",
       " '40000',\n",
       " 'itv1',\n",
       " 'subotic',\n",
       " 'munchen',\n",
       " 'championsleague',\n",
       " 'klopp',\n",
       " '8000',\n",
       " '3pointer',\n",
       " 'game5',\n",
       " 'gobrooklyn',\n",
       " 'netslife',\n",
       " 'nofloorseats',\n",
       " 'outchea',\n",
       " 'goooo',\n",
       " 'maddym17',\n",
       " 'heatbucks',\n",
       " 'watchn',\n",
       " 'kingjames',\n",
       " 'kaykay032',\n",
       " 'realskipbayless',\n",
       " 'motd2',\n",
       " 'claymaker',\n",
       " 'bucksheat',\n",
       " 'turnt',\n",
       " 'zzzzzzzzzzzzzzzz',\n",
       " 'miamiheat',\n",
       " 'fantards',\n",
       " '62',\n",
       " '67',\n",
       " 'areana',\n",
       " 'miamibucks',\n",
       " 'epl',\n",
       " 'bundesligas',\n",
       " 'twitchyteam',\n",
       " '30',\n",
       " 'naterobinson',\n",
       " 'itd',\n",
       " 'ashtonirwow',\n",
       " 'hiiiii',\n",
       " 'ilysm',\n",
       " 'fkn',\n",
       " 'x48',\n",
       " 'sososo',\n",
       " 'nerly',\n",
       " 'folllow',\n",
       " 'babycake',\n",
       " '04',\n",
       " 'youuuu',\n",
       " 'calum5sos',\n",
       " 'cutiecalum',\n",
       " 'heyyyitslizz',\n",
       " 'pleaseeeee',\n",
       " '5sosupdate',\n",
       " 'x39',\n",
       " 'almondnarry',\n",
       " 'calummmm',\n",
       " 'theirgroupie',\n",
       " 'x70',\n",
       " 'leahmclaren',\n",
       " 'togethermoms',\n",
       " '10th',\n",
       " 'sotheresthat',\n",
       " '109',\n",
       " '3er',\n",
       " '444',\n",
       " 'derrota',\n",
       " 'sumisin',\n",
       " 'guilitine',\n",
       " 'espnuk',\n",
       " '120',\n",
       " 'yammed',\n",
       " 'yamming',\n",
       " 'woaaaaah',\n",
       " 'lmfaooo',\n",
       " 'connorclark15',\n",
       " 'darrenevanss',\n",
       " 'yammin',\n",
       " 'postered',\n",
       " 'durants',\n",
       " 'boofed',\n",
       " 'dooed',\n",
       " 'duuuunked',\n",
       " 'podiumgame',\n",
       " 'and1',\n",
       " 'posterized',\n",
       " '35',\n",
       " '1035',\n",
       " '36',\n",
       " '24',\n",
       " 'dzhokhar',\n",
       " '28',\n",
       " 'nfldraftthat',\n",
       " 'kcchiefs',\n",
       " 'otcentral',\n",
       " 'macsports',\n",
       " 'chael',\n",
       " 'sonnen',\n",
       " 'whopped',\n",
       " 'jonnybones',\n",
       " 'joneschael',\n",
       " '401',\n",
       " '2nyt',\n",
       " 'elclasico',\n",
       " 'gotze',\n",
       " 'germansa',\n",
       " 'warmack',\n",
       " 'levitre',\n",
       " 'galabama',\n",
       " 'rolltide',\n",
       " 'shmedium',\n",
       " '350',\n",
       " 'midday180',\n",
       " 'sumnerpaschke',\n",
       " '549',\n",
       " 'selectchance',\n",
       " 'gaurd',\n",
       " 'rolltideroll',\n",
       " 'rtr',\n",
       " 'bruhhhhhhhhh',\n",
       " 'nbaontnt',\n",
       " 'dawgg',\n",
       " 'pittcon',\n",
       " 'hellobleached',\n",
       " 'subtchicago',\n",
       " 'aileybayrae',\n",
       " 'justinbieber',\n",
       " 'gdragon',\n",
       " 'mixxo',\n",
       " 'yuchai',\n",
       " 'pinkmartini',\n",
       " 'gds',\n",
       " 'seungri',\n",
       " '2011',\n",
       " 'mattbarkley',\n",
       " 'analogie',\n",
       " 'leggos',\n",
       " 'bermanfree',\n",
       " 'timtebow',\n",
       " 'bermans',\n",
       " 'lechler',\n",
       " '1500espnreusse',\n",
       " 'startribune',\n",
       " 'taeos',\n",
       " 'gruden',\n",
       " '19th',\n",
       " 'turbin',\n",
       " 'marshawn',\n",
       " '62nd',\n",
       " 'babyyyyyy',\n",
       " 'bowwowymcmb',\n",
       " 'kerihilson',\n",
       " 'babydaddy',\n",
       " 'perolawiberg',\n",
       " 'sexxyy',\n",
       " 'badd',\n",
       " '1future',\n",
       " 'mairs',\n",
       " 'hollldddd',\n",
       " 'knw',\n",
       " 'wunm',\n",
       " 'hitmansteviej',\n",
       " 'lawwwwdddd',\n",
       " 'abcfamily',\n",
       " 'famm',\n",
       " 'cindarella',\n",
       " 'cinddyyrelaa',\n",
       " 'kingdont',\n",
       " 'niallofficial',\n",
       " '19',\n",
       " '1950',\n",
       " 'orginal',\n",
       " 'hotcole',\n",
       " 'snappin',\n",
       " 'samepage',\n",
       " 'rulegoverence',\n",
       " 'congressgovt',\n",
       " 'cordarelle',\n",
       " '29',\n",
       " 'kroos',\n",
       " 'pirlo',\n",
       " 'sturridge',\n",
       " 'dantoni',\n",
       " 'wantknows',\n",
       " 'nfldraft2013',\n",
       " 'gonn',\n",
       " '12th',\n",
       " 'raidernation',\n",
       " 'coog',\n",
       " 'uofh',\n",
       " 'wthe',\n",
       " 'dignit',\n",
       " 'subgenius',\n",
       " 'bushcenter',\n",
       " 'damontre',\n",
       " '81st',\n",
       " '90',\n",
       " 'nyg',\n",
       " 'datone',\n",
       " 'jonesgreen',\n",
       " '51',\n",
       " 'amerson',\n",
       " 'bihhh',\n",
       " '51st',\n",
       " 'cbnc',\n",
       " 'redskinsnation',\n",
       " 'joeyorck',\n",
       " 'davidamerson1',\n",
       " 'washingthon',\n",
       " 'httr',\n",
       " 'seleciona',\n",
       " 'ncsu',\n",
       " 'davidarchie',\n",
       " 'fangirls',\n",
       " 'livestream',\n",
       " '1110',\n",
       " 'wwtt',\n",
       " 'officialadele',\n",
       " 'davidsbackpack',\n",
       " '1100',\n",
       " '26',\n",
       " 'nebtd',\n",
       " 'darelle',\n",
       " 'revis',\n",
       " 'relyin',\n",
       " 'gabbert',\n",
       " 'ayee',\n",
       " 'trufant',\n",
       " '22nd',\n",
       " 'bge',\n",
       " 'brees',\n",
       " 'forreal',\n",
       " 'uwfootballs',\n",
       " 'atlantafalcons',\n",
       " 'altanta',\n",
       " '122',\n",
       " 'westsidehell',\n",
       " 'dblock',\n",
       " '365',\n",
       " 'tsn2',\n",
       " 'jobaint',\n",
       " 'gundogan',\n",
       " 'illkay',\n",
       " 'ilkay',\n",
       " 'bmt',\n",
       " 'supposely',\n",
       " 'iker',\n",
       " 'goalkeepers',\n",
       " 'worldie',\n",
       " 'casilas',\n",
       " 'tht',\n",
       " 'qualitty',\n",
       " 'really2',\n",
       " 'awzum',\n",
       " 'madridistas',\n",
       " 'todayhell',\n",
       " 'gundagun',\n",
       " 'realmadrid',\n",
       " 'mccguire',\n",
       " 'mguire',\n",
       " 'wft',\n",
       " 'awww',\n",
       " 'ommggg',\n",
       " 'migure',\n",
       " 'checc',\n",
       " 'fangirling',\n",
       " 'mcguiree',\n",
       " 'tbt',\n",
       " 'mgguire',\n",
       " 'omgggggggggggffhkeekneeieirbrvejriiehw',\n",
       " 'ohmygawd',\n",
       " 'guire',\n",
       " 'shoutout',\n",
       " 'sarawilliams96',\n",
       " 'lizziemcguire',\n",
       " 'omgg',\n",
       " 'teennick',\n",
       " 'throwbackthursday',\n",
       " 'nowwww',\n",
       " 'schumaker',\n",
       " 'hahahahaha',\n",
       " 'mcdavid',\n",
       " 'dortmunds',\n",
       " '31',\n",
       " '41',\n",
       " 'humilate',\n",
       " 'oneoff',\n",
       " 'ricken',\n",
       " '90quid',\n",
       " 'bayerndortmund',\n",
       " 'dortmundreal',\n",
       " 'leandowski',\n",
       " 'shwd',\n",
       " 'dortmunddestruction',\n",
       " 'unbealivable',\n",
       " 'tattaz',\n",
       " '4pt',\n",
       " 'bullsvsnets',\n",
       " '127127',\n",
       " '123123',\n",
       " 'netsbulls',\n",
       " 'bullsnets',\n",
       " 'chicagobrooklyn',\n",
       " 'dayummm',\n",
       " '55',\n",
       " 'at127',\n",
       " 'chitown',\n",
       " 'nbaplayoffs',\n",
       " 'fiya',\n",
       " 'okc',\n",
       " 'loooove',\n",
       " 'dejando',\n",
       " 'pillo',\n",
       " 'atlantahell',\n",
       " 'rhoa',\n",
       " 'pourjeinventoryblowout',\n",
       " 'taureandirect',\n",
       " 'todoautosurplus',\n",
       " 'wayyyyy',\n",
       " 'harbaugh',\n",
       " '49ers',\n",
       " 'couldve',\n",
       " 'goldsons',\n",
       " 'ninersnation',\n",
       " 'superbound',\n",
       " '49erfam',\n",
       " 'ansah',\n",
       " 'byu',\n",
       " 'lensfree',\n",
       " 'reald',\n",
       " 'wout',\n",
       " 'real3d',\n",
       " 'lenseless',\n",
       " 'detroitlionsnfl',\n",
       " 'reald3d',\n",
       " 'ahahahha',\n",
       " 'lmfaoooooooooo',\n",
       " 'scotiaweb',\n",
       " '11',\n",
       " '130',\n",
       " 'meeeee',\n",
       " 'starwars',\n",
       " 'khali',\n",
       " 'yellowbastard',\n",
       " 'realsummerwwe',\n",
       " '27',\n",
       " 'lobbin',\n",
       " 'alleyoop',\n",
       " 'shump',\n",
       " 'rhythem',\n",
       " 'shumperts',\n",
       " 'foles',\n",
       " 'qbs',\n",
       " 'harrystyles',\n",
       " 'tmht',\n",
       " '1839',\n",
       " 'miserables',\n",
       " 'lamichael',\n",
       " 'lattimore',\n",
       " 'lamicheal',\n",
       " '1133',\n",
       " 'cryy',\n",
       " 'spx',\n",
       " '23',\n",
       " 'protags',\n",
       " 'tde',\n",
       " 'gwb',\n",
       " 'liebary',\n",
       " 'hooha',\n",
       " 'thebushcenter',\n",
       " 'tearyeyed',\n",
       " '68',\n",
       " 'fromyale',\n",
       " 'barackobama',\n",
       " 'noshow',\n",
       " 'michaelmoore',\n",
       " 'wowww',\n",
       " 'spursvslakers',\n",
       " 'celticsknicks',\n",
       " 'foul2',\n",
       " 'bostons',\n",
       " 'smmfh',\n",
       " 'tmrw',\n",
       " 'therealjrsmith',\n",
       " 'whhaaattt',\n",
       " 'ayeeee',\n",
       " 'bulllshit',\n",
       " 'recieving',\n",
       " 'terrybrandon',\n",
       " 'artest',\n",
       " 'fga',\n",
       " 'overunder',\n",
       " 'dammm',\n",
       " 'jrsmith',\n",
       " 'wfan',\n",
       " 'knicksboston',\n",
       " 'boutta',\n",
       " 'heatnationthat',\n",
       " 'dukies',\n",
       " 'thibs',\n",
       " 'whatevz',\n",
       " '11091',\n",
       " 'thecross87',\n",
       " 'netsvbulls',\n",
       " 'soooooo',\n",
       " 'brooklynnets',\n",
       " 'seered',\n",
       " 'calisi',\n",
       " 'mtn',\n",
       " 'revengeance',\n",
       " 'squeeeee',\n",
       " '1299',\n",
       " 'shaunwkeaveny',\n",
       " 's01',\n",
       " 'ohdont',\n",
       " 'fuuuuuuuck',\n",
       " 'nunggu',\n",
       " '1040',\n",
       " 'thoooouuugh',\n",
       " 'buttcentric',\n",
       " 'timeeee',\n",
       " 'ahahahaha',\n",
       " 'garrard',\n",
       " 'buttfumble',\n",
       " 'mcelory',\n",
       " 'lmaoo',\n",
       " 'mceiroy',\n",
       " 'sanchize',\n",
       " 'btwn',\n",
       " 'competish',\n",
       " 'teraz',\n",
       " 'mcilroy',\n",
       " 'mccelroy',\n",
       " 'dellorto',\n",
       " '20m',\n",
       " 'defuk',\n",
       " 'kaepernick',\n",
       " 'rg3',\n",
       " '33',\n",
       " 'coolin',\n",
       " '39',\n",
       " '5050',\n",
       " 'ppl',\n",
       " 'nassib',\n",
       " 'holgerson',\n",
       " 'heeeeeated',\n",
       " 'toinght',\n",
       " 'ahahah',\n",
       " 'blackmon',\n",
       " '85',\n",
       " 'wowwww',\n",
       " 'boomd',\n",
       " 'deadass',\n",
       " 'sheeesssh',\n",
       " 'dirtyyyyyy',\n",
       " 'knightd',\n",
       " '2ndq',\n",
       " 'top10',\n",
       " 'ouu',\n",
       " 'haaaaaard',\n",
       " 'damnnn',\n",
       " 'lawddd',\n",
       " 'dunkfaced',\n",
       " 'damnn',\n",
       " 'jsmoovenba',\n",
       " 'posturized',\n",
       " 'thefieldhouse',\n",
       " 'ooowee',\n",
       " 'iamshake',\n",
       " 'sportcenter',\n",
       " 'teamnovapetey',\n",
       " 'sumtin',\n",
       " 'steds',\n",
       " 'ww3',\n",
       " 'kachidgameboi',\n",
       " 'wnna',\n",
       " '81',\n",
       " 'balotelli',\n",
       " 'fcbayern',\n",
       " '1937',\n",
       " '166',\n",
       " 'wgitmo',\n",
       " '2016',\n",
       " 'icloud',\n",
       " 'sciencegoogle',\n",
       " 'iphoneipad',\n",
       " 'disponible',\n",
       " 'iphone5',\n",
       " 'iphonehacks',\n",
       " 'searchs',\n",
       " 'zdnetgoogle',\n",
       " 'compell',\n",
       " 'goudelock',\n",
       " 'cofc',\n",
       " 'walkoff',\n",
       " 'valdespine',\n",
       " 'beyonc',\n",
       " 'terminou',\n",
       " 'choreo',\n",
       " 'harvick',\n",
       " '400',\n",
       " 'keelan',\n",
       " 'wellid',\n",
       " 'gwc',\n",
       " 'logano',\n",
       " 'p6',\n",
       " 'rcr',\n",
       " 'laptimes',\n",
       " 'jpm',\n",
       " 'kb78',\n",
       " 'toyotaowners400',\n",
       " 'wowwwww',\n",
       " 'moytona',\n",
       " 'jpmontoya',\n",
       " 'wannnaaaa',\n",
       " 'fightshoving',\n",
       " 'slayyyyyyyyys',\n",
       " 'frikken',\n",
       " 'parkerighile',\n",
       " 'gaylesbian',\n",
       " 'leodicaprio',\n",
       " 'leofans',\n",
       " 'hollywoodcelebrity',\n",
       " 'essien',\n",
       " 'chocolatevanilla',\n",
       " '752',\n",
       " 'askscandal',\n",
       " 'sosighit',\n",
       " 'jasika',\n",
       " 'higuain',\n",
       " 'dyoub',\n",
       " 'ramez',\n",
       " 'phuck',\n",
       " 'matts',\n",
       " 'leftfoot',\n",
       " 'rightfooted',\n",
       " 'ibaka',\n",
       " 'poten',\n",
       " 'ccccrrrrraaaaazzzzzzyyyyyy',\n",
       " 'disneymarvels',\n",
       " 'smd',\n",
       " '3dganci',\n",
       " 'hihi',\n",
       " '175m',\n",
       " 'tgwtg',\n",
       " 'laaurenjay',\n",
       " 'hellionvladimir',\n",
       " 'apicton12',\n",
       " 'li3y',\n",
       " '1953',\n",
       " 'craaaazy',\n",
       " 'aaaand',\n",
       " 'javale',\n",
       " 'boget',\n",
       " 'farried',\n",
       " '21st',\n",
       " 'sht',\n",
       " 'mcgeed',\n",
       " 'shaqtin',\n",
       " 'faried',\n",
       " 'andrewbogut',\n",
       " 'tripledouble',\n",
       " 'mozgovd',\n",
       " 'warriorsnuggets',\n",
       " 'fbis',\n",
       " 'sctop10',\n",
       " '0o',\n",
       " 'cyprien',\n",
       " '82',\n",
       " 'ilovecheese',\n",
       " '45',\n",
       " 'jariana',\n",
       " 'grandelaughs',\n",
       " 'tweetlimit',\n",
       " 'x36',\n",
       " '54',\n",
       " 'muchh',\n",
       " 'x1',\n",
       " '53',\n",
       " 'hayjdbieber',\n",
       " 'xx20',\n",
       " '243jaibrooks1',\n",
       " '05',\n",
       " 'jaibrooks1',\n",
       " 'slothanator',\n",
       " 'plase',\n",
       " 'trsrdy787876',\n",
       " 'mallys',\n",
       " '58',\n",
       " '76',\n",
       " 'pleeeeease',\n",
       " 'lyfe',\n",
       " 'plazzzzzzz',\n",
       " 'tomlinshiit',\n",
       " 'xxoxoxo',\n",
       " 'iilovecheese',\n",
       " 'pleasee',\n",
       " 'follw',\n",
       " 'kakam',\n",
       " 'anyssaayala',\n",
       " 'itzjessicahoex3',\n",
       " 'x32',\n",
       " 'rted',\n",
       " 'x73',\n",
       " 'eole',\n",
       " 'iamtyleriacona',\n",
       " 'patrickquirky',\n",
       " 'ttlyteala',\n",
       " '70',\n",
       " '86',\n",
       " 'king8jamie',\n",
       " '52',\n",
       " '52nd',\n",
       " 'olb',\n",
       " '60th',\n",
       " '65',\n",
       " 'selecionou',\n",
       " '52bd',\n",
       " 'usm',\n",
       " 'nepick',\n",
       " 'cstodd89',\n",
       " 'studentprintz',\n",
       " 'usmalumni',\n",
       " 'usmgoldeneagles',\n",
       " 'mikereiss',\n",
       " 'pissin',\n",
       " 'mwp',\n",
       " 'antawn',\n",
       " '2001',\n",
       " 'stanely',\n",
       " 'boguts',\n",
       " 'lmfaoo',\n",
       " 'lmfaooooo',\n",
       " 'shiited',\n",
       " 'rotflmfao',\n",
       " 'hmmmmmmmmmmm',\n",
       " 'mhmm',\n",
       " 'hawtest',\n",
       " 'ntdt',\n",
       " 'patt',\n",
       " 'officialjoan',\n",
       " 'sidehug',\n",
       " 'rogerjoan',\n",
       " 'georgekotsi',\n",
       " 'ironman3',\n",
       " 'joanrivers',\n",
       " 'dawnjoan',\n",
       " 'foxyashell',\n",
       " 'yeen',\n",
       " 'joaning',\n",
       " 'athome',\n",
       " 'celebrityapprentice',\n",
       " 'joans',\n",
       " 'halfassed',\n",
       " 'acually',\n",
       " 'dh2',\n",
       " 'andruzzi',\n",
       " 'bostonstrong',\n",
       " '11s',\n",
       " 'okcvshou',\n",
       " 'getglue',\n",
       " 'ygrittejon',\n",
       " '85th',\n",
       " '3pts',\n",
       " 'rentre',\n",
       " 'sigb',\n",
       " 'seau',\n",
       " 'nxt',\n",
       " 'manyi',\n",
       " 'sandiego',\n",
       " 'sdchargers',\n",
       " 'syas',\n",
       " 'seauteo',\n",
       " 'mantei',\n",
       " 'kieper',\n",
       " 'seauget',\n",
       " 'vols',\n",
       " 'fantasyfootball',\n",
       " 'cordarrelle',\n",
       " 'ayyy',\n",
       " 'wahoooo',\n",
       " 'tenessee',\n",
       " '29th',\n",
       " '34th',\n",
       " 'cordarralle',\n",
       " 'vinnytraance',\n",
       " 'nygiants',\n",
       " 'mayock',\n",
       " 'trex',\n",
       " 'sharrif',\n",
       " 'gthe',\n",
       " 'pughcant',\n",
       " 'gmen',\n",
       " 'otog',\n",
       " 'olineman',\n",
       " 'beattheblock',\n",
       " 'khedndiby',\n",
       " 'whysomad',\n",
       " 'vaccarro',\n",
       " 'vaccaro',\n",
       " 'whodatnation',\n",
       " 'dreamchasin23',\n",
       " '15th',\n",
       " 'bevobeat',\n",
       " '1961',\n",
       " 'blakegriffin32',\n",
       " 'kiagriffin',\n",
       " 'sooooooo',\n",
       " 'killllllll',\n",
       " 'lolll',\n",
       " 'fuggin',\n",
       " 'griffens',\n",
       " 'uuuuup',\n",
       " 'blakekia',\n",
       " 'hahahah',\n",
       " 'commericals',\n",
       " 'commericials',\n",
       " 'kiablake',\n",
       " 'any1',\n",
       " 'jkim',\n",
       " 'dayummmmmm',\n",
       " 'looooool',\n",
       " 'reenacts',\n",
       " 'sextape',\n",
       " 'klookalike',\n",
       " '20112012',\n",
       " 'sactown',\n",
       " 'purplearmy',\n",
       " 'ziggler',\n",
       " 'roynelsonmma',\n",
       " 'cheik',\n",
       " 'fukked',\n",
       " 'kuntry',\n",
       " '203',\n",
       " 'r1',\n",
       " 'fck',\n",
       " 'kod',\n",
       " 'chieck',\n",
       " 'wowwwwww',\n",
       " 'knocced',\n",
       " 'fugg',\n",
       " 'outtttt',\n",
       " 'chiek',\n",
       " 'bigcountry',\n",
       " 'knockes',\n",
       " 'uo',\n",
       " '115',\n",
       " 'shamarko',\n",
       " '115th',\n",
       " 'titbumpinpandas',\n",
       " 'polian',\n",
       " 'roethlisberger',\n",
       " 'dallascowboy',\n",
       " 'ecarey67',\n",
       " 'jptheasshole',\n",
       " 'leveon',\n",
       " 'bettis',\n",
       " '48th',\n",
       " 'steelernation',\n",
       " '44',\n",
       " 'contrao',\n",
       " 'mourinho',\n",
       " 'coentrao',\n",
       " 'tapout',\n",
       " 'acabo',\n",
       " 'buscar',\n",
       " 'utilizar',\n",
       " 'reggaetton',\n",
       " 'duuude',\n",
       " 'devaneykk',\n",
       " 'guillermodiazyo',\n",
       " 'watchung',\n",
       " 'poppycorn',\n",
       " '16mins',\n",
       " '1810',\n",
       " 'yday',\n",
       " 'haaaaang',\n",
       " '11th',\n",
       " 'condra',\n",
       " 'lucic',\n",
       " 'krejcilucic',\n",
       " 'dagauvins',\n",
       " 'neiler',\n",
       " 'lucickrejcidaugavins',\n",
       " 'turris',\n",
       " 'mainevent',\n",
       " 'loveeee',\n",
       " 'knicksnation',\n",
       " 'aite',\n",
       " 'coogi',\n",
       " 'yannah',\n",
       " 'twaz',\n",
       " 'yesto',\n",
       " 'bpl',\n",
       " '101',\n",
       " '1h',\n",
       " 'fucktheotherteams',\n",
       " 'awal',\n",
       " 'baru',\n",
       " 'champ20ns',\n",
       " 'babak1',\n",
       " 'louding',\n",
       " '24m',\n",
       " 'ggmu',\n",
       " 'nonton',\n",
       " 'drez4prez',\n",
       " 'eiferty80',\n",
       " 'latttwoone',\n",
       " '864',\n",
       " 'niceeeee',\n",
       " 'glab',\n",
       " '9ers',\n",
       " 'ayyyye',\n",
       " '131st',\n",
       " 'lemonier',\n",
       " 'dpbrugler',\n",
       " 'congrata',\n",
       " 'bruschi',\n",
       " 'tedy',\n",
       " 'casserly',\n",
       " 'sheamus',\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                            output_dim = EMBEDDING_SIZE,\n",
    "                            input_length = MAX_SEQ_LEN,\n",
    "                            weights = [EMBEDDING_MATRIX], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 18, 300)      3252600     input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, 300), (None, 721200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  [(None, 300), (None, 721200      embedding_3[1][0]                \n",
      "                                                                 lstm_13[0][1]                    \n",
      "                                                                 lstm_13[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            301         lstm_14[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,695,301\n",
      "Trainable params: 1,442,701\n",
      "Non-trainable params: 3,252,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, TimeDistributed, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "encoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "encoder_embedding = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "decoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "decoder_embedding = embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "outputs = Dense(1, activation='sigmoid')(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10377 samples, validate on 1153 samples\n",
      "Epoch 1/5\n",
      "10377/10377 [==============================] - 16s 2ms/step - loss: 0.4656 - acc: 0.7916 - val_loss: 0.6464 - val_acc: 0.6826\n",
      "Epoch 2/5\n",
      "10377/10377 [==============================] - 16s 2ms/step - loss: 0.4622 - acc: 0.7918 - val_loss: 0.6295 - val_acc: 0.6869\n",
      "Epoch 3/5\n",
      "10377/10377 [==============================] - 16s 2ms/step - loss: 0.4549 - acc: 0.7948 - val_loss: 0.6629 - val_acc: 0.6904\n",
      "Epoch 4/5\n",
      "10377/10377 [==============================] - 16s 2ms/step - loss: 0.4506 - acc: 0.8016 - val_loss: 0.6350 - val_acc: 0.7042\n",
      "Epoch 5/5\n",
      "10377/10377 [==============================] - 16s 2ms/step - loss: 0.4460 - acc: 0.8010 - val_loss: 0.6647 - val_acc: 0.6852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159e56e10>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 5, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predicted = [prob > 0.5 for prob in model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.95      0.78      2672\n",
      "        True       0.55      0.10      0.18      1470\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      4142\n",
      "   macro avg       0.61      0.53      0.48      4142\n",
      "weighted avg       0.62      0.65      0.57      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dev_labels, labels_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import LSTM, Lambda, concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "HIDDEN_DIM=100\n",
    "\n",
    "def exponent_neg_manhattan_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    return K.exp(-K.sum(K.abs(x[:,:hidden_size] - x[:,hidden_size:]), axis=1, keepdims=True))\n",
    "\n",
    "def exponent_neg_cosine_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    leftNorm = K.l2_normalize(x[:,:hidden_size], axis=-1)\n",
    "    rightNorm = K.l2_normalize(x[:,hidden_size:], axis=-1)\n",
    "    return K.exp(K.sum(K.prod([leftNorm, rightNorm], axis=0), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence1 (InputLayer)          (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence2 (InputLayer)          (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 18, 300)      3252600     sequence1[0][0]                  \n",
      "                                                                 sequence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, 100)          160400      embedding_3[42][0]               \n",
      "                                                                 embedding_3[43][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 200)          0           lstm_35[0][0]                    \n",
      "                                                                 lstm_35[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         205824      concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            1025        dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,619,849\n",
      "Trainable params: 367,249\n",
      "Non-trainable params: 3,252,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_1 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence1')\n",
    "seq_2 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence2')\n",
    "\n",
    "input_1 = embedding_layer(seq_1)\n",
    "input_2 = embedding_layer(seq_2)\n",
    "\n",
    "l1 = LSTM(units=HIDDEN_DIM)\n",
    "\n",
    "l1_out = l1(input_1)\n",
    "l2_out = l1(input_2)\n",
    "\n",
    "concats = concatenate([l1_out, l2_out], axis=-1)\n",
    "\n",
    "#main_output = Lambda(exponent_neg_cosine_distance, output_shape=(1,))(concats)\n",
    "#main_output = Lambda(exponent_neg_manhattan_distance, output_shape=(1,))(concats)\n",
    "dense_ouput = Dense(1024, activation=\"relu\")(concats)\n",
    "main_output = Dense(1, activation=\"sigmoid\")(dense_ouput)\n",
    "\n",
    "model = Model(inputs=[seq_1, seq_2], outputs=[main_output])\n",
    "\n",
    "opt = keras.optimizers.Adadelta(lr=1, clipnorm=1.25)\n",
    "\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer=opt,loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10377 samples, validate on 1153 samples\n",
      "Epoch 1/5\n",
      "10377/10377 [==============================] - 5s 438us/step - loss: 0.4944 - acc: 0.7756 - val_loss: 0.5762 - val_acc: 0.7268\n",
      "Epoch 2/5\n",
      "10377/10377 [==============================] - 5s 443us/step - loss: 0.4905 - acc: 0.7775 - val_loss: 0.5802 - val_acc: 0.7268\n",
      "Epoch 3/5\n",
      "10377/10377 [==============================] - 5s 470us/step - loss: 0.4860 - acc: 0.7777 - val_loss: 0.5861 - val_acc: 0.7225\n",
      "Epoch 4/5\n",
      "10377/10377 [==============================] - 5s 443us/step - loss: 0.4837 - acc: 0.7791 - val_loss: 0.5550 - val_acc: 0.7277\n",
      "Epoch 5/5\n",
      "10377/10377 [==============================] - 5s 441us/step - loss: 0.4791 - acc: 0.7843 - val_loss: 0.5952 - val_acc: 0.7216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1589a80b8>"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 5, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_similarity = model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.96      0.87       663\n",
      "        True       0.34      0.07      0.12       175\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       838\n",
      "   macro avg       0.57      0.52      0.50       838\n",
      "weighted avg       0.70      0.78      0.72       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, [prob > 0.5 for prob in predicted_similarity]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
