{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  257M  100  257M    0     0  1955k      0  0:02:14  0:02:14 --:--:-- 2770k6k\n"
     ]
    }
   ],
   "source": [
    "!curl https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz --output numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gunzip numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "numberbatch = KeyedVectors.load_word2vec_format(\"numberbatch-en-17.06.txt.gz\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contravariant_functor', 0.9674091935157776),\n",
       " ('forgetful_functor', 0.9666687250137329),\n",
       " ('yoneda_embedding', 0.9497337937355042),\n",
       " ('endofunctor', 0.9360368847846985),\n",
       " ('representable_functor', 0.9314213991165161),\n",
       " ('cofunctor', 0.9296932220458984),\n",
       " ('natural_transformation', 0.9164144992828369),\n",
       " ('yoneda_lemma', 0.9022417664527893),\n",
       " ('coaugmentation', 0.8871707320213318),\n",
       " ('category_theory', 0.8751257658004761)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberbatch.most_similar(['functor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Phrase = namedtuple('Phrase', 'original candidate label')\n",
    "Token = namedtuple('Token', 'text tags')\n",
    "\n",
    "def split_tokens(sent):\n",
    "    tokens = []\n",
    "    for token in sent.split():\n",
    "        tags = token.split('/')\n",
    "        tokens.append(Token(tags[0].lower(), tuple(tags[1:])))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def readData(filename, eval_label, ignoreNone):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) == 7:\n",
    "                (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = fields\n",
    "            else:\n",
    "                continue\n",
    "            label = eval_label(judge)\n",
    "            if ((label is None) and ignoreNone):\n",
    "                continue\n",
    "            data.append(Phrase(split_tokens(origsenttag), split_tokens(candsenttag), label))\n",
    "    \n",
    "    return data\n",
    "                \n",
    "def eval_amt_label(label):\n",
    "    nYes = eval(label)[0]            \n",
    "    \n",
    "    if nYes >= 3:\n",
    "        return True\n",
    "    elif nYes <= 1:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "def eval_expert_label(label):\n",
    "    nYes = int(label[0])\n",
    "    \n",
    "    if nYes >= 4:\n",
    "        return True\n",
    "    elif nYes <= 2:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def readTrainData(filename):\n",
    "    return readData(filename, eval_amt_label, True)\n",
    "\n",
    "def readTestData(filename):\n",
    "    return readData(filename, eval_expert_label, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readTrainData(\"SemEval-PIT2015-py3/data/train.data\")\n",
    "dev_data = readTrainData(\"SemEval-PIT2015-py3/data/dev.data\")\n",
    "test_data = [p for p in readTestData(\"SemEval-PIT2015-py3/data/test.data\") if p.label is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='bro', tags=('O', 'NN', 'I-NP', 'O')), Token(text='from', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='757', tags=('O', 'CD', 'I-NP', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'I-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='gone', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='can', tags=('O', 'MD', 'B-VP', 'O')), Token(text='believe', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='went', tags=('O', 'VBD', 'B-VP', 'O')), Token(text='as', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='what', tags=('O', 'WP', 'I-NP', 'O'))], label=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='last', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='nevr', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='old', tags=('O', 'JJ', 'B-NP', 'O')), Token(text='ahah', tags=('O', 'JJ', 'I-NP', 'O'))], label=False),\n",
       " Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'O')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='at', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='end', tags=('O', 'NN', 'I-NP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='me', tags=('O', 'PRP', 'B-NP', 'O')), Token(text='so', tags=('O', 'RB', 'B-ADVP', 'O')), Token(text='hype', tags=('O', 'JJ', 'I-ADVP', 'O'))], label=False),\n",
       " Phrase(original=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='ending', tags=('O', 'VBG', 'I-NP', 'B-EVENT')), Token(text='to', tags=('O', 'TO', 'I-NP', 'O')), Token(text='8', tags=('O', 'CD', 'I-NP', 'O')), Token(text='mile', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='fav', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='part', tags=('O', 'NN', 'I-NP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='whole', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='movie', tags=('O', 'NN', 'I-NP', 'B-EVENT'))], candidate=[Token(text='rabbit', tags=('O', 'NNP', 'B-NP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='out', tags=('O', 'IN', 'B-PP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='place', tags=('O', 'NN', 'B-NP', 'O')), Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='determined', tags=('O', 'VBD', 'B-VP', 'B-EVENT')), Token(text='to', tags=('O', 'TO', 'I-VP', 'O')), Token(text='make', tags=('O', 'VB', 'I-VP', 'O')), Token(text='it', tags=('O', 'PRP', 'B-NP', 'O'))], label=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def clean_sent(sent):\n",
    "    new_sent = []\n",
    "    for token in sent:\n",
    "        if token.tags[0].startswith('B-'):\n",
    "            new_sent.append(token.tags[0].split('-')[1])\n",
    "            continue\n",
    "        if token.tags[0].startswith('I-'): #or token.text in stopwords.words('english'):\n",
    "            continue\n",
    "        if token.tags[1] == 'CD':\n",
    "            new_sent.append('number')\n",
    "            continue\n",
    "        new_sent.append(token.text)\n",
    "                            \n",
    "    return new_sent\n",
    "\n",
    "def clean_data(data):\n",
    "    return [Phrase(clean_sent(phrase.original), clean_sent(phrase.candidate), phrase.label) \\\n",
    "            for phrase in tqdm_notebook(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26050189cabb440b957a9c6248e3453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d07f5c8ca7499c8e9c871295fb8330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4142), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_dev_data = clean_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2844a88ce7b445a8649d7f3ee33c360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=838), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=['person', 'the', 'number', 'qb', 'to', 'go', 'in', 'this', 'draft'], candidate=['but', 'my', 'bro', 'from', 'the', 'number', 'person', 'is', 'the', 'number', 'qb', 'gone'], label=True),\n",
       " Phrase(original=['person', 'the', 'number', 'qb', 'to', 'go', 'in', 'this', 'draft'], candidate=['can', 'believe', 'person', 'went', 'as', 'the', 'number', 'qb', 'in', 'the', 'draft'], label=True),\n",
       " Phrase(original=['person', 'the', 'number', 'qb', 'to', 'go', 'in', 'this', 'draft'], candidate=['person', 'is', 'the', 'number', 'qb', 'what'], label=True)]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8975"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def phrase_tokens(phrase):\n",
    "    return [token for token in (phrase.original + phrase.candidate)]\n",
    "    \n",
    "\n",
    "vocab = Dictionary([phrase_tokens(p) for p in clean_train_data + clean_dev_data + clean_test_data])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8976\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab) + 1 # +1 for padding\n",
    "\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11530\n",
      "11530\n",
      "11530\n"
     ]
    }
   ],
   "source": [
    "def data_to_sequences(data):\n",
    "    \n",
    "    encoder_seqs = []\n",
    "    decoder_seqs = []\n",
    "    labels = []\n",
    "    \n",
    "    for phrase in data:\n",
    "        encoder_seqs.append(text_to_sequence([t for t in phrase.original]))\n",
    "        decoder_seqs.append(text_to_sequence([t for t in phrase.candidate]))\n",
    "        labels.append(phrase.label)\n",
    "        \n",
    "    return encoder_seqs, decoder_seqs, labels \n",
    "\n",
    "train_encoder_seqs, train_decoder_seqs, train_labels = data_to_sequences(clean_train_data)\n",
    "\n",
    "print(len(train_encoder_seqs))\n",
    "print(len(train_decoder_seqs))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'the', 'number', 'qb', 'to', 'go', 'in', 'this', 'draft']\n",
      "['but', 'my', 'bro', 'from', 'the', 'number', 'person', 'is', 'the', 'number', 'qb', 'gone']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_encoder_seqs[0]))\n",
    "print(sequence_to_text(train_decoder_seqs[0]))\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoder_seqs, dev_decoder_seqs, dev_labels = data_to_sequences(clean_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_seqs, test_decoder_seqs, test_labels = data_to_sequences(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max([len(seq) for seq in (train_encoder_seqs + train_decoder_seqs + \\\n",
    "                                       dev_encoder_seqs + dev_decoder_seqs + \\\n",
    "                                       test_decoder_seqs + test_encoder_seqs)])\n",
    "\n",
    "print(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(sequences):\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LEN, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_MATRIX = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "  \n",
    "missed = set([])\n",
    "for word, i in vocab.token2id.items():\n",
    "    try:\n",
    "        EMBEDDING_MATRIX[i] = numberbatch[word]\n",
    "    except KeyError:\n",
    "        missed.add(word)\n",
    "\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'supposely',\n",
       " 'itsstephsoricex',\n",
       " 'ltrauzzi',\n",
       " 'iker',\n",
       " 'fbis',\n",
       " 'courside',\n",
       " 'bmthofficial',\n",
       " 'rulegoverence',\n",
       " '81st',\n",
       " 'spursvslakers',\n",
       " '40000',\n",
       " 'latttwoone',\n",
       " 'prayforkrista',\n",
       " 'yammed',\n",
       " 'daterape',\n",
       " 'at127',\n",
       " 'squeeeee',\n",
       " '90s',\n",
       " 'hardinghe',\n",
       " '59th',\n",
       " 'fukk',\n",
       " 'airrielle',\n",
       " 'subtchicago',\n",
       " '9inning',\n",
       " 'snappin',\n",
       " 'revolutionised',\n",
       " 'collegeboard',\n",
       " 'nlc',\n",
       " 'grooooooown',\n",
       " 'nialler',\n",
       " 'outchea',\n",
       " '52nd',\n",
       " 'ggmu',\n",
       " '27footers',\n",
       " 'homewreaker',\n",
       " 'sdchargers',\n",
       " 'fga',\n",
       " 'worldie',\n",
       " 'tylerwinkeljohn',\n",
       " 'uo',\n",
       " 'davidarchie',\n",
       " 'boofed',\n",
       " 'lightskinnedmixed',\n",
       " 'hackaasik',\n",
       " 'calumhooding',\n",
       " 'athome',\n",
       " 'hahah',\n",
       " 'solomonswisdom',\n",
       " 'dagauvins',\n",
       " 'teamvedo',\n",
       " 'maloofs',\n",
       " 'beatt',\n",
       " 'kunalmerchant',\n",
       " 'bulllshit',\n",
       " 'lightskin',\n",
       " 'nerly',\n",
       " 'meshugeneh',\n",
       " 'outtt',\n",
       " 'coentrao',\n",
       " 'frankcaliendo',\n",
       " 'miggitymiggitymiggitymiggity',\n",
       " 'dealine',\n",
       " '12',\n",
       " 'kuntry',\n",
       " 'appli',\n",
       " '35th',\n",
       " 'smmfh',\n",
       " 'ciroc',\n",
       " 'babycake',\n",
       " 'couldshould',\n",
       " 'assel',\n",
       " 'loooool',\n",
       " 'gezwxm87',\n",
       " 'zzzzzzzzzzzzzzzz',\n",
       " 'pfdc',\n",
       " 'queeeeeen',\n",
       " 'heatpacer',\n",
       " 'and1',\n",
       " 'haslem',\n",
       " 'goodasgold',\n",
       " 'ectwins',\n",
       " 'usbelgium',\n",
       " 'timecatconvo',\n",
       " 'firouzabadi',\n",
       " 'gds',\n",
       " 'steadmon',\n",
       " 'backtoback',\n",
       " 'lewandowski',\n",
       " 'baaad',\n",
       " '9ers',\n",
       " 'wde',\n",
       " 'duchene',\n",
       " 'alidoee',\n",
       " 'spx',\n",
       " 'stellahudgens',\n",
       " 'ravensnation',\n",
       " 'marxmegu',\n",
       " 'lockeroom',\n",
       " 'alumn',\n",
       " 'meeeee',\n",
       " 'therecant',\n",
       " 'poten',\n",
       " 'priscillamnyc',\n",
       " 'blaxpolitation',\n",
       " 'dreamchasin23',\n",
       " 'soooooooo',\n",
       " 'fkk',\n",
       " 'pl0x',\n",
       " 'dooped',\n",
       " 'dreamings',\n",
       " '1littleliongirl',\n",
       " 'sacramentokings',\n",
       " 'aaaaaaaaaaaaaahahaha',\n",
       " 'wowwwwww',\n",
       " 'tayshauntony',\n",
       " 'wasnt',\n",
       " 'hiiiiii',\n",
       " 'biiiiiitch',\n",
       " 'amerson',\n",
       " 'liveview',\n",
       " 'nonosquito',\n",
       " 'rted',\n",
       " '25th',\n",
       " 'lemieuxdraper',\n",
       " 'bishhhhhhh',\n",
       " 'wowwww',\n",
       " 'dirtyyyyyy',\n",
       " 'gooo',\n",
       " 'iamshake',\n",
       " 'miggity',\n",
       " 'richjeff',\n",
       " 'dejando',\n",
       " 'ayeee',\n",
       " 'promarriage',\n",
       " 'bumbass',\n",
       " 'bullsnation',\n",
       " 'championsleague',\n",
       " 'mallys',\n",
       " '3pointer',\n",
       " 'dtafted',\n",
       " 'myyyyyyyyyy',\n",
       " 'ctfuuu',\n",
       " '8216',\n",
       " 'dleague',\n",
       " 'sissshie',\n",
       " 'yooooooo',\n",
       " 'inlove',\n",
       " 'blackstrapbbq',\n",
       " 'tooooo',\n",
       " 'broo',\n",
       " 'tooo',\n",
       " 'ohhhhhhh',\n",
       " 'damnbackstrom',\n",
       " 'sheeeitttt',\n",
       " 'teamusher',\n",
       " 'asik',\n",
       " '15',\n",
       " '401',\n",
       " 'indirecting',\n",
       " 'muchh',\n",
       " 'fellani',\n",
       " 'bacarri',\n",
       " 'sagfasgakfgafsdas',\n",
       " '2nyt',\n",
       " 'aaaaand',\n",
       " 'mancrush',\n",
       " 'wayyy',\n",
       " 'yeahh',\n",
       " 'x6',\n",
       " 'eole',\n",
       " 'bobsaget',\n",
       " 'klopp',\n",
       " 'dourtmound',\n",
       " '31st',\n",
       " 'twerks',\n",
       " 'dortmundreal',\n",
       " '25',\n",
       " 'dwighthoward',\n",
       " 'watchn',\n",
       " '48th',\n",
       " 'caliendos',\n",
       " 'comcast',\n",
       " 'ukbig',\n",
       " 'shxt',\n",
       " 'ayyy',\n",
       " 'choreo',\n",
       " 'todayyyy',\n",
       " 'turris',\n",
       " 'montaged',\n",
       " 'wooped',\n",
       " 'wonna',\n",
       " 'biggnbadd',\n",
       " 'homered',\n",
       " 'nebtd',\n",
       " 'follw',\n",
       " 'shittest',\n",
       " 'cinderellaill',\n",
       " 'thourpe',\n",
       " 'eeeeverything',\n",
       " 'onball',\n",
       " 'pleeeeeeease',\n",
       " 'dortmunds',\n",
       " 'vakation',\n",
       " 'nyg',\n",
       " 'reli',\n",
       " 'outtttt',\n",
       " 'lolll',\n",
       " 'xmaggiepayne',\n",
       " 'tarryandzella',\n",
       " 'centerbacks',\n",
       " 'fergies',\n",
       " 'meetingsstuff',\n",
       " 'tixs',\n",
       " 'ayeeee',\n",
       " 'acabo',\n",
       " 'heeeeer',\n",
       " 'danfischer17',\n",
       " 'pleasee',\n",
       " 'hipho',\n",
       " 'garrard',\n",
       " 'x100',\n",
       " 'functionalities',\n",
       " 'jont',\n",
       " 'bandwagoners',\n",
       " 'legendim',\n",
       " 'ucl',\n",
       " 'wayyyyy',\n",
       " 'laurennolinf',\n",
       " 'bpl',\n",
       " 'teamnovapetey',\n",
       " 'bernabeu',\n",
       " '40cant',\n",
       " 'ahahaha',\n",
       " 'bostonbombing',\n",
       " 'aaaand',\n",
       " 'thecowboys',\n",
       " 'realsummerwwe',\n",
       " 'vanphan86',\n",
       " 'hackashaq',\n",
       " 'hiiii',\n",
       " 'cheik',\n",
       " 'dism',\n",
       " 'anyhtjjng',\n",
       " 'sextape',\n",
       " 'threepointers',\n",
       " 'bew',\n",
       " 'liebary',\n",
       " 'iosandroid',\n",
       " 'inefficiencies',\n",
       " 'vv',\n",
       " 'doee',\n",
       " 'freeeeeeeeeeee',\n",
       " 'mclvkdd',\n",
       " 'jennabreanne13',\n",
       " 'bruhhhhhhhhh',\n",
       " 'hooha',\n",
       " 'michael5sos',\n",
       " 'shiii',\n",
       " 'oiii',\n",
       " 'lotulelei',\n",
       " 'dblock',\n",
       " 'dwill',\n",
       " 'spgreenwood',\n",
       " 'seleciona',\n",
       " 'pinecove',\n",
       " 'slothanator',\n",
       " 'chicagobears',\n",
       " 'boosiest',\n",
       " 'stubhub',\n",
       " 'assn',\n",
       " 'winemakers',\n",
       " 'harcore',\n",
       " 'bgcomgmoments',\n",
       " 'harbaugh',\n",
       " 'grillers',\n",
       " 'commericials',\n",
       " 'dispositivos',\n",
       " '85',\n",
       " 'sbjsvsjwjw',\n",
       " 'yute',\n",
       " 'kerrywashington',\n",
       " 'sewins',\n",
       " 'fukin',\n",
       " 'regularseason',\n",
       " 'samepage',\n",
       " '19th',\n",
       " '24',\n",
       " 'fangirls',\n",
       " 'hihi',\n",
       " 'kurts',\n",
       " 'twaz',\n",
       " 'itzjessicahoex3',\n",
       " 'dezduron',\n",
       " 'surfthechannel',\n",
       " 'hitmansteviej',\n",
       " 'txt',\n",
       " 'espnnfcnblog',\n",
       " 'bihh',\n",
       " 'drakee',\n",
       " 'shiited',\n",
       " 'protags',\n",
       " 'bkst',\n",
       " 'theatrea',\n",
       " 'niggga',\n",
       " 'allwest',\n",
       " 'fkn',\n",
       " '1500espnreusse',\n",
       " 'goldsons',\n",
       " 'onehanded',\n",
       " 'heartingselena',\n",
       " '119th',\n",
       " 'cbaire1',\n",
       " 'foles',\n",
       " 'cryinggg',\n",
       " 'nowplaying',\n",
       " 'toinght',\n",
       " 'nickiminaj',\n",
       " 'andrewbogut',\n",
       " 'foxyashell',\n",
       " 'lucic',\n",
       " 'wildavs',\n",
       " 'wooohooooo',\n",
       " 'ayyyy',\n",
       " 'livvveeee',\n",
       " 'grl',\n",
       " 'poyer',\n",
       " '957thegame',\n",
       " 'kylieeeee4',\n",
       " 'aap',\n",
       " 'vintagezarry',\n",
       " 'tripledouble',\n",
       " 'jobaint',\n",
       " 'gasol',\n",
       " 'kreedom',\n",
       " 'pointguard',\n",
       " '17th',\n",
       " 'rimjob',\n",
       " '49ers',\n",
       " 'hebitch',\n",
       " 'reggaetton',\n",
       " 'overunder',\n",
       " 'ewwww',\n",
       " 'nckxnxkx',\n",
       " 'wrs',\n",
       " 'r1',\n",
       " 'sciencegoogle',\n",
       " 'dawgggg',\n",
       " 'mikevick',\n",
       " 'you19',\n",
       " 'okiestate',\n",
       " 'scandalabc',\n",
       " 'teraz',\n",
       " 'lollll',\n",
       " 'realtalk',\n",
       " 'lmfaoooooooooo',\n",
       " 'fromyale',\n",
       " 'dpbrugler',\n",
       " 'tomgreenlive',\n",
       " 'alecdaley',\n",
       " 'kj',\n",
       " 'aguante',\n",
       " 'congresspersons',\n",
       " 'kingdont',\n",
       " 'safirah',\n",
       " 'doesnt',\n",
       " 'jbflexes',\n",
       " 'goldenstate',\n",
       " 'realmoney',\n",
       " 'shamarko',\n",
       " 'sht',\n",
       " 'ibaka',\n",
       " 'trynna',\n",
       " '864',\n",
       " 'youuuuuu',\n",
       " 'lavergne',\n",
       " 'uuuuup',\n",
       " 'wsup',\n",
       " 'violentcbs',\n",
       " 'holddd',\n",
       " 'vols',\n",
       " 'dawgg',\n",
       " 'x1',\n",
       " 'playiiin',\n",
       " 'toyotaowners400',\n",
       " 'lizziemcguire',\n",
       " 'cowboysnation',\n",
       " 'vancouveryoure',\n",
       " 'cheick',\n",
       " 'ravensplus',\n",
       " 'followback',\n",
       " 'wasup',\n",
       " '17',\n",
       " 'yday',\n",
       " 'csart',\n",
       " 'ninersnation',\n",
       " 'x36',\n",
       " 'any1',\n",
       " 'nhlbruins',\n",
       " 'patrickquirky',\n",
       " '10x',\n",
       " 'reald',\n",
       " 'govteducationcompanies',\n",
       " 'usmnt',\n",
       " 'sactown',\n",
       " 'syleenajohnson',\n",
       " 'someonesomething',\n",
       " 'maaan',\n",
       " 'canshould',\n",
       " 'niceeeee',\n",
       " 'panthersoft',\n",
       " 'previewing',\n",
       " 'okk',\n",
       " 'ughhh',\n",
       " 'alleyooped',\n",
       " 'crazyy',\n",
       " 'sanchize',\n",
       " 'formely',\n",
       " 'ahrd',\n",
       " 'pissin',\n",
       " 'myh',\n",
       " 'corookies',\n",
       " 'shoutouts',\n",
       " 'youuuu',\n",
       " 'gobraves',\n",
       " 'aauthorsmusic',\n",
       " 'gongcha',\n",
       " 'clasico',\n",
       " 'qtr',\n",
       " 'zdnetgoogle',\n",
       " 'vneck',\n",
       " 'wudda',\n",
       " 'tde',\n",
       " 'prototypeversion',\n",
       " 'advisegiver',\n",
       " 'sticksaveswag',\n",
       " 'hmmmmmmmmmmm',\n",
       " 'babak1',\n",
       " 'cutiecalum',\n",
       " 'haswellbased',\n",
       " 'thexx',\n",
       " 'wenatchee',\n",
       " 'enuff',\n",
       " 'dukies',\n",
       " 'madriddortmund',\n",
       " 'playpffs',\n",
       " 'jerkoffs',\n",
       " 'burritolirry',\n",
       " 'andforcing',\n",
       " 'lmaoo',\n",
       " 'ilyyyy',\n",
       " '1future',\n",
       " 'nofloorseats',\n",
       " 'asdfgjkl',\n",
       " 'lydiais',\n",
       " 'allllllllll',\n",
       " 'saramcmann',\n",
       " 'buzzerbeater',\n",
       " 'x10',\n",
       " 'iamsb3',\n",
       " 'phucked',\n",
       " 'xgames',\n",
       " 'mehn',\n",
       " 'godddddd',\n",
       " 'bullshett',\n",
       " 'bispingbelcher',\n",
       " 'ziggler',\n",
       " 'fangirling',\n",
       " 'udfa',\n",
       " 'yikketty',\n",
       " 'softrocker',\n",
       " 'mfg',\n",
       " 'calum5sos',\n",
       " 'raceoff',\n",
       " 'fucc',\n",
       " 'lesean',\n",
       " 'pleeease',\n",
       " 'tayvon',\n",
       " 'goood',\n",
       " 'img',\n",
       " 'tyga',\n",
       " 'heatvspacers',\n",
       " 'threeandahalf',\n",
       " 'leeeetsss',\n",
       " 'steppd',\n",
       " 'harrystyles',\n",
       " 'daylabour',\n",
       " 'asujhbasda',\n",
       " 'matchups',\n",
       " 'russwest44',\n",
       " '15th',\n",
       " 'wmark',\n",
       " 'guitarrista',\n",
       " 'shiting',\n",
       " 'forreal',\n",
       " 'goodgood',\n",
       " 'itineraryim',\n",
       " 'uofh',\n",
       " 'sooooooo',\n",
       " 'rvp',\n",
       " 'fantards',\n",
       " 'knw',\n",
       " 'ridics',\n",
       " 'thts',\n",
       " 'sweeeeeet',\n",
       " 'tweetgrubes',\n",
       " 'nitery',\n",
       " 'soldout',\n",
       " 'aaawww',\n",
       " 'hw',\n",
       " 'aileybayrae',\n",
       " 's2g',\n",
       " 'steelernation',\n",
       " 'aite',\n",
       " 'interestedjeff',\n",
       " 'kleinspencer',\n",
       " 'dammnnnnnn',\n",
       " 'taime',\n",
       " 'glenperkins',\n",
       " 'nfldraft',\n",
       " 'warmups',\n",
       " 'allstar',\n",
       " 'iilovecheese',\n",
       " 'watchung',\n",
       " 'welldeserved',\n",
       " 'nbahall',\n",
       " 'griffinzach',\n",
       " 'heyyyitslizz',\n",
       " 'kaepernick',\n",
       " 'godbad',\n",
       " 'joanuh',\n",
       " 'cbnc',\n",
       " 'telana',\n",
       " 'lightshow',\n",
       " 'usm',\n",
       " 'meee',\n",
       " 'margella',\n",
       " 'annies',\n",
       " 'commericals',\n",
       " 'thunderup',\n",
       " 'blatent',\n",
       " 'fucktheotherteams',\n",
       " 'westboro',\n",
       " 'lightheavyweight',\n",
       " 'oneoff',\n",
       " 'loool',\n",
       " 'knowitall',\n",
       " 'premarch',\n",
       " 'haaaaaard',\n",
       " 'bbqbrolaws',\n",
       " 'holllllleeerrrrrred',\n",
       " 'secbig',\n",
       " 'forazerbaijan',\n",
       " 'farried',\n",
       " 'tht',\n",
       " 'tearyeyed',\n",
       " 'mitad',\n",
       " 'babygirls',\n",
       " '10th',\n",
       " 'soccersix',\n",
       " 'selenagomez',\n",
       " 'ilysm',\n",
       " 'yeeaahhh',\n",
       " '3pt',\n",
       " 'poppycorn',\n",
       " 'llega',\n",
       " 'clowney',\n",
       " 'costellosband',\n",
       " '4star',\n",
       " 'sunsilk',\n",
       " 'wooooo',\n",
       " '41st',\n",
       " 'justinbieber',\n",
       " 'hawtest',\n",
       " '30',\n",
       " 'sonnen',\n",
       " 'modelsinger',\n",
       " 'studentprintz',\n",
       " 'wholllllllllle',\n",
       " 'exok',\n",
       " 'damontre',\n",
       " 'fawk',\n",
       " 'unbealivable',\n",
       " 'buffalobills',\n",
       " 'ogmfmddkd',\n",
       " 'onehalf',\n",
       " 'lhhatl',\n",
       " 'shouldve',\n",
       " 'therealjrsmith',\n",
       " 'whodatnation',\n",
       " 'NONE',\n",
       " 'wwdc',\n",
       " 'x48',\n",
       " 'bizarrolebron',\n",
       " 'lmaoooo',\n",
       " 'candiceai12',\n",
       " 'ttlyteala',\n",
       " 'nepick',\n",
       " 'tradi',\n",
       " 'hardhitting',\n",
       " 'idealware',\n",
       " 'rewtardid',\n",
       " 'sheamus',\n",
       " 'outboarding',\n",
       " 'thoo',\n",
       " 'darkskin',\n",
       " 'warriorsnuggets',\n",
       " 'pinacle',\n",
       " 'mcfc',\n",
       " 'jux',\n",
       " '4541',\n",
       " 'andriod',\n",
       " 'pleaaase',\n",
       " 'yayay',\n",
       " 'ozsome',\n",
       " 'inittowinit',\n",
       " 'schumaker',\n",
       " 'timmywhitehead',\n",
       " 'zoovie',\n",
       " 'stellasus',\n",
       " 'alones',\n",
       " 'tbfh',\n",
       " 'awwwww',\n",
       " 'goalkeepers',\n",
       " 'tshirt',\n",
       " 'sportcenter',\n",
       " 'tearyeyes',\n",
       " 'distroyd',\n",
       " 'jpmontoya',\n",
       " 'nfldraftthat',\n",
       " 'mshasnothingonme',\n",
       " 'kidrauhlflexes',\n",
       " 'kdtrey5',\n",
       " 'whove',\n",
       " 'whitesux',\n",
       " 'jox',\n",
       " 'mixtapeseps',\n",
       " 'celebritory',\n",
       " 'folllow',\n",
       " 'congressgovt',\n",
       " 'iloveyousomuch5',\n",
       " 'dayumm',\n",
       " 'harvardmagazine',\n",
       " 'stratfordavery',\n",
       " 'lmfaoo',\n",
       " 'beeotch',\n",
       " 'cryy',\n",
       " 'abcfamily',\n",
       " 'icloud',\n",
       " 'whattttt',\n",
       " 'orrrrr',\n",
       " 'yooo',\n",
       " 'techworker',\n",
       " 'lyfe',\n",
       " 'theletout',\n",
       " 'todoautosurplus',\n",
       " 'fiesty',\n",
       " 'mehd',\n",
       " 'adamlambert',\n",
       " 'fllow',\n",
       " 'tbt',\n",
       " 'famm',\n",
       " 'midnightfuze',\n",
       " 'floping',\n",
       " 'migitty',\n",
       " 'kod',\n",
       " 'awww',\n",
       " 'latenight',\n",
       " 'qbs',\n",
       " 'mutting',\n",
       " 'hardingu',\n",
       " 'offfff',\n",
       " 'mcl',\n",
       " 'lovebenzox',\n",
       " 'hahahah',\n",
       " '1hitter',\n",
       " 'allaccess',\n",
       " 'magalhaes',\n",
       " 'x70',\n",
       " 'eeeeeevvvviiiilll',\n",
       " 'ugghe',\n",
       " 'sexfactory',\n",
       " 'lawwwwdddd',\n",
       " 'pleaseeeee',\n",
       " 'amareisreal',\n",
       " 'duuude',\n",
       " 'cuuuuuuuuute',\n",
       " 'thunderingherd',\n",
       " 'wht',\n",
       " 'kachidgameboi',\n",
       " 'knockes',\n",
       " 'nevr',\n",
       " 'turnersportsej',\n",
       " 'skxidjjjxhhsjahxnja',\n",
       " 'stalkerstage',\n",
       " 'marksanford',\n",
       " 'haaaaang',\n",
       " 'lmfaaaaooooo',\n",
       " 'atbats',\n",
       " 'muuuud',\n",
       " '13',\n",
       " 'sosighit',\n",
       " 'v103atlanta',\n",
       " 'wrf',\n",
       " 'nicklomax',\n",
       " 'mainevent',\n",
       " 'almondnarry',\n",
       " 'actualllyyyy',\n",
       " 'westsidehell',\n",
       " 'caliendo',\n",
       " 'belieee',\n",
       " 'togethermoms',\n",
       " 'haaaaaaater',\n",
       " 'fiya',\n",
       " 'spingebob',\n",
       " 'bestlookingever',\n",
       " 'dreamchasers',\n",
       " 'harvick',\n",
       " 'beautybenzo',\n",
       " 'beattheblock',\n",
       " 'thirstiest',\n",
       " 'relyin',\n",
       " 'yorka',\n",
       " 'drayaface',\n",
       " 'bogut',\n",
       " 'pleeeeease',\n",
       " 'syas',\n",
       " 'phuck',\n",
       " 'halfassed',\n",
       " 'thoooouuugh',\n",
       " 'lmfaoooooo',\n",
       " '20th',\n",
       " 'askscandal',\n",
       " 'oldschool',\n",
       " 'nassib',\n",
       " 'marriageequality',\n",
       " 'harveyday',\n",
       " 'hotcole',\n",
       " 'bmt',\n",
       " '29',\n",
       " 'mayerhawthorne',\n",
       " 'mickjagger',\n",
       " 'daaanng',\n",
       " 'soooooo',\n",
       " 'shittin',\n",
       " 'miggitymac',\n",
       " 'bbm',\n",
       " 'twitcam',\n",
       " 'checc',\n",
       " 'firstofitskind',\n",
       " 'illkay',\n",
       " 'tevez',\n",
       " 'cstodd89',\n",
       " 'wft',\n",
       " '3yrs',\n",
       " 'x25',\n",
       " 'firstdraft',\n",
       " 'highestpaid',\n",
       " 'raaaiiiiders',\n",
       " 'rg3',\n",
       " 'balotelli',\n",
       " 'hossa',\n",
       " 'smackabitch',\n",
       " 'tmt',\n",
       " 'audiance',\n",
       " 'javale',\n",
       " 'migiddy',\n",
       " 'seewhatididtheir',\n",
       " 'ilysfmluke',\n",
       " 'awwww',\n",
       " 'aniexty',\n",
       " 'hellyeah',\n",
       " 'awzum',\n",
       " 'backwardsclothes',\n",
       " '40yd',\n",
       " 'theoph',\n",
       " 'trex',\n",
       " 'bayless',\n",
       " 'turiaf',\n",
       " 'hboboxing',\n",
       " 'line2',\n",
       " 'taemin',\n",
       " 'btwn',\n",
       " 'birdgang',\n",
       " 'lamicheal',\n",
       " '50',\n",
       " 'fastbreaks',\n",
       " 'joanrivers',\n",
       " '752',\n",
       " 'withchu',\n",
       " 'killaa',\n",
       " 'trulyyours2',\n",
       " 'itd',\n",
       " 'heyyyy',\n",
       " '12sec',\n",
       " 'frfr',\n",
       " 'donzell',\n",
       " '40yard',\n",
       " 'kjmayorjohnson',\n",
       " 'hienas',\n",
       " '4pt',\n",
       " '1000',\n",
       " 'naterobinson',\n",
       " 'kershawzito',\n",
       " 'disponible',\n",
       " 'rightfooted',\n",
       " 'pangosallamericancamp',\n",
       " 'panera',\n",
       " 'badu',\n",
       " 'nahh',\n",
       " 'oneman',\n",
       " 'thejetontnt',\n",
       " 'performace',\n",
       " 'omggg',\n",
       " 'sotheresthat',\n",
       " 'rtr',\n",
       " 'nonton',\n",
       " 'sfresno',\n",
       " 'spinpm',\n",
       " 'wwtt',\n",
       " '85th',\n",
       " '58',\n",
       " 'xxxxxxxxxxxxx',\n",
       " 'thras',\n",
       " 'alterego',\n",
       " '98',\n",
       " 'benzino',\n",
       " 'wondercon',\n",
       " 'damnn',\n",
       " 'hellthat',\n",
       " 'grammywinning',\n",
       " 'doubledouble',\n",
       " 'vedothesinger',\n",
       " 'oscarhe',\n",
       " 'nonhd',\n",
       " 'denyed',\n",
       " '62nd',\n",
       " 'ypu',\n",
       " 'lmaaaaaaaaaaaaaaaaaoooo',\n",
       " 'tattaz',\n",
       " 'jcolenc',\n",
       " 'chivettes',\n",
       " 'dallascowboy',\n",
       " 'lhhhh',\n",
       " 'tuneinradio',\n",
       " 'jonesgreen',\n",
       " 'simoncowell',\n",
       " '1425',\n",
       " 'kaner',\n",
       " 'tmht',\n",
       " 'twizy',\n",
       " 'dayummmmmm',\n",
       " 'whatevz',\n",
       " 'nbaontnt',\n",
       " 'couldve',\n",
       " 'boutta',\n",
       " 'dortmunddestruction',\n",
       " 'nyknicks',\n",
       " 'favv',\n",
       " 'khali',\n",
       " 'concievec',\n",
       " 'favesssss',\n",
       " 'nouisniam',\n",
       " 'ouu',\n",
       " '5050',\n",
       " 'deadass',\n",
       " 'seered',\n",
       " 'thordarkworld',\n",
       " '3year',\n",
       " 'ayyee',\n",
       " 'logic301',\n",
       " 'mosnter',\n",
       " 'hannahbush',\n",
       " 'fivehole',\n",
       " 'patriotsnation',\n",
       " 'mtn',\n",
       " 'ozil',\n",
       " 'latahhhh',\n",
       " 'sykesational',\n",
       " 'jondeuce',\n",
       " 'apologised',\n",
       " 'sportsnation',\n",
       " 'wadeshaq',\n",
       " 'noms',\n",
       " 'runnerrrgirl1',\n",
       " 'recommendeds',\n",
       " 'wattermelon',\n",
       " 'pillo',\n",
       " 'utilizar',\n",
       " 'mediadriven',\n",
       " 'adjustmentdevelopment',\n",
       " '20112012',\n",
       " 'doublefoul',\n",
       " 'fingerool',\n",
       " 'baaaaaaaaaddddd',\n",
       " 'dmcfadden20',\n",
       " 'thirdstraight',\n",
       " 'fsu',\n",
       " 'downtoearth',\n",
       " '21st',\n",
       " '112th',\n",
       " 'mowhawk',\n",
       " 'calisi',\n",
       " 'bisping',\n",
       " 'hahahahah',\n",
       " 'sweeped',\n",
       " 'terminou',\n",
       " 'shaqtin',\n",
       " 'leahmclaren',\n",
       " 'canuckssharks',\n",
       " 'hooooray',\n",
       " 'zbo',\n",
       " 'anddd',\n",
       " 'folow',\n",
       " 'dvntownsend',\n",
       " 'lagi',\n",
       " 'sportsteam',\n",
       " 'spossed',\n",
       " 'wvu',\n",
       " 'warmack',\n",
       " 'hollywoodcelebrity',\n",
       " 'youngrest',\n",
       " 'perect',\n",
       " 'iphone5',\n",
       " 'shoutout',\n",
       " 'wolfmanwerewolf',\n",
       " 'alllllll',\n",
       " 'syleena',\n",
       " 'chicagoitll',\n",
       " 'dysert',\n",
       " 'bouta',\n",
       " 'kyokushin',\n",
       " 'breh',\n",
       " 'comjng',\n",
       " 'defuk',\n",
       " 'pancackes',\n",
       " 'toshfromthebill',\n",
       " '2ndround',\n",
       " '1h',\n",
       " 'vendorsthe',\n",
       " 'shouldnt',\n",
       " 'sleeppppp',\n",
       " 'wantknows',\n",
       " 'acually',\n",
       " 'miscontrol',\n",
       " 'ianoconnor',\n",
       " 'noles',\n",
       " 'timeeee',\n",
       " 'cofc',\n",
       " 'squirtle',\n",
       " 'ansah',\n",
       " 'oonnnnn',\n",
       " 'respusha',\n",
       " 'tminus',\n",
       " 'lewondoski',\n",
       " 'xx20',\n",
       " 'strombone1',\n",
       " 'uberfa6ma',\n",
       " 'mmxii',\n",
       " 'grizclips',\n",
       " 'manryu',\n",
       " 'tunn',\n",
       " 'boardin',\n",
       " 'whopped',\n",
       " 'popsbeadles',\n",
       " 'shumperts',\n",
       " 'yishun',\n",
       " 'liamlbdr',\n",
       " 'foulthing',\n",
       " 'jetaime',\n",
       " 'hahgayy',\n",
       " '32nd',\n",
       " 'replyfollow',\n",
       " 'whysomad',\n",
       " 'wnna',\n",
       " 'murphwatkins',\n",
       " 'shaunwkeaveny',\n",
       " 'germansa',\n",
       " 'fuuuuun',\n",
       " 'cantwont',\n",
       " 'gooooood',\n",
       " 'top10',\n",
       " 'lutzenkirchen',\n",
       " 'nonqb',\n",
       " 'ashtonirwow',\n",
       " 'ammieee94',\n",
       " 'ugliests',\n",
       " '128th',\n",
       " 'solaveiregionals',\n",
       " 'singledigit',\n",
       " 'grandelaughs',\n",
       " 'gulliet',\n",
       " 'thiss',\n",
       " 'ppl',\n",
       " 'lmfaooo',\n",
       " 'babyyyyyyy',\n",
       " 'revi',\n",
       " 'olineman',\n",
       " 'rhythem',\n",
       " 'badd',\n",
       " 'receiever',\n",
       " ...}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1905292479108635"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missed)/len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_missed = [(p, t.text) for p in train_data for t in p.original + p.candidate if t.text in missed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7278404163052905"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_missed)/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='bro', tags=('O', 'NN', 'I-NP', 'O')), Token(text='from', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='757', tags=('O', 'CD', 'I-NP', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'I-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='gone', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       "  '757'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='wow', tags=('O', 'UH', 'B-INTJ', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='fsu', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NNP', 'I-NP', 'O'))], label=True),\n",
       "  'fsu'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='selected', tags=('O', 'VBD', 'B-VP', 'B-EVENT')), Token(text='as', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='2013', tags=('O', 'CD', 'I-NP', 'O')), Token(text='nfl', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='draft', tags=('O', 'NNP', 'I-NP', 'O'))], label=True),\n",
       "  '2013'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='will', tags=('O', 'MD', 'B-VP', 'O')), Token(text='be', tags=('O', 'VB', 'I-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='2013', tags=('O', 'CD', 'I-NP', 'O')), Token(text='nfl', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='draft', tags=('O', 'NNP', 'I-NP', 'O'))], label=True),\n",
       "  '2013'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='wow', tags=('O', 'UH', 'B-INTJ', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='fsu', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NNP', 'I-NP', 'O'))], label=True),\n",
       "  'fsu'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='was', tags=('O', 'VBD', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='picked', tags=('O', 'VBD', 'B-VP', 'B-EVENT')), Token(text='overall', tags=('O', 'JJ', 'B-ADJP', 'O'))], candidate=[Token(text='1st', tags=('O', 'NNP', 'B-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O')), Token(text='go', tags=('O', 'NN', 'B-NP', 'O')), Token(text='noles', tags=('O', 'VBZ', 'I-NP', 'O'))], label=True),\n",
       "  'noles'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='was', tags=('O', 'VBD', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='picked', tags=('O', 'VBD', 'B-VP', 'B-EVENT')), Token(text='overall', tags=('O', 'JJ', 'B-ADJP', 'O'))], candidate=[Token(text='so', tags=('O', 'IN', 'B-PP', 'O')), Token(text='to', tags=('O', 'TO', 'B-PP', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='boy', tags=('O', 'NN', 'I-NP', 'O')), Token(text='cbaire1', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='for', tags=('O', 'IN', 'B-PP', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='being', tags=('O', 'VBG', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       "  'cbaire1'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='nfldraft', tags=('O', 'NNP', 'I-NP', 'O'))], candidate=[Token(text='1st', tags=('O', 'NNP', 'B-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='17th', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='pick', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='nfldraft', tags=('B-geo-loc', 'NNP', 'B-NP', 'O')), Token(text='different', tags=('O', 'JJ', 'I-NP', 'O'))], label=True),\n",
       "  'nfldraft'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='nfldraft', tags=('O', 'NNP', 'I-NP', 'O'))], candidate=[Token(text='1st', tags=('O', 'NNP', 'B-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='17th', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='pick', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='nfldraft', tags=('B-geo-loc', 'NNP', 'B-NP', 'O')), Token(text='different', tags=('O', 'JJ', 'I-NP', 'O'))], label=True),\n",
       "  '17th'),\n",
       " (Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='nfldraft', tags=('O', 'NNP', 'I-NP', 'O'))], candidate=[Token(text='1st', tags=('O', 'NNP', 'B-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='taken', tags=('O', 'NN', 'I-NP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='17th', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='pick', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='nfldraft', tags=('B-geo-loc', 'NNP', 'B-NP', 'O')), Token(text='different', tags=('O', 'JJ', 'I-NP', 'O'))], label=True),\n",
       "  'nfldraft')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_missed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                            output_dim = EMBEDDING_SIZE,\n",
    "                            input_length = MAX_SEQ_LEN,\n",
    "                            weights = [EMBEDDING_MATRIX], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 18, 300)      2692800     input_12[0][0]                   \n",
      "                                                                 input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  [(None, 300), (None, 721200      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, 300), (None, 721200      embedding_3[1][0]                \n",
      "                                                                 lstm_19[0][1]                    \n",
      "                                                                 lstm_19[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            301         lstm_20[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,135,501\n",
      "Trainable params: 1,442,701\n",
      "Non-trainable params: 2,692,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, TimeDistributed, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "encoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "encoder_embedding = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "decoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "decoder_embedding = embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "outputs = Dense(1, activation='sigmoid')(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11530 samples, validate on 4142 samples\n",
      "Epoch 1/5\n",
      "11530/11530 [==============================] - 10s 874us/step - loss: 0.5272 - acc: 0.7476 - val_loss: 0.6658 - val_acc: 0.6637\n",
      "Epoch 2/5\n",
      "11530/11530 [==============================] - 10s 872us/step - loss: 0.5177 - acc: 0.7546 - val_loss: 0.7502 - val_acc: 0.6574\n",
      "Epoch 3/5\n",
      "11530/11530 [==============================] - 10s 875us/step - loss: 0.5074 - acc: 0.7631 - val_loss: 0.7033 - val_acc: 0.6007\n",
      "Epoch 4/5\n",
      "11530/11530 [==============================] - 10s 880us/step - loss: 0.5032 - acc: 0.7670 - val_loss: 0.6513 - val_acc: 0.6663\n",
      "Epoch 5/5\n",
      "11530/11530 [==============================] - 10s 879us/step - loss: 0.4989 - acc: 0.7670 - val_loss: 0.7059 - val_acc: 0.6627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e7e200518>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 5, validation_data=([padding(dev_encoder_seqs), padding(dev_decoder_seqs)], dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.94      0.78      2672\n",
      "        True       0.59      0.16      0.25      1470\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      4142\n",
      "   macro avg       0.63      0.55      0.52      4142\n",
      "weighted avg       0.64      0.66      0.59      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dev_labels, [prob > 0.5 for prob in model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.95      0.87       663\n",
      "        True       0.29      0.08      0.12       175\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       838\n",
      "   macro avg       0.54      0.51      0.50       838\n",
      "weighted avg       0.69      0.77      0.71       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, [prob > 0.5 for prob in model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_scores(filename, predicted_similarity):\n",
    "    with open(filename, 'w+') as f:\n",
    "        for estimate in predicted_similarity:                    \n",
    "            f.write(\"{}\\t{:.4f}\\n\".format(str(estimate.item() > 0.5).lower(), estimate.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_scores('PIT2015_zubovych_autoencoder.output', model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838\tzubovych\tautoencoder\t\tF: 0.125\tPrec: 0.286\tRec: 0.080\t\tP-corr: 0.060\tF1: 0.347\tPrec: 0.210\tRec: 1.000\r\n"
     ]
    }
   ],
   "source": [
    "!python SemEval-PIT2015-py3/scripts/pit2015_eval_single.py SemEval-PIT2015-py3/data/test_bin.label PIT2015_zubovych_autoencoder.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import LSTM, Lambda, concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "HIDDEN_DIM=100\n",
    "\n",
    "def exponent_neg_manhattan_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    return K.exp(-K.sum(K.abs(x[:,:hidden_size] - x[:,hidden_size:]), axis=1, keepdims=True))\n",
    "\n",
    "def exponent_neg_cosine_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    leftNorm = K.l2_normalize(x[:,:hidden_size], axis=-1)\n",
    "    rightNorm = K.l2_normalize(x[:,hidden_size:], axis=-1)\n",
    "    return K.exp(K.sum(K.prod([leftNorm, rightNorm], axis=0), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence1 (InputLayer)          (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence2 (InputLayer)          (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 18, 300)      2692800     sequence1[0][0]                  \n",
      "                                                                 sequence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  (None, 100)          160400      embedding_3[6][0]                \n",
      "                                                                 embedding_3[7][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 200)          0           lstm_23[0][0]                    \n",
      "                                                                 lstm_23[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 1)            0           concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 2,853,200\n",
      "Trainable params: 160,400\n",
      "Non-trainable params: 2,692,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_1 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence1')\n",
    "seq_2 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence2')\n",
    "\n",
    "input_1 = embedding_layer(seq_1)\n",
    "input_2 = embedding_layer(seq_2)\n",
    "\n",
    "l1 = LSTM(units=HIDDEN_DIM)\n",
    "\n",
    "l1_out = l1(input_1)\n",
    "l2_out = l1(input_2)\n",
    "\n",
    "concats = concatenate([l1_out, l2_out], axis=-1)\n",
    "\n",
    "#main_output = Lambda(exponent_neg_cosine_distance, output_shape=(1,))(concats)\n",
    "main_output = Lambda(exponent_neg_manhattan_distance, output_shape=(1,))(concats)\n",
    "#dense_ouput = Dense(1024, activation=\"relu\")(concats)\n",
    "#main_output = Dense(1, activation=\"sigmoid\")(dense_ouput)\n",
    "\n",
    "model = Model(inputs=[seq_1, seq_2], outputs=[main_output])\n",
    "\n",
    "opt = keras.optimizers.Adadelta(lr = 0.1, clipnorm=1.25)\n",
    "\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=opt,loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11530 samples, validate on 4142 samples\n",
      "Epoch 1/2\n",
      "11530/11530 [==============================] - 5s 414us/step - loss: 0.2162 - acc: 0.6674 - val_loss: 0.2311 - val_acc: 0.6388\n",
      "Epoch 2/2\n",
      "11530/11530 [==============================] - 5s 420us/step - loss: 0.2101 - acc: 0.6740 - val_loss: 0.2315 - val_acc: 0.6376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e7e221fd0>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 2, validation_data = ([padding(dev_encoder_seqs), padding(dev_decoder_seqs)], dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.86      0.75      2672\n",
      "        True       0.48      0.23      0.31      1470\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4142\n",
      "   macro avg       0.57      0.55      0.53      4142\n",
      "weighted avg       0.60      0.64      0.60      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_labels, [prob > 0.5 for prob in  model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.87      0.84       663\n",
      "        True       0.31      0.23      0.26       175\n",
      "\n",
      "   micro avg       0.73      0.73      0.73       838\n",
      "   macro avg       0.56      0.55      0.55       838\n",
      "weighted avg       0.71      0.73      0.72       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, [prob > 0.5 for prob in model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_scores('PIT2015_zubovych_MaLSTM.output', model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"SemEval-PIT2015-py3/scripts/pit2015_eval_single.py\", line 205, in <module>\r\n",
      "    print(PITEval(testlabelfile, outputfile))\r\n",
      "  File \"SemEval-PIT2015-py3/scripts/pit2015_eval_single.py\", line 181, in PITEval\r\n",
      "    return EvalSingleSystem(labelfile, outfile)\r\n",
      "  File \"SemEval-PIT2015-py3/scripts/pit2015_eval_single.py\", line 168, in EvalSingleSystem\r\n",
      "    pcorrelation = pearson(sysscores, goldscores)\r\n",
      "  File \"SemEval-PIT2015-py3/scripts/pit2015_eval_single.py\", line 32, in pearson\r\n",
      "    assert len(x) == len(y)\r\n",
      "AssertionError\r\n"
     ]
    }
   ],
   "source": [
    "!python SemEval-PIT2015-py3/scripts/pit2015_eval_single.py SemEval-PIT2015-py3/data/test.label PIT2015_zubovych_MaLSTM.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
