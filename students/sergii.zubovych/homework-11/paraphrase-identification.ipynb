{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  257M  100  257M    0     0  1955k      0  0:02:14  0:02:14 --:--:-- 2770k6k\n"
     ]
    }
   ],
   "source": [
    "!curl https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz --output numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/loretoparisi/word2vec-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gunzip numberbatch-en-17.06.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   326  100   326    0     0    765      0 --:--:-- --:--:-- --:--:--   765\n"
     ]
    }
   ],
   "source": [
    "!curl http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip --output glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.twitter.27B.zip\n",
      "  inflating: glove.twitter.27B.25d.txt  \n",
      "  inflating: glove.twitter.27B.50d.txt  \n",
      "  inflating: glove.twitter.27B.100d.txt  \n",
      "  inflating: glove.twitter.27B.200d.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "_ = glove2word2vec('glove.twitter.27B.200d.txt', tmp_file)\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gloves', 0.7430825233459473),\n",
       " ('helmet', 0.6035714149475098),\n",
       " ('cleats', 0.5549577474594116),\n",
       " ('shoe', 0.5447742938995361),\n",
       " ('boot', 0.5390670299530029),\n",
       " ('leather', 0.5314798355102539),\n",
       " ('hand', 0.5252668857574463),\n",
       " ('pads', 0.523605227470398),\n",
       " ('gear', 0.5217989683151245),\n",
       " ('coat', 0.5216744542121887)]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(['glove'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberbatch = KeyedVectors.load_word2vec_format(\"numberbatch-en-17.06.txt.gz\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('contravariant_functor', 0.9674091935157776),\n",
       " ('forgetful_functor', 0.9666687250137329),\n",
       " ('yoneda_embedding', 0.9497337937355042),\n",
       " ('endofunctor', 0.9360368847846985),\n",
       " ('representable_functor', 0.9314213991165161),\n",
       " ('cofunctor', 0.9296932220458984),\n",
       " ('natural_transformation', 0.9164144992828369),\n",
       " ('yoneda_lemma', 0.9022417664527893),\n",
       " ('coaugmentation', 0.8871707320213318),\n",
       " ('category_theory', 0.8751257658004761)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberbatch.most_similar(['functor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Phrase = namedtuple('Phrase', 'original candidate label')\n",
    "Token = namedtuple('Token', 'text tags')\n",
    "\n",
    "def split_tokens(sent):\n",
    "    tokens = []\n",
    "    for token in sent.split():\n",
    "        tags = token.split('/')\n",
    "        tokens.append(Token(tags[0].lower(), tuple(tags[1:])))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def readData(filename, eval_label, ignoreNone):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) == 7:\n",
    "                (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = fields\n",
    "            else:\n",
    "                continue\n",
    "            label = eval_label(judge)\n",
    "            if ((label is None) and ignoreNone):\n",
    "                continue\n",
    "            data.append(Phrase(split_tokens(origsenttag), split_tokens(candsenttag), label))\n",
    "    \n",
    "    return data\n",
    "                \n",
    "def eval_amt_label(label):\n",
    "    nYes = eval(label)[0]            \n",
    "    \n",
    "    if nYes >= 3:\n",
    "        return True\n",
    "    elif nYes <= 1:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "def eval_expert_label(label):\n",
    "    nYes = int(label[0])\n",
    "    \n",
    "    if nYes >= 4:\n",
    "        return True\n",
    "    elif nYes <= 2:\n",
    "        return False\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def readTrainData(filename):\n",
    "    return readData(filename, eval_amt_label, True)\n",
    "\n",
    "def readTestData(filename):\n",
    "    return readData(filename, eval_expert_label, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = readTrainData(\"SemEval-PIT2015-py3/data/train.data\")\n",
    "dev_data = readTrainData(\"SemEval-PIT2015-py3/data/dev.data\")\n",
    "test_data = [p for p in readTestData(\"SemEval-PIT2015-py3/data/test.data\") if p.label is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "838"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='bro', tags=('O', 'NN', 'I-NP', 'O')), Token(text='from', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='757', tags=('O', 'CD', 'I-NP', 'O')), Token(text='ej', tags=('B-person', 'NNP', 'I-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='gone', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='can', tags=('O', 'MD', 'B-VP', 'O')), Token(text='believe', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='went', tags=('O', 'VBD', 'B-VP', 'O')), Token(text='as', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], label=True),\n",
       " Phrase(original=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='to', tags=('O', 'TO', 'B-VP', 'O')), Token(text='go', tags=('O', 'VB', 'I-VP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'I-EVENT')), Token(text='this', tags=('O', 'DT', 'B-NP', 'O')), Token(text='draft', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='ej', tags=('B-person', 'NNP', 'B-NP', 'O')), Token(text='manuel', tags=('I-person', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='1st', tags=('O', 'CD', 'I-NP', 'O')), Token(text='qb', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='what', tags=('O', 'WP', 'I-NP', 'O'))], label=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='last', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='in', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='nevr', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='old', tags=('O', 'JJ', 'B-NP', 'O')), Token(text='ahah', tags=('O', 'JJ', 'I-NP', 'O'))], label=False),\n",
       " Phrase(original=[Token(text='all', tags=('O', 'DT', 'B-NP', 'O')), Token(text='the', tags=('O', 'DT', 'I-NP', 'O')), Token(text='home', tags=('O', 'NN', 'I-NP', 'O')), Token(text='alones', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='watching', tags=('O', 'VBG', 'I-VP', 'B-EVENT')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O'))], candidate=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='rap', tags=('O', 'NN', 'I-NP', 'O')), Token(text='battle', tags=('O', 'NN', 'I-NP', 'B-EVENT')), Token(text='at', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='end', tags=('O', 'NN', 'I-NP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='gets', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='me', tags=('O', 'PRP', 'B-NP', 'O')), Token(text='so', tags=('O', 'RB', 'B-ADVP', 'O')), Token(text='hype', tags=('O', 'JJ', 'I-ADVP', 'O'))], label=False),\n",
       " Phrase(original=[Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='ending', tags=('O', 'VBG', 'I-NP', 'B-EVENT')), Token(text='to', tags=('O', 'TO', 'I-NP', 'O')), Token(text='8', tags=('O', 'CD', 'I-NP', 'O')), Token(text='mile', tags=('O', 'NNP', 'I-NP', 'O')), Token(text='is', tags=('O', 'VBZ', 'B-VP', 'O')), Token(text='my', tags=('O', 'PRP$', 'B-NP', 'O')), Token(text='fav', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='part', tags=('O', 'NN', 'I-NP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='the', tags=('O', 'DT', 'B-NP', 'O')), Token(text='whole', tags=('O', 'JJ', 'I-NP', 'O')), Token(text='movie', tags=('O', 'NN', 'I-NP', 'B-EVENT'))], candidate=[Token(text='rabbit', tags=('O', 'NNP', 'B-NP', 'O')), Token(text='on', tags=('O', 'IN', 'B-PP', 'O')), Token(text='8', tags=('O', 'CD', 'B-NP', 'O')), Token(text='mile', tags=('O', 'NN', 'I-NP', 'O')), Token(text='out', tags=('O', 'IN', 'B-PP', 'O')), Token(text='of', tags=('O', 'IN', 'B-PP', 'O')), Token(text='place', tags=('O', 'NN', 'B-NP', 'O')), Token(text='but', tags=('O', 'CC', 'O', 'O')), Token(text='determined', tags=('O', 'VBD', 'B-VP', 'B-EVENT')), Token(text='to', tags=('O', 'TO', 'I-VP', 'O')), Token(text='make', tags=('O', 'VB', 'I-VP', 'O')), Token(text='it', tags=('O', 'PRP', 'B-NP', 'O'))], label=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "NUM = set(['1st'])\n",
    "\n",
    "def clean_sent(sent):\n",
    "    new_sent = []\n",
    "    for token in sent:\n",
    "        if token.tags[0].startswith('B-'):\n",
    "            new_sent.append(token.tags[0].split('-')[1])\n",
    "            continue\n",
    "        if token.tags[0].startswith('I-') or token.text in stopwords.words('english'):\n",
    "            continue\n",
    "        if token.tags[1] == 'CD' or token.text in NUM or token.text.isnumeric():\n",
    "            new_sent.append('number')\n",
    "            continue\n",
    "        new_sent.append(token.text)\n",
    "                            \n",
    "    return new_sent\n",
    "\n",
    "def clean_data(data):\n",
    "    return [Phrase(clean_sent(phrase.original), clean_sent(phrase.candidate), phrase.label) \\\n",
    "            for phrase in tqdm_notebook(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852ed6d2e0304b61ad719531e4028b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11530), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_train_data = clean_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c442b2084d54487b80fd08d712edd67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4142), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_dev_data = clean_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09701d20ec44f5799e34046d423640a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=838), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_test_data = clean_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Phrase(original=['person', 'number', 'qb', 'go', 'draft'], candidate=['bro', 'number', 'person', 'number', 'qb', 'gone'], label=True),\n",
       " Phrase(original=['person', 'number', 'qb', 'go', 'draft'], candidate=['believe', 'person', 'went', 'number', 'qb', 'draft'], label=True),\n",
       " Phrase(original=['person', 'number', 'qb', 'go', 'draft'], candidate=['person', 'number', 'qb'], label=True)]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8787"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def phrase_tokens(phrase):\n",
    "    return [token for token in (phrase.original + phrase.candidate)]\n",
    "    \n",
    "\n",
    "vocab = Dictionary([phrase_tokens(p) for p in clean_train_data + clean_dev_data + clean_test_data])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8788\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab) + 1 # +1 for padding\n",
    "\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11530\n",
      "11530\n",
      "11530\n"
     ]
    }
   ],
   "source": [
    "def data_to_sequences(data):\n",
    "    \n",
    "    encoder_seqs = []\n",
    "    decoder_seqs = []\n",
    "    labels = []\n",
    "    \n",
    "    for phrase in data:\n",
    "        encoder_seqs.append(text_to_sequence([t for t in phrase.original]))\n",
    "        decoder_seqs.append(text_to_sequence([t for t in phrase.candidate]))\n",
    "        labels.append(phrase.label)\n",
    "        \n",
    "    return encoder_seqs, decoder_seqs, labels \n",
    "\n",
    "train_encoder_seqs, train_decoder_seqs, train_labels = data_to_sequences(clean_train_data)\n",
    "\n",
    "print(len(train_encoder_seqs))\n",
    "print(len(train_decoder_seqs))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'number', 'qb', 'go', 'draft']\n",
      "['bro', 'number', 'person', 'number', 'qb', 'gone']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_encoder_seqs[0]))\n",
    "print(sequence_to_text(train_decoder_seqs[0]))\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_encoder_seqs, dev_decoder_seqs, dev_labels = data_to_sequences(clean_dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_seqs, test_decoder_seqs, test_labels = data_to_sequences(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LEN = max([len(seq) for seq in (train_encoder_seqs + train_decoder_seqs + \\\n",
    "                                       dev_encoder_seqs + dev_decoder_seqs + \\\n",
    "                                       test_decoder_seqs + test_encoder_seqs)])\n",
    "\n",
    "print(MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def padding(sequences):\n",
    "    return pad_sequences(sequences, maxlen=MAX_SEQ_LEN, dtype='int32', padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_MATRIX = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "def get_glove(word):\n",
    "    try:\n",
    "        return glove[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return np.zeros(200)\n",
    "\n",
    "def get_numberbatch(word):\n",
    "    try:\n",
    "        return numberbatch[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return np.zeros(300)\n",
    "\n",
    "\n",
    "missed = set([])\n",
    "\n",
    "for word, i in vocab.token2id.items():   \n",
    "    EMBEDDING_MATRIX[i] = np.append(get_numberbatch(word), get_glove(word))\n",
    "\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toooo', 'ltrauzzi', 'itsstephsoricex', 'courside', 'fbis', '3pts', 'bmthofficial', 'prototypeversion', 'advisegiver', 'rulegoverence', '81st', 'jaibrooks1', 'spursvslakers', 'dayummmmmm', 'sticksaveswag', 'ccccrrrrraaaaazzzzzzyyyyyy', 'hmmmmmmmmmmm', '40000', 'latttwoone', 'babak1', 'guideshop', 'prayforkrista', 'izamakarewicz', 'daterape', 'cutiecalum', 'thooooo', 'meeee', 'haswellbased', 'dortmunddestruction', 'at127', 'squeeeee', 'mickid', 'madriddortmund', 'playpffs', 'concievec', 'mikereiss', 'burritolirry', '90s', 'favesssss', 'nouisniam', 'andforcing', 'hardinghe', '5th', '59th', 'ilyyyy', 'kinkybootsbway', '200th', '5050', '1future', 'thordarkworld', 'dunkfaced', 'airrielle', 'subtchicago', '3year', 'nofloorseats', 'gametheyre', 'lydiais', 'allllllllll', 'logic301', 'a1', '9inning', 'hannahbush', 'fivehole', 'saramcmann', '11th', 'pointwe', 'grooooooown', 'jonnysaraceno', '52nd', '27footers', '1', 'chuddyer', 'homewreaker', 'sdchargers', 'x10', 'latahhhh', 'nonhockey', 'sykesational', 'iamsb3', 'jondeuce', 'technothriller', 'tylerwinkeljohn', 'davidarchie', 'tmcdonaldjr', 'treykell', 'boofed', '14th', 'lightskinnedmixed', 'wadeshaq', 'iphoneipad', 'hackaasik', 'calumhooding', 'quck', 'runnerrrgirl1', 'godddddd', 'recommendeds', 'bullshett', 'bispingbelcher', 'holanouis', 'wattermelon', 'solomonswisdom', 'dagauvins', 'kingdomlion', 'teamvedo', 'nucular', 'hollldddd', 'compell', 'm3', 'clipperstrippys', 'yikketty', 'mediadriven', 'adjustmentdevelopment', '52', 'kunalmerchant', 'softrocker', '5k', 'calum5sos', '8', 'sumnerpaschke', '20112012', 'raceoff', 'doublefoul', 'seemy', 'nerly', 'meshugeneh', '2014', 'fingerool', 'baaaaaaaaaddddd', 'outtt', 'anyssaayala', 'frankcaliendo', 'dmcfadden20', 'selecionou', 'dealine', 'thirdstraight', 'miggitymiggitymiggitymiggity', '12', 'clippersgrizzlies', '21st', '112th', 'heatvspacers', '28th', 'dayyy', 'thisnesrine', '35th', '9th', '1810', 'calisi', 'threeandahalf', 'leeeetsss', 'couldshould', 'steppd', 'mikaelaloves1dx', 'gezwxm87', 'daylabour', 'asujhbasda', 'pfdc', 'heatpacer', 'leahmclaren', 'sbomb', 'canuckssharks', 'instaclassic', 'anddd', 'and1', '8th', '15th', 'russwest44', 'dvntownsend', 'lensless', 'sportsteam', 'goodasgold', 'questionsobamawontanswer', 'wannnaaaa', 'rustyrockets', 'wmark', 'spossed', 'ectwins', 'adone', 'itineraryim', 'usbelgium', 'timecatconvo', 'firouzabadi', 'hollywoodcelebrity', 'steadmon', 'youngrest', 'analogie', 'heatnationthat', 'tkod', 'weswelker', 'iphone5', 'errr', 'ridics', '9ers', 'thefieldhouse', 'wolfmanwerewolf', 'alllllll', '2day', 'chicagoitll', 'shondarhimes', 'alidoee', 'tweetgrubes', 'dysert', 'aaawww', 'stellahudgens', 'comjng', 'defuk', 'kyokushin', 'pancackes', 'aileybayrae', 's2g', 'maiikfc', 'toshfromthebill', 'marxmegu', 'footrace', 'x28', '2ndround', 'wardell', 'interestedjeff', 'kleinspencer', 'o2', 'dammnnnnnn', 'meeeee', 'therecant', 'dignit', '1h', 'broskeeeeeeeez', 'vendorsthe', 'herabotdfsgtc4l', 'reporterchris', 'glenperkins', 'sleeppppp', 'wantknows', 's3', 'priscillamnyc', 'blaxpolitation', 'miscontrol', 'otog', 'dreamchasin23', 'ohhhh', 'pl0x', 'iilovecheese', 'dooped', 'ianoconnor', 'elliedelancey', 'dreamings', '1littleliongirl', 'sacramentokings', 'aaaaaaaaaaaaaahahaha', '14', 'wowwwwww', 'nbahall', 'griffinzach', 'bushcenter', 'heyyyitslizz', 'timeeee', 'godbad', 'ohhhhhh', 'joanuh', 'tayshauntony', 'cbnc', 'telana', 'oonnnnn', 'aauthorsmusic', 'hiiiiii', 'respusha', 'usmalumni', 'liveview', 'nonosquito', 'lewondoski', '127127', 'margella', 'xx20', 'strombone1', '25th', 'lemieuxdraper', '4th', 'bishhhhhhh', '0', 'careerhigh', 'wowwww', 'uberfa6ma', 'fucktheotherteams', 'azlottery', 'kuukuwa', 'grizclips', 'lightheavyweight', 'theirgroupie', 'dirtyyyyyy', 'manryu', 'popsbeadles', '980', 'startribune', 'shumperts', 'sighmon', 'humilate', 'liamlbdr', 'iamshake', 'pinkmartini', 'premarch', 'foulthing', 'bbqbrolaws', 'yussss', 'holllllleeerrrrrred', 'hahgayy', 'secbig', 'forazerbaijan', '32nd', 'farried', 'whysomad', 'tearyeyed', 'replyfollow', 'dooed', 'griffens', 'miggity', '131st', 'x9', 'cinddyyrelaa', '27th', 'murphwatkins', 'richjeff', '10th', 'moloofs', '1040', 'shaunwkeaveny', 'ayeee', 'scuffeddeflected', 'promarriage', 'str8', '22nd', '7', 'germansa', 'bumbass', 'shittttt', 'yeeaahhh', '3pt', 'poppycorn', 'mallys', 'alll', '3pointer', 'dtafted', 'cantwont', 'costellosband', 'candices', 'myyyyyyyyyy', 'ctfuuu', '4star', '37th', 'top10', 'lutzenkirchen', 'nonqb', 'ashtonirwow', 'chrissmoove', 'ammieee94', '8216', 'ugliests', '41st', 'wiretapped', '9pm', '128th', 'sissshie', 'pourjeinventoryblowout', '30', 'modelsinger', 'transportredoing', 'studentprintz', 'wholllllllllle', 'todayeven', '3rd', 'solaveiregionals', '1824', 'sharieff', 'yooooooo', 'blackstrapbbq', 'tooooo', 'unbealivable', 'singledigit', 'grandelaughs', 'ohhhhhhh', 'gulliet', 'damnbackstrom', 'searchs', 'buffalobills', 'ogmfmddkd', 'sheeeitttt', 'maddym17', '15', 'onehalf', '401', '1990s', 'babyyyyyyy', 'olineman', 'rhythem', 'rbatto', 'bacarri', 'antiair', 'sagfasgakfgafsdas', '2nyt', 'wahoooo', 'therealjrsmith', 'NONE', '5sosupdate', 'receiever', 'x48', 'wayyy', 'bizarrolebron', 'frictio', 'lmaoooo', 'candiceai12', 'peytonshead', 'x6', 'eole', 'ttlyteala', 'nepick', 'hardhitting', 'bobsaget', 'biiiiiitccchhhhhh', 'idealware', 'rewtardid', '1900hawaii', 'guilitine', 'omgggggggggggffhkeekneeieirbrvejriiehw', 'dourtmound', 'shwd', '3rdthat', 'lmfaoooo', 'outboarding', '31st', 'dortmundreal', 'warriorsnuggets', 'pinacle', '25', 'comparingcontrasting', 'chieck', '2012', 'stephencurry30', 'fightshoving', '48th', 'caliendos', '4541', 'sheeesssh', 'ukbig', 'ayyy', 'herrr', 'ozsome', 'belivin', 'newsmichael', 'sircharles', 'e3', 'todayyyy', 'threepoint', '7th', 'mairs', 'cordarelle', 'testosteronic', 'montaged', 'loveeee', 'clipgrizz', 'leandowski', 'timmywhitehead', '51', 'mazzzzz', 'stellasus', 'biggnbadd', 'frontfacing', 'nebtd', 'cinderellaill', 'hiiiii', 'eeeeverything', 'dammm', 'thourpe', 'onball', 'furori', 'tearyeyes', '35', 'distroyd', 'vakation', 'ollydawes', 'todayhell', 'thercms', 'jpmontoya', 'nfldraftthat', 'mshasnothingonme', 'plsssss', 'kidrauhlflexes', 'kdtrey5', '40th', 'fullblown', 'boomd', 'whitesux', 'outtttt', 'superbound', 'p2', '5615', 'lolll', '2k', 'raunchiness', 'ww3', 'xmaggiepayne', 'castrating', 'mixtapeseps', 'celebritory', 'centerbacks', 'meetingsstuff', 'ayeeee', 'delaplaine', 'congressgovt', 'iloveyousomuch5', 'x8', 'danfischer17', 'dinnerpowwow', 'qualitty', 'yessss', 'hipho', 'dawwwwwwg', 'shamarko21ya', 'harvardmagazine', 'hellionvladimir', 'stratfordavery', '3', 'x100', 'paaigeenicolee', 'doeeee', 'kcchiefs', '11am', 'sctop10', 'functionalities', 'goshhhhhh', 'twitchyteam', 'colbertbuschsc', 'samesex', 'legendim', 'wayyyyy', 'whattttt', 'percilla', '3ot', 'laurennolinf', 'duuuunked', 'teamnovapetey', 'orrrrr', 'saleannnnnd', '40cant', 'techworker', 'kaykay032', 'nowwww', 'chocolatevanilla', 'lmaoooooooo', 'khlarica', 'todoautosurplus', 'yamming', 'yeahhh', 'mehd', 'ovaaa', 'thecowboys', 'outshit', 'miggidy', 'oomgggg', 'x32', 'realsummerwwe', 'thecross87', 'vanphan86', 'hackashaq', 'iphonehacks', 'celticsknicks', 'chicagocultural', 'hiiii', 'midnightfuze', 'relativeperspec', 'anyhtjjng', 'floping', 'migitty', 'lawddd', 'cr7', 'yanksmets', 'mentiontag', 'threepointers', 'abbeyview', '74th', 'liebary', 'newsseattle', 'overtreatment', 'iosandroid', 'hardingu', 'taureandirect', 's4', 'mrsh', 'inefficiencies', 'ballive', 'chambered', 'offfff', 'freeeeeeeeeeee', 'glab', 'mclvkdd', '3d', 'jennabreanne13', 'bruhhhhhhhhh', 'pleaseee', 'michael5sos', 'shiii', 'lovebenzox', 'oiii', 'terriewilliams', 'expresidents', '1hitter', 'lakersk', 'eeeeeevvvviiiilll', 'x70', 'ugghe', 'sexfactory', 'lawwwwdddd', 'wowww', 'pleaseeeee', 'tasticdrews', 'hopegrace10', 'amareisreal', 'aumf', 'yellowbastard', 'spgreenwood', 'reginaboy', 'upppp', 'thunderingherd', 'blakegriffin32', 'ohdont', 'blutbad', 'cutondime25', 'pittcon', 'reversesit', 'kachidgameboi', 'knockes', 'pinecove', 'slothanator', 'boosiest', 'turnersportsej', 'q1', 'skxidjjjxhhsjahxnja', 'devaneykk', 'stalkerstage', 'marksanford', 'haaaaang', '9', 'diccckkkk', 'lmfaaaaooooo', '19', 'atbats', 'muuuud', 'goooo', '13', 'shmedium', 'bgcomgmoments', 'saaanging', 'sosighit', 'at34', 'v103atlanta', '85', 'lmboooooo', 'bitsoftheworldwhichsoundfunny', 'sbjsvsjwjw', 'prespinning', 'kerrywashington', 'sewins', 'ckwinter12', 'nicklomax', 'butler182', 'heeeeeated', 'joneschael', 'regularseason', 'almondnarry', 'samepage', 'postered', 'westsidehell', 'joeyorck', '4', '19th', '24', 'actualllyyyy', '2013', 'belieee', 'togethermoms', '100000', 'haaaaaaater', 'turbocharging', 'posturized', 'spingebob', 'roynelsonmma', 'josevancouver', 'gigure', 'bestlookingever', 'itzjessicahoex3', 'dezduron', 'surfthechannel', 'hitmansteviej', 'flynnkatie', 'momentuem', 'maddd', 'espnnfcnblog', '6', 'roughen', 'competish', 'beattheblock', 'netflixinstant', 'reginasnow', 'playofs', 'beautybenzo', 'liveeee', 'miamibucks', 'booooomm', 'shiited', 'lagbt', 'falgs', 'adamvduke', 'protags', '12th', 'relyin', 'bkst', 'yorka', '68', 'amaazinggg', 'theatrea', 'allwest', 'stacey79', 'goldsons', '1500espnreusse', 'onehanded', 'drayaface', 'pinfall', 'muniscis', '13th', 'heartingselena', 'dieit', 'overdiagnosis', 'watchibg', '119th', 'buttcentric', '10', 'cochampions', 'halfassed', 'congratrs', 'thoooouuugh', 'fullcourt', 'lensfree', 'cbaire1', 'dh2', 'loveeeee', 'atlantafalcons', 'cryinggg', 'lmfaoooooo', 'thehalfpintnyc', '20th', 'ios7', 'andrewbogut', '40m', 'foxyashell', 'asdfghjklajajdkxhskwixbelwuwpwieozkeurpvkd', 'harveyday', 'sidehug', 'hotcole', 'wildavs', 'jumpjumpin', 'stealsproper', '29', 'dortmundcant', 'mayerhawthorne', 'hackasik', 'mickjagger', 'conerback', 'ayyyy', 'davidsbackpack', 'wooohooooo', 'nacwc', 'grimble', 'toddlerlike', 'miggitymac', 'livvveeee', '93wibc', 'trub', 'lifesocial', 'lyknx', 'ahhhhhhhhhhh', 'lobbin', 'poyer', 'alskiiii', 'lemonier', 'firstofitskind', 'atbat', 'illkay', '8000', 'kylieeeee4', '957thegame', '40million', 'eamaddennfl', '1370', 'cstodd89', 'vintagezarry', 'wowwwww', 'pedalnyc', '3yrs', 'x25', 'wooooooooo', 'bullsvsnets', 'highestpaid', '5', 'tripledouble', 'firstdraft', 'jobaint', 'theyslacking', 'aize', 'raaaiiiiders', 'netsbulls', 'kreedom', 'pointguard', 'hinduon', 'amslingshots', '17th', 'g5', 'rg3', 'reald3d', '49ers', 'overunder', 'jamiesalud1d', 'ewwww', 'nckxnxkx', 'kriskross', 'khedndiby', 'r1', 'smackabitch', 'sciencegoogle', 'x39', 'jptheasshole', 'ungeekedeliteschicago', 'dawgggg', 'mikevick', 'thurmanthomas', 'perolawiberg', 'migiddy', 'seewhatididtheir', 'amarder5', 'hayjdbieber', 'fittz', 'ilysfmluke', 'you19', 'therethat', 'okiestate', 'scandalabc', 'mccelroy', '3s', 'zxxxxxxx', 'aniexty', 'aaahhh', 'ughhhhhhh', 'lollll', 'calummmm', 'walmarttarget', 'x43', '039', 'againnnn', 'purplearmy', 'awzum', '78th', 'louding', 'mannnn', 'backwardsclothes', 'bigmeeks5', '40yd', 'theoph', 'giardiniera', '05', 'lmfaoooooooooo', 'outruns', 'fromyale', 'dpbrugler', 'tomgreenlive', 'hboboxing', 'alecdaley', 'congresspersons', 'kingdont', '60th', 'line2', 'safirah', '2', 'imskipsgirl', 'prisilla', 'lamicheal', 'jbflexes', 'killllllll', 'ps3', 'tomlinshiit', '50', 'northsiderrr', 'fastbreaks', 'joanrivers', '752', 'culminated', 'wowim', 'versitile', 'baaaallin', 'realmoney', 'fivemonth', 'shamarko', 'trulyyours2', 'heyyyy', '12sec', 'pleaseeee', 'donzell', 'sumall', 'real3d', 'biggnaicaa', 'okcs', 'itsyaraaa', '40yard', 'michaelmoore', 'podiumgame', '864', 'youuuuuu', '4pt', 'sputtered', 'roubd', 'kjmayorjohnson', 'todaywhere', 'naterobinson', 'x7', '1000', 'kershawzito', '6th', 'untouchablejay4', 'nfldraft2013', 'violentcbs', 'gundagun', 'rightfooted', 'holddd', 'ashleyblove', 'ruvilla', 'lawwddd', 'x1', 'w0', 'playiiin', 'pangosallamericancamp', '2325million', 'yorkbased', 'chesmar', 'kb78', 'dellorto', 'poprap', 'toyotaowners400', 'laptimes', 'knicksboston', 'whhaaattt', 'sigb', 'brookelyn', 'vancouveryoure', 'ravensplus', 'semfinal', 'thejetontnt', '757', 'omggg', 'gkkbkn', '17', 'sotheresthat', 'davemorin', 'wellid', 'csart', 'ahhhhhh', 'ninersnation', 'offfield', 'foul2', 'lenseless', 'sfresno', 'spinpm', '110m', '85th', 'jrsmith', 'wgitmo', 'x36', 'any1', 'poopyourtrousers', 'yestrdy', 'cp3', 'madden25', '58', 'colbyculbertson', 'xxxxxxxxxxxxx', 'coog', 'krisskross', 'bargains4wahms', 'nbaguru', 'patrickquirky', 'phaty', 'jonnybones', 'bowwowymcmb', 'thras', 'stephenasmith', '243jaibrooks1', 'heatbucks', '10x', 'reald', '98', 'ashshayse', '1425', 'dsgahhgadf', 'govteducationcompanies', '2nd', 'hairim', 'hellthat', 'mensiscus', 'bihhh', 'leftfoot', 'gurlll', 'grammywinning', 'syleenajohnson', 'vedothesinger', 'someonesomething', 'horsecollar', 'mozgovd', 'canshould', 'niceeeee', 'oscarhe', 'desirableziall', 'roofed', 'f150', 'panthersoft', 'nonhd', 'ughhh', 'denyed', 'sumisin', '62nd', 'headhunt', 'dstern', 'maaaannnn', '51st', 'alleyooped', 'sanchize', 'guarddddd', 'formely', 'experimentaion', 'righttt', 'parkerighile', 'thooo', 'wooohoooo', 'lmaaaaaaaaaaaaaaaaaoooo', 'warmies', 'plazzzzzzz', 'this25', 'mysportslegion', 'goodnat', 'tattaz', 'jcolenc', 'corookies', 'greatlate', 'bayerndortmund', 'youuuu', 'thebushcenter', 'dallascowboy', 'lhhhh', 'jonesgreen', 'simoncowell', 'zdnetgoogle', 'xx11'}\n"
     ]
    }
   ],
   "source": [
    "print(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1139182883805622"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missed)/len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_missed = [(p, t) for p in clean_train_data for t in p.original + p.candidate if t in missed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3678230702515178"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_missed)/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Phrase(original=['person', 'number', 'qb', 'taken'], candidate=['bills', 'take', 'sportsteam', 'number', 'qb', 'board'], label=True),\n",
       "  'sportsteam'),\n",
       " (Phrase(original=['person', 'number', 'qb', 'picked', 'overall'], candidate=['sportsteam', 'person', 'number', 'qb', 'taken'], label=True),\n",
       "  'sportsteam'),\n",
       " (Phrase(original=['person', 'number', 'qb', 'picked', 'overall'], candidate=['boy', 'cbaire1', 'person', 'number', 'qb', 'taken'], label=True),\n",
       "  'cbaire1'),\n",
       " (Phrase(original=['person', 'number', 'qb', 'taken', 'nfldraft'], candidate=['number', 'qb', 'taken', '17th', 'pick', 'geo', 'different'], label=True),\n",
       "  '17th'),\n",
       " (Phrase(original=['person', 'number', 'qb', 'taken', 'nfldraft'], candidate=['person', 'go', 'NONE'], label=True),\n",
       "  'NONE'),\n",
       " (Phrase(original=['person', 'number', 'qb', 'taken', 'nfldraft'], candidate=['sportsteam', 'person', 'number', 'qb', 'taken'], label=True),\n",
       "  'sportsteam'),\n",
       " (Phrase(original=['person', 'number', 'qb', 'taken', 'huh'], candidate=['buffalobills', 'number', 'qb', 'taken', 'number', 'overall', 'pick'], label=False),\n",
       "  'buffalobills'),\n",
       " (Phrase(original=['person', 'coming', 'other', 'family', 'right'], candidate=['ahhhhhh', 'original', 'company', 'movies', 'person', 'family'], label=False),\n",
       "  'ahhhhhh'),\n",
       " (Phrase(original=['figures', 'patriots', 'get', 'person'], candidate=['sportsteam', 'select', 'person', 'wide', 'receiver'], label=True),\n",
       "  'sportsteam'),\n",
       " (Phrase(original=['figures', 'patriots', 'get', 'other'], candidate=['number', 'overall', 'pick', 'person', 'selected', 'sportsteam'], label=True),\n",
       "  'sportsteam')]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_missed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(input_dim = VOCAB_SIZE, \n",
    "                            output_dim = EMBEDDING_SIZE,\n",
    "                            input_length = MAX_SEQ_LEN,\n",
    "                            weights = [EMBEDDING_MATRIX], trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 13, 500)      4394000     input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  [(None, 300), (None, 961200      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  [(None, 300), (None, 961200      embedding_7[1][0]                \n",
      "                                                                 lstm_33[0][1]                    \n",
      "                                                                 lstm_33[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            301         lstm_34[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,316,701\n",
      "Trainable params: 1,922,701\n",
      "Non-trainable params: 4,394,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Embedding, TimeDistributed, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "encoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "encoder_embedding = embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "    \n",
    "decoder_inputs = Input(shape=(MAX_SEQ_LEN, ), dtype='int32',)\n",
    "decoder_embedding = embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "    \n",
    "outputs = Dense(1, activation='sigmoid')(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11530 samples, validate on 4142 samples\n",
      "Epoch 1/5\n",
      "11530/11530 [==============================] - 14s 1ms/step - loss: 0.6247 - acc: 0.6632 - val_loss: 0.6324 - val_acc: 0.6613\n",
      "Epoch 2/5\n",
      "11530/11530 [==============================] - 9s 743us/step - loss: 0.5882 - acc: 0.6970 - val_loss: 0.6305 - val_acc: 0.6649\n",
      "Epoch 3/5\n",
      "11530/11530 [==============================] - 9s 746us/step - loss: 0.5545 - acc: 0.7270 - val_loss: 0.6623 - val_acc: 0.6224\n",
      "Epoch 4/5\n",
      "11530/11530 [==============================] - 9s 749us/step - loss: 0.5405 - acc: 0.7383 - val_loss: 0.7017 - val_acc: 0.6564\n",
      "Epoch 5/5\n",
      "11530/11530 [==============================] - 9s 755us/step - loss: 0.5238 - acc: 0.7517 - val_loss: 0.6807 - val_acc: 0.6475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e5d573860>"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 5, validation_data=([padding(dev_encoder_seqs), padding(dev_decoder_seqs)], dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.88      0.76      2672\n",
      "        True       0.51      0.22      0.31      1470\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      4142\n",
      "   macro avg       0.59      0.55      0.54      4142\n",
      "weighted avg       0.61      0.65      0.60      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dev_labels, [prob.item() > 0.5 for prob in model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.89      0.84       663\n",
      "        True       0.28      0.16      0.20       175\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       838\n",
      "   macro avg       0.54      0.53      0.52       838\n",
      "weighted avg       0.69      0.74      0.71       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, [prob.item() > 0.5 for prob in model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_scores(filename, predicted_similarity):\n",
    "    with open(filename, 'w+') as f:\n",
    "        for estimate in predicted_similarity:                    \n",
    "            f.write(\"{}\\t{:.4f}\\n\".format(str(estimate.item() > 0.5).lower(), estimate.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_scores('PIT2015_zubovych_autoencoder.output', model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838\tzubovych\tautoencoder\t\tF: 0.204\tPrec: 0.283\tRec: 0.160\t\tP-corr: 0.111\tF1: 0.348\tPrec: 0.213\tRec: 0.943\r\n"
     ]
    }
   ],
   "source": [
    "!python SemEval-PIT2015-py3/scripts/pit2015_eval_single.py SemEval-PIT2015-py3/data/test_bin.label PIT2015_zubovych_autoencoder.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import LSTM, Lambda, concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "HIDDEN_DIM=300\n",
    "\n",
    "def exponent_neg_manhattan_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    return K.exp(-K.sum(K.abs(x[:,:hidden_size] - x[:,hidden_size:]), axis=1, keepdims=True))\n",
    "\n",
    "def exponent_neg_cosine_distance(x, hidden_size=HIDDEN_DIM):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "    leftNorm = K.l2_normalize(x[:,:hidden_size], axis=-1)\n",
    "    rightNorm = K.l2_normalize(x[:,hidden_size:], axis=-1)\n",
    "    return K.exp(K.sum(K.prod([leftNorm, rightNorm], axis=0), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence1 (InputLayer)          (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence2 (InputLayer)          (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 13, 500)      4394000     sequence1[0][0]                  \n",
      "                                                                 sequence2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                  (None, 300)          961200      embedding_7[12][0]               \n",
      "                                                                 embedding_7[13][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 600)          0           lstm_40[0][0]                    \n",
      "                                                                 lstm_40[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2048)         1230848     concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            2049        dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,588,097\n",
      "Trainable params: 2,194,097\n",
      "Non-trainable params: 4,394,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_1 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence1')\n",
    "seq_2 = Input(shape=(MAX_SEQ_LEN,), dtype='int32', name='sequence2')\n",
    "\n",
    "input_1 = embedding_layer(seq_1)\n",
    "input_2 = embedding_layer(seq_2)\n",
    "\n",
    "l1 = LSTM(units=HIDDEN_DIM)\n",
    "\n",
    "l1_out = l1(input_1)\n",
    "l2_out = l1(input_2)\n",
    "\n",
    "concats = concatenate([l1_out, l2_out], axis=-1)\n",
    "\n",
    "#main_output = Lambda(exponent_neg_cosine_distance, output_shape=(1,))(concats)\n",
    "#main_output = Lambda(exponent_neg_manhattan_distance, output_shape=(1,))(concats)\n",
    "dense_ouput = Dense(2048, activation=\"relu\")(concats)\n",
    "main_output = Dense(1, activation=\"sigmoid\")(dense_ouput)\n",
    "\n",
    "model = Model(inputs=[seq_1, seq_2], outputs=[main_output])\n",
    "\n",
    "opt = keras.optimizers.Adadelta(lr = 0.1, clipnorm=1.25)\n",
    "\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='mean_squared_error', metrics=['accuracy'])\n",
    "#model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=opt,loss='mean_squared_error', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11530 samples, validate on 4142 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "model.fit([padding(train_encoder_seqs), padding(train_decoder_seqs)], np.array(train_labels),\n",
    "          batch_size = 100, epochs = 5, validation_data = ([padding(dev_encoder_seqs), padding(dev_decoder_seqs)], dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00      2672\n",
      "        True       0.35      1.00      0.52      1470\n",
      "\n",
      "   micro avg       0.35      0.35      0.35      4142\n",
      "   macro avg       0.18      0.50      0.26      4142\n",
      "weighted avg       0.13      0.35      0.19      4142\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szubovych/.virtualenvs/nlp/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dev_labels, [prob.item() > 0.5 for prob in model.predict([padding(dev_encoder_seqs), padding(dev_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.00      0.00       663\n",
      "        True       0.21      1.00      0.35       175\n",
      "\n",
      "   micro avg       0.21      0.21      0.21       838\n",
      "   macro avg       0.60      0.50      0.17       838\n",
      "weighted avg       0.83      0.21      0.07       838\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_labels, [prob.item() > 0.5 for prob in model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_scores('PIT2015_zubovych_MaLSTM.output', model.predict([padding(test_encoder_seqs), padding(test_decoder_seqs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838\tzubovych\tMaLSTM\t\tF: 0.346\tPrec: 0.209\tRec: 1.000\t\tP-corr: 0.248\tF1: 0.419\tPrec: 0.322\tRec: 0.600\r\n"
     ]
    }
   ],
   "source": [
    "!python SemEval-PIT2015-py3/scripts/pit2015_eval_single.py SemEval-PIT2015-py3/data/test_bin.label PIT2015_zubovych_MaLSTM.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
