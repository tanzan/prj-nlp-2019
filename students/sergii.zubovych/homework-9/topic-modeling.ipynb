{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Спочатку завантажуємо word embeddings для української мови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://lang.org.ua/static/downloads/models/news.lowercased.tokenized.word2vec.300d.bz2 --output news.lowercased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bunzip2 news.lowercased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 1.49 s, total: 1min 42s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "%time wv = KeyedVectors.load_word2vec_format('news.lowercased.tokenized.word2vec.300d', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('дієслово', 0.6502863764762878),\n",
       " ('слівце', 0.6484909653663635),\n",
       " ('словосполучення', 0.6456568241119385),\n",
       " ('гасло', 0.5913079977035522),\n",
       " ('слово**', 0.555127739906311),\n",
       " (\"прислів'я\", 0.5407627820968628),\n",
       " ('письмо', 0.5235773324966431),\n",
       " ('прізвище', 0.52119380235672),\n",
       " ('пророцтво', 0.5125285983085632),\n",
       " ('ремесло', 0.5058826804161072)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('слово')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потім розпаковуємо та завантажуємо дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace 1551/Інші-Подяки.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip -q ../../../tasks/1551.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аварійний--травмонебезпечний-стан-утримання-об-єктів-благоустрою.txt\r\n",
      "Бажаючі-отримати--Картки-киянина--КК--.txt\r\n",
      "Будівництво-АЗС.txt\r\n",
      "Будівництво-в-нічний-час.txt\r\n",
      "Будівництво-дооблаштування-дитячого-майданчику.txt\r\n",
      "Будівництво--дооблаштування-спортивних-майданчиків.txt\r\n",
      "Будівництво-та-реконструкція-об-єктів-освіти.txt\r\n",
      "Взаємовідносини-з-сусідами.txt\r\n",
      "Вивезення--утилізація-твердих-та-негабаритних-відходів.txt\r\n",
      "Видалення-аварійних--пошкоджених-хворобами-дерев.txt\r\n",
      "Видача-розрахункових-книжок--квитанцій--довідок.txt\r\n",
      "Вилов-безпритульних-тварин.txt\r\n",
      "Вирізування--кронування--гілля-дерев.txt\r\n",
      "Виток-холодної-води-на-поверхню.txt\r\n",
      "Відновлення-благоустрою-після-вик--планових-аварійних-робіт-на-об-єктах-благоуст.txt\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls 1551 | head -n 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1551/Незадовільна-температура-ГВП.txt',\n",
       " '1551/Незадовільне-обслуговування-в-амбулаторно-поліклінічних-установах.txt',\n",
       " '1551/Відсутнє-електропостачання.txt',\n",
       " '1551/Порушення-правил-тиші--після-------.txt',\n",
       " '1551/Неякісне-ХВП.txt',\n",
       " '1551/Нанесення-дорожньої-розмітки.txt',\n",
       " '1551/Робота-циркуляційної-системи.txt',\n",
       " '1551/Встановлення-світлофора.txt',\n",
       " '1551/Завезення-піску-на-дитячий-майданчик.txt',\n",
       " '1551/Скошування-трави.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"1551/*\")\n",
    "\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "Document = namedtuple('Document', 'id topic_id tags content')\n",
    "\n",
    "def parse_tags(file):\n",
    "    return [tag for tag in os.path.basename(file.name)[:-4].split('-') if tag]\n",
    "\n",
    "def parse_topic_file(topic_id, filename):\n",
    "    documents = []    \n",
    "    with open(filename) as f:\n",
    "        tags = parse_tags(f)        \n",
    "        _id = None\n",
    "        idx = -1        \n",
    "        for line in f:            \n",
    "            if line.strip().isnumeric():\n",
    "                _id = int(line.strip())\n",
    "                documents.append(Document(_id, topic_id, tags, []))\n",
    "                idx +=1\n",
    "                continue\n",
    "            if not (_id is None) and line.strip():                \n",
    "                documents[idx].content.append(line.strip())                \n",
    "    \n",
    "    return [doc._replace(content = ''.join(doc.content)) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114348"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents = [doc for topic_id, file in enumerate(files) \\\n",
    "                 for doc in parse_topic_file(topic_id, file) if len(doc.content) > 0]\n",
    "\n",
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=2697865, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Недогрев горячей воды (вода нормальной температуры подавалась неделю с15 по 23, до этого была частичная подача горячей воды (пару часов вечером и ночью горячая), остальное время теплой), сейчас опять температура порядка 40 градусов. Эта ситуация продолжается на фоне постоянного недогрева батарей в квартире, ДУ 12 ККЕУ  МОУкраины  не реагирует на ситуацию.'),\n",
       " Document(id=3170827, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Из горячего крана течет холодная вода, в вечернее и утреннее время купаться нет возможности. Необходимо или пересчитывать тарифы или включать горячую воду.'),\n",
       " Document(id=3165270, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Я поживаю на 6 этаже 9и - этажного дома на протяжении долгого промежутка времени у нас в квартире из крана горячей воды, особенно утром и в первой половине дня течёт если не холодная вода, то вода чуть тёплая. По утрам для того чтобы совершить утренний туалет приходится долго спускать воду (и эта проблема у большей части жильцов нашего дома). В свете того, что с мая месяца у нас очень выросли тарифы на горячую воду, меня интересует вопрос - почему я должна платить деньги за некачественную услугу. Огромная просьба посодействовать в решении данной проблемы. Местные сантехники подтверждают, что проблемы с горячей водой не только в нашей квартире, но решить эту проблему они не могут, так как это от них не зависит.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер фільтруємо документи з українською мовою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.detector import LangDetectException\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def memoize(filename, compute):  \n",
    "    \n",
    "    fullname = filename + '.can'\n",
    "    \n",
    "    if os.path.isfile(fullname):\n",
    "        with open(fullname, 'rb') as f:                        \n",
    "            return pickle.load(f)\n",
    "    \n",
    "    result = compute()\n",
    "    with open(fullname, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def is_uk(text):\n",
    "    \n",
    "    if len(text):\n",
    "        try:\n",
    "            return detect(text[:1024]) == 'uk'\n",
    "        except LangDetectException as e:\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "uk_documents = memoize('uk_documents', \n",
    "                       lambda: [doc for doc in tqdm_notebook(all_documents) if is_uk(doc.content)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивимся на дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "uk_doc_df = pandas.DataFrame([doc._replace(tags = \"-\".join(doc.tags)) for doc in uk_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3152784</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Відсутнітність горячого водопостачання належно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3143050</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Добрий вечір.Прошу розібратися з проблемою нев...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3142427</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>На моє звернення № Г-6623 відповідь написав ди...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130991</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Доброго дня! Вже більше двох тижнів гаряче вод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2405990</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>На звернення:Номер звернення:\\tГ-6478Зареєстро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3115494</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Звертаюсь до Вас стосовно вирішення питання, щ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3104107</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Доброго дня!!! Моє звернення від 02.12.14 #Г-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3091571</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Протягом останнього тижня гаряча вода недостат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2690156</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Прошу прийняти необхідні заходи по покращенню ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2748419</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>немає  температури гарячої води</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  topic_id                          tags  \\\n",
       "0  3152784         0  Незадовільна-температура-ГВП   \n",
       "1  3143050         0  Незадовільна-температура-ГВП   \n",
       "2  3142427         0  Незадовільна-температура-ГВП   \n",
       "3  3130991         0  Незадовільна-температура-ГВП   \n",
       "4  2405990         0  Незадовільна-температура-ГВП   \n",
       "5  3115494         0  Незадовільна-температура-ГВП   \n",
       "6  3104107         0  Незадовільна-температура-ГВП   \n",
       "7  3091571         0  Незадовільна-температура-ГВП   \n",
       "8  2690156         0  Незадовільна-температура-ГВП   \n",
       "9  2748419         0  Незадовільна-температура-ГВП   \n",
       "\n",
       "                                             content  \n",
       "0  Відсутнітність горячого водопостачання належно...  \n",
       "1  Добрий вечір.Прошу розібратися з проблемою нев...  \n",
       "2  На моє звернення № Г-6623 відповідь написав ди...  \n",
       "3  Доброго дня! Вже більше двох тижнів гаряче вод...  \n",
       "4  На звернення:Номер звернення:\\tГ-6478Зареєстро...  \n",
       "5  Звертаюсь до Вас стосовно вирішення питання, щ...  \n",
       "6  Доброго дня!!! Моє звернення від 02.12.14 #Г-1...  \n",
       "7  Протягом останнього тижня гаряча вода недостат...  \n",
       "8  Прошу прийняти необхідні заходи по покращенню ...  \n",
       "9                    немає  температури гарячої води  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.182900e+04</td>\n",
       "      <td>61829.000000</td>\n",
       "      <td>61829</td>\n",
       "      <td>61829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188</td>\n",
       "      <td>56061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Відсутність-ГВП</td>\n",
       "      <td>Відсутнє гаряче водопостачання</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6564</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.159625e+06</td>\n",
       "      <td>105.551731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.084360e+07</td>\n",
       "      <td>55.922291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.841555e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.083712e+06</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.245460e+06</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.013102e+09</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id      topic_id             tags  \\\n",
       "count   6.182900e+04  61829.000000            61829   \n",
       "unique           NaN           NaN              188   \n",
       "top              NaN           NaN  Відсутність-ГВП   \n",
       "freq             NaN           NaN             6564   \n",
       "mean    3.159625e+06    105.551731              NaN   \n",
       "std     1.084360e+07     55.922291              NaN   \n",
       "min     1.000000e+01      0.000000              NaN   \n",
       "25%     2.841555e+06     58.000000              NaN   \n",
       "50%     3.083712e+06    121.000000              NaN   \n",
       "75%     3.245460e+06    150.000000              NaN   \n",
       "max     2.013102e+09    187.000000              NaN   \n",
       "\n",
       "                               content  \n",
       "count                            61829  \n",
       "unique                           56061  \n",
       "top     Відсутнє гаряче водопостачання  \n",
       "freq                                46  \n",
       "mean                               NaN  \n",
       "std                                NaN  \n",
       "min                                NaN  \n",
       "25%                                NaN  \n",
       "50%                                NaN  \n",
       "75%                                NaN  \n",
       "max                                NaN  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <th>Відсутність-ГВП</th>\n",
       "      <td>6564</td>\n",
       "      <td>6564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <th>Укладання-та-ремонт-асфальтного-покриття</th>\n",
       "      <td>3628</td>\n",
       "      <td>3628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>Відсутність-опалення</th>\n",
       "      <td>3142</td>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <th>Перевірка-дозвільної-документації-демонтаж-кіосків-ларків</th>\n",
       "      <td>2199</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <th>Прибирання-та-санітарний-стан-територій</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <th>Технічний-стан-проїжджих-частин-вулиць-та-тротуарів</th>\n",
       "      <td>1303</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <th>Відновлення-благоустрою-після-вик-планових-аварійних-робіт-на-об-єктах-благоуст</th>\n",
       "      <td>1289</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <th>Відсутність-освітлення-у-під-їзді-за-відсутності-несправності-лампочок</th>\n",
       "      <td>1256</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>Не-працює-пасажирський-ліфт</th>\n",
       "      <td>1220</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <th>Ремонт-під-їзду</th>\n",
       "      <td>1198</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>Незадовільна-температура-ГВП</th>\n",
       "      <td>1116</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <th>Перерахунок-та-нарахування-плати-за-інші-види-житлово-комунальних-послуг</th>\n",
       "      <td>1097</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <th>Про-розгляд-звернень-громадян</th>\n",
       "      <td>993</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <th>Відсутність-опалення-по-стояку</th>\n",
       "      <td>989</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <th>ГЛ-Несанкціонована-торгівля</th>\n",
       "      <td>826</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <th>Прибирання-приміщень</th>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <th>Відсутнє-ХВП</th>\n",
       "      <td>785</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <th>Освітлення-в-приміщенні-й-при-вході-в-нього</th>\n",
       "      <td>743</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <th>Інші-технічні-недоліки-стану-ліфту</th>\n",
       "      <td>695</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <th>Ремонт-дахів</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <th>Перевірка-наявності-дозволів-на-виконання-будівельних-робіт</th>\n",
       "      <td>646</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <th>Незадовільна-температура-опалення</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>Будівництво-дооблаштування-дитячого-майданчику</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <th>Утримання-підвалів-колясочних-технічних-поверхів</th>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <th>Відсутність-освітлення-на-опорних-стовпах-за-відсутності-несправності-лампочок</th>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <th>Питання-освітлення-на-опорних-стовпах</th>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <th>Встановлення-та-експлуатація-лічильників-на-водопостачання</th>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>Робота-світлофора</th>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <th>Стихійне-сміттєзвалище</th>\n",
       "      <td>477</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>Робота-циркуляційної-системи</th>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <th>Технічне-обслуговування-систем-тепло-водопостачання-та-водовідведення-і-зливов</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>Демонтаж-рекламних-конструкцій-і-вивісок</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <th>Скління-та-ремонт-вікон-на-сходових-клітинах</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>Встановлення-лічильників-на-опалення</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>Вологе-прибирання-приміщень</th>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>Незадовільний-вивіз-сміття-з-контейнерів-та-урн-для-сміття</th>\n",
       "      <td>383</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <th>Встановлення-та-експлуатація-дорожніх-знаків</th>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Нанесення-дорожньої-розмітки</th>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <th>Паркування-авто-у-місцях-загального-користування</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <th>Встановлення-сміттєвих-контейнерів-та-урн-для-сміття</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <th>Видалення-аварійних-пошкоджених-хворобами-дерев</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <th>Контроль-за-станом-рекламних-засобів</th>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>Перевірка-дозвільної-документації-демонтаж-літніх-майданчиків-кафе-ресторанів</th>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <th>Інші-Подяки</th>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>Встановлення-огородження-зеленої-зони</th>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <th>Знищення-омели-амброзії-та-рослин-паразитів</th>\n",
       "      <td>283</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>Аварійний-травмонебезпечний-стан-утримання-об-єктів-благоустрою</th>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <th>Не-працює-вантажний-ліфт</th>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <th>Встановлення-сигнальних-стовпчиків-бар-єрних-огороджень-бордюрів</th>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <th>Ремонт-і-обслуговування-сміттєпроводів-та-сміттєзбірників-в-приміщенні</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               id  content\n",
       "topic_id tags                                                             \n",
       "138      Відсутність-ГВП                                     6564     6564\n",
       "180      Укладання-та-ремонт-асфальтного-покриття            3628     3628\n",
       "27       Відсутність-опалення                                3142     3142\n",
       "127      Перевірка-дозвільної-документації-демонтаж-кіос...  2199     2199\n",
       "67       Прибирання-та-санітарний-стан-територій             2005     2005\n",
       "79       Технічний-стан-проїжджих-частин-вулиць-та-троту...  1303     1303\n",
       "155      Відновлення-благоустрою-після-вик-планових-авар...  1289     1289\n",
       "121      Відсутність-освітлення-у-під-їзді-за-відсутност...  1256     1256\n",
       "58       Не-працює-пасажирський-ліфт                         1220     1220\n",
       "183      Ремонт-під-їзду                                     1198     1198\n",
       "0        Незадовільна-температура-ГВП                        1116     1116\n",
       "101      Перерахунок-та-нарахування-плати-за-інші-види-ж...  1097     1097\n",
       "143      Про-розгляд-звернень-громадян                        993      993\n",
       "88       Відсутність-опалення-по-стояку                       989      989\n",
       "173      ГЛ-Несанкціонована-торгівля                          826      826\n",
       "161      Прибирання-приміщень                                 800      800\n",
       "176      Відсутнє-ХВП                                         785      785\n",
       "171      Освітлення-в-приміщенні-й-при-вході-в-нього          743      743\n",
       "148      Інші-технічні-недоліки-стану-ліфту                   695      695\n",
       "178      Ремонт-дахів                                         671      671\n",
       "22       Перевірка-наявності-дозволів-на-виконання-будів...   646      646\n",
       "174      Незадовільна-температура-опалення                    635      635\n",
       "68       Будівництво-дооблаштування-дитячого-майданчику       633      633\n",
       "181      Утримання-підвалів-колясочних-технічних-поверхів     617      617\n",
       "164      Відсутність-освітлення-на-опорних-стовпах-за-ві...   538      538\n",
       "109      Питання-освітлення-на-опорних-стовпах                516      516\n",
       "154      Встановлення-та-експлуатація-лічильників-на-вод...   481      481\n",
       "10       Робота-світлофора                                    480      480\n",
       "41       Стихійне-сміттєзвалище                               477      477\n",
       "6        Робота-циркуляційної-системи                         473      473\n",
       "131      Технічне-обслуговування-систем-тепло-водопостач...   458      458\n",
       "114      Демонтаж-рекламних-конструкцій-і-вивісок             458      458\n",
       "116      Скління-та-ремонт-вікон-на-сходових-клітинах         431      431\n",
       "33       Встановлення-лічильників-на-опалення                 430      430\n",
       "16       Вологе-прибирання-приміщень                          407      407\n",
       "40       Незадовільний-вивіз-сміття-з-контейнерів-та-урн...   383      383\n",
       "57       Встановлення-та-експлуатація-дорожніх-знаків         359      359\n",
       "5        Нанесення-дорожньої-розмітки                         353      353\n",
       "69       Паркування-авто-у-місцях-загального-користування     351      351\n",
       "78       Встановлення-сміттєвих-контейнерів-та-урн-для-с...   348      348\n",
       "136      Видалення-аварійних-пошкоджених-хворобами-дерев      321      321\n",
       "62       Контроль-за-станом-рекламних-засобів                 318      318\n",
       "32       Перевірка-дозвільної-документації-демонтаж-літн...   316      316\n",
       "168      Інші-Подяки                                          307      307\n",
       "105      Встановлення-огородження-зеленої-зони                289      289\n",
       "74       Знищення-омели-амброзії-та-рослин-паразитів          283      283\n",
       "14       Аварійний-травмонебезпечний-стан-утримання-об-є...   280      280\n",
       "145      Не-працює-вантажний-ліфт                             275      275\n",
       "128      Встановлення-сигнальних-стовпчиків-бар-єрних-ог...   273      273\n",
       "112      Ремонт-і-обслуговування-сміттєпроводів-та-смітт...   265      265"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.groupby(['topic_id', 'tags']).count().sort_values(['id'], ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виділяємо лейбли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61839"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_labels = np.array([doc.topic_id for doc in uk_documents])\n",
    "len(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "І розбиваємо дані на тренувальні і тестові."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_documents, test_documents, train_topic_labels, test_topic_labels = \\\n",
    "    train_test_split(uk_documents, topic_labels, random_state = 26, test_size = 0.3, stratify = topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43287\n",
      "43287\n"
     ]
    }
   ],
   "source": [
    "print(len(train_documents))\n",
    "print(len(train_topic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18552\n",
      "18552\n"
     ]
    }
   ],
   "source": [
    "print(len(test_documents))\n",
    "print(len(test_topic_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Будуємо бейзлайн, знаходимо суму векторів слів по кожному документу і використовуємо kNN на знайденних векторах. Для порівняння векторів застосовуємо cosine similarity. Перед знаходженням суми векторів, документ токенізується та видаляються stop words. Знайдені вектори нормалізуються, в такому випадку eclidean distance для kNN має той самий ефект що й cosine distance, при цьому алгоритм дозволяє використовувати більш ефективні структури данних, такі як, наприклад, k-d tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize_uk\n",
    "import string\n",
    "import html\n",
    "import re\n",
    "\n",
    "with open('uk_stop_words.txt') as f:\n",
    "    STOP_WORDS = f.read().split()\n",
    "    \n",
    "EXT_PUNCTUATION = \"”...«»№\"\n",
    "\n",
    "def contain_numbers(s):\n",
    "    return bool(re.search(r'\\d', s))\n",
    "\n",
    "def non_stop_word(word):\n",
    "    return not (word in string.punctuation or word in EXT_PUNCTUATION \\\n",
    "                or word in STOP_WORDS or contain_numbers(word) or len(word) < 2)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if non_stop_word(token.lower())]\n",
    "\n",
    "def tokenize_doc(doc):\n",
    "    return [word.lower() for word in \\\n",
    "            remove_stop_words(tokenize_uk.tokenize_words(html.unescape(doc.content)))]\n",
    "\n",
    "def normalize_vec(x):\n",
    "    m = np.max(x)\n",
    "    if m > 0.0:\n",
    "        return x/np.sqrt(np.dot(x,x))\n",
    "    return x\n",
    "    \n",
    "def doc_to_sum_vec(doc):\n",
    "    words = tokenize_doc(doc)    \n",
    "    vec = np.zeros(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec += wv[word]\n",
    "        except KeyError as e:            \n",
    "            pass\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рахуємо вектори для тренувальних і тестових документів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aab569043db443cb51da82378ff206a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_doc_sum_vecs = np.array([doc_to_sum_vec(doc) for doc in tqdm_notebook(train_documents)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b40a9d274514fee83be411635739579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_doc_sum_vecs = np.array([doc_to_sum_vec(doc) for doc in tqdm_notebook(test_documents)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN+sum vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренуємо модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels):\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_labels = train_labels\n",
    "        self.test_vectors = test_vectors\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.fit(self.train_vectors, self.train_labels)\n",
    "        self.topics_predicted = self.model.predict(self.test_vectors)\n",
    "        \n",
    "    def test(self):\n",
    "        self.test_report = classification_report(self.test_labels, self.topics_predicted)\n",
    "        print(classification_report(self.test_labels, self.topics_predicted))  \n",
    "\n",
    "class KnnModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels, n = 1):\n",
    "        super().__init__(np.array([normalize_vec(doc) for doc in train_vectors]),\\\n",
    "                       train_labels,\\\n",
    "                       np.array([normalize_vec(doc) for doc in test_vectors]),\\\n",
    "                       test_labels)                \n",
    "        self.model = KNeighborsClassifier(n_neighbors = n, algorithm='kd_tree', metric = 'euclidean', n_jobs = 6)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KnnModel(train_doc_sum_vecs, train_topic_labels, test_doc_sum_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 14s, sys: 282 ms, total: 11min 15s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%time knn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.55      0.50       335\n",
      "           1       0.77      0.80      0.79        41\n",
      "           2       0.55      0.30      0.39        79\n",
      "           3       0.38      0.28      0.32        39\n",
      "           4       0.88      0.32      0.47        22\n",
      "           5       0.42      0.30      0.35       106\n",
      "           6       0.22      0.32      0.26       142\n",
      "           7       0.25      0.32      0.28        62\n",
      "           8       0.81      0.43      0.57        30\n",
      "           9       0.40      0.36      0.38        22\n",
      "          10       0.57      0.53      0.55       144\n",
      "          11       0.21      0.17      0.19        29\n",
      "          12       0.44      0.36      0.40        22\n",
      "          13       0.33      0.31      0.32        45\n",
      "          14       0.23      0.18      0.20        84\n",
      "          15       0.40      0.25      0.31        16\n",
      "          16       0.51      0.48      0.49       122\n",
      "          17       0.25      0.19      0.21        16\n",
      "          18       0.29      0.28      0.29        53\n",
      "          19       0.36      0.26      0.30        19\n",
      "          20       0.83      0.31      0.45        16\n",
      "          21       0.21      0.25      0.23        24\n",
      "          22       0.46      0.43      0.45       194\n",
      "          23       0.81      0.49      0.61        51\n",
      "          24       0.46      0.40      0.43        47\n",
      "          25       0.29      0.32      0.31        28\n",
      "          26       0.50      0.24      0.32        25\n",
      "          27       0.55      0.63      0.59       943\n",
      "          28       0.31      0.30      0.31        56\n",
      "          29       0.25      0.33      0.29        63\n",
      "          30       0.23      0.16      0.19        44\n",
      "          31       0.25      0.04      0.07        25\n",
      "          32       0.53      0.55      0.54        95\n",
      "          33       0.54      0.59      0.56       129\n",
      "          34       0.40      0.47      0.43        45\n",
      "          35       0.21      0.20      0.21        35\n",
      "          36       0.24      0.22      0.23        50\n",
      "          37       0.33      0.23      0.27        40\n",
      "          38       0.53      0.41      0.46        39\n",
      "          39       0.41      0.34      0.37        44\n",
      "          40       0.55      0.57      0.56       115\n",
      "          41       0.41      0.36      0.38       143\n",
      "          42       0.57      0.65      0.61        62\n",
      "          43       0.30      0.15      0.20        41\n",
      "          44       0.19      0.21      0.20        29\n",
      "          45       0.32      0.25      0.28        71\n",
      "          46       0.51      0.43      0.47        56\n",
      "          47       0.22      0.19      0.21        21\n",
      "          48       0.11      0.12      0.12        16\n",
      "          49       0.42      0.26      0.32        19\n",
      "          50       0.36      0.20      0.26        44\n",
      "          51       0.44      0.20      0.28        20\n",
      "          52       0.13      0.22      0.16        27\n",
      "          53       0.44      0.47      0.45        53\n",
      "          54       0.34      0.56      0.43        18\n",
      "          55       0.17      0.23      0.20        26\n",
      "          56       0.56      0.32      0.41        28\n",
      "          57       0.32      0.38      0.35       108\n",
      "          58       0.55      0.52      0.54       366\n",
      "          59       0.23      0.15      0.18        74\n",
      "          60       0.30      0.29      0.30        34\n",
      "          61       0.23      0.29      0.25        24\n",
      "          62       0.45      0.39      0.42        95\n",
      "          63       0.17      0.11      0.13        18\n",
      "          64       0.67      0.54      0.60        37\n",
      "          65       0.62      0.33      0.43        15\n",
      "          66       0.28      0.30      0.29        33\n",
      "          67       0.51      0.58      0.54       602\n",
      "          68       0.49      0.61      0.54       190\n",
      "          69       0.27      0.31      0.29       105\n",
      "          70       0.55      0.44      0.49        36\n",
      "          71       0.43      0.27      0.33        48\n",
      "          72       0.27      0.18      0.22        22\n",
      "          73       0.23      0.27      0.25        33\n",
      "          74       0.79      0.76      0.78        85\n",
      "          75       0.52      0.53      0.52        30\n",
      "          76       0.61      0.46      0.52        50\n",
      "          77       0.32      0.21      0.25        43\n",
      "          78       0.41      0.37      0.39       104\n",
      "          79       0.39      0.42      0.40       391\n",
      "          80       0.20      0.06      0.09        17\n",
      "          81       0.67      0.64      0.65        28\n",
      "          82       0.24      0.30      0.27        20\n",
      "          83       0.23      0.12      0.16        24\n",
      "          84       0.37      0.42      0.39        53\n",
      "          85       0.57      0.35      0.43        23\n",
      "          86       0.40      0.23      0.29        74\n",
      "          87       0.24      0.35      0.29        34\n",
      "          88       0.40      0.38      0.39       297\n",
      "          89       0.55      0.45      0.49        40\n",
      "          90       0.30      0.19      0.23        16\n",
      "          91       0.33      0.26      0.29        39\n",
      "          92       0.32      0.35      0.33        26\n",
      "          93       0.81      0.59      0.68        22\n",
      "          94       0.29      0.30      0.29        20\n",
      "          95       0.25      0.07      0.11        42\n",
      "          96       0.27      0.14      0.19        21\n",
      "          97       0.65      0.46      0.54        24\n",
      "          98       0.35      0.36      0.35        25\n",
      "          99       0.50      0.41      0.45        34\n",
      "         100       0.22      0.40      0.29        55\n",
      "         101       0.50      0.57      0.53       329\n",
      "         102       0.67      0.43      0.52        75\n",
      "         103       0.46      0.32      0.37        19\n",
      "         104       0.81      0.58      0.68        38\n",
      "         105       0.20      0.15      0.17        87\n",
      "         106       0.16      0.30      0.21        47\n",
      "         107       0.69      0.28      0.40        32\n",
      "         108       0.78      0.67      0.72        21\n",
      "         109       0.38      0.37      0.37       155\n",
      "         110       0.28      0.30      0.29        66\n",
      "         111       0.50      0.65      0.57        20\n",
      "         112       0.19      0.19      0.19        80\n",
      "         113       0.38      0.45      0.41        56\n",
      "         114       0.43      0.42      0.43       137\n",
      "         115       0.25      0.18      0.21        17\n",
      "         116       0.46      0.49      0.47       129\n",
      "         117       0.20      0.15      0.17        78\n",
      "         118       0.30      0.28      0.29        39\n",
      "         119       0.33      0.30      0.31        47\n",
      "         120       0.26      0.43      0.32        14\n",
      "         121       0.48      0.47      0.48       377\n",
      "         122       0.56      0.61      0.58        38\n",
      "         123       0.24      0.33      0.28        78\n",
      "         124       0.19      0.16      0.17        25\n",
      "         125       0.34      0.40      0.37        73\n",
      "         126       0.79      0.75      0.77        20\n",
      "         127       0.53      0.52      0.53       660\n",
      "         128       0.20      0.29      0.24        82\n",
      "         129       0.53      0.50      0.51        20\n",
      "         130       0.12      0.06      0.08        18\n",
      "         131       0.23      0.25      0.24       137\n",
      "         132       0.26      0.23      0.24        43\n",
      "         133       0.35      0.24      0.28        55\n",
      "         134       0.52      0.44      0.47        64\n",
      "         135       0.34      0.28      0.31        57\n",
      "         136       0.44      0.36      0.40        96\n",
      "         137       0.31      0.21      0.25        61\n",
      "         138       0.69      0.73      0.71      1969\n",
      "         139       0.44      0.35      0.39        54\n",
      "         140       0.25      0.19      0.21        16\n",
      "         141       0.38      0.40      0.39        25\n",
      "         142       0.18      0.24      0.21        17\n",
      "         143       0.37      0.35      0.36       298\n",
      "         144       0.14      0.10      0.11        41\n",
      "         145       0.46      0.52      0.49        83\n",
      "         146       0.56      0.40      0.47        25\n",
      "         147       0.30      0.12      0.17        25\n",
      "         148       0.34      0.39      0.36       209\n",
      "         149       0.46      0.40      0.43        15\n",
      "         150       0.30      0.36      0.33        61\n",
      "         151       0.27      0.30      0.29        20\n",
      "         152       0.42      0.25      0.31        20\n",
      "         153       0.47      0.24      0.31        34\n",
      "         154       0.52      0.52      0.52       144\n",
      "         155       0.42      0.43      0.42       387\n",
      "         156       0.24      0.21      0.23        56\n",
      "         157       0.18      0.12      0.15        24\n",
      "         158       0.40      0.37      0.39        51\n",
      "         159       0.57      0.35      0.43        23\n",
      "         160       0.27      0.27      0.27        44\n",
      "         161       0.48      0.43      0.45       240\n",
      "         162       0.44      0.26      0.33        27\n",
      "         163       0.29      0.26      0.28        19\n",
      "         164       0.33      0.34      0.34       161\n",
      "         165       0.56      0.30      0.39        66\n",
      "         166       0.40      0.33      0.36        70\n",
      "         167       0.55      0.43      0.48        61\n",
      "         168       0.46      0.17      0.25        92\n",
      "         169       0.39      0.53      0.45        34\n",
      "         170       0.19      0.16      0.18        37\n",
      "         171       0.36      0.39      0.37       223\n",
      "         172       0.20      0.08      0.12        24\n",
      "         173       0.62      0.58      0.60       248\n",
      "         174       0.45      0.46      0.45       191\n",
      "         175       0.20      0.21      0.20        53\n",
      "         176       0.42      0.32      0.36       236\n",
      "         177       0.32      0.28      0.30        39\n",
      "         178       0.37      0.48      0.42       201\n",
      "         179       0.12      0.16      0.14        31\n",
      "         180       0.51      0.57      0.54      1088\n",
      "         181       0.37      0.36      0.37       185\n",
      "         182       0.30      0.25      0.27        28\n",
      "         183       0.41      0.45      0.43       359\n",
      "         184       0.52      0.68      0.59        19\n",
      "         185       0.16      0.14      0.15        35\n",
      "         186       0.47      0.23      0.31        39\n",
      "         187       0.42      0.40      0.41        25\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     18549\n",
      "   macro avg       0.40      0.35      0.36     18549\n",
      "weighted avg       0.46      0.46      0.46     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imrovements\n",
    "\n",
    "Намагаємося покращити результат. Спочатку будемо використовувати логістичну регресію, потім проробимо все те саме але з векторами Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg+sum vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LogregModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels, iters = 3000):\n",
    "        super().__init__(train_vectors, train_labels, test_vectors, test_labels)\n",
    "        self.model = LogisticRegression(random_state=26, n_jobs = 6, solver=\"lbfgs\", \\\n",
    "                                        multi_class=\"multinomial\", max_iter = iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogregModel(train_doc_sum_vecs, train_topic_labels, test_doc_sum_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 407 ms, sys: 308 ms, total: 715 ms\n",
      "Wall time: 27min 32s\n"
     ]
    }
   ],
   "source": [
    "%time logreg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        28\n",
      "           1       0.34      0.20      0.25        87\n",
      "           2       0.47      0.52      0.49        83\n",
      "           3       0.71      0.62      0.67        40\n",
      "           4       0.44      0.37      0.40       185\n",
      "           5       0.43      0.48      0.45        25\n",
      "           6       0.62      0.64      0.63        50\n",
      "           7       0.22      0.30      0.25        44\n",
      "           8       0.29      0.27      0.28        22\n",
      "           9       0.85      0.77      0.81        22\n",
      "          10       0.06      0.08      0.07        25\n",
      "          11       0.64      0.45      0.53        20\n",
      "          12       0.51      0.64      0.57        53\n",
      "          13       0.19      0.31      0.24        16\n",
      "          14       0.18      0.25      0.21        24\n",
      "          15       0.44      0.37      0.40        19\n",
      "          16       0.46      0.49      0.48       137\n",
      "          17       0.18      0.22      0.20        18\n",
      "          18       0.38      0.34      0.36       105\n",
      "          19       0.16      0.26      0.20        19\n",
      "          20       0.24      0.36      0.29        33\n",
      "          21       0.31      0.38      0.34        48\n",
      "          22       0.26      0.24      0.25        45\n",
      "          23       0.42      0.57      0.48        23\n",
      "          24       0.52      0.51      0.51        96\n",
      "          25       0.07      0.06      0.07        47\n",
      "          26       0.57      0.47      0.51        43\n",
      "          27       0.17      0.21      0.19        29\n",
      "          28       0.69      0.48      0.56        23\n",
      "          29       0.65      0.57      0.61        30\n",
      "          30       0.66      0.66      0.66      1089\n",
      "          31       0.53      0.58      0.55       377\n",
      "          32       0.31      0.31      0.31        74\n",
      "          33       0.46      0.43      0.45       104\n",
      "          34       0.42      0.34      0.38        32\n",
      "          35       0.25      0.41      0.31        56\n",
      "          36       0.08      0.10      0.09        40\n",
      "          37       0.27      0.31      0.29        55\n",
      "          38       0.23      0.22      0.22        74\n",
      "          39       0.32      0.41      0.36        27\n",
      "          40       0.37      0.49      0.42        47\n",
      "          41       0.30      0.44      0.35        39\n",
      "          42       0.74      0.61      0.67       249\n",
      "          43       0.48      0.50      0.49        24\n",
      "          44       0.38      0.56      0.45        57\n",
      "          45       0.68      0.59      0.63       201\n",
      "          46       0.20      0.31      0.24        29\n",
      "          47       0.58      0.63      0.61       106\n",
      "          48       0.26      0.33      0.29        52\n",
      "          49       0.35      0.38      0.36        16\n",
      "          50       0.62      0.72      0.67        36\n",
      "          51       0.62      0.70      0.65       145\n",
      "          52       0.07      0.06      0.07        16\n",
      "          53       0.19      0.25      0.22        20\n",
      "          54       0.53      0.37      0.43       391\n",
      "          55       0.37      0.27      0.31       224\n",
      "          56       0.58      0.53      0.56       360\n",
      "          57       0.72      0.72      0.72        85\n",
      "          58       0.25      0.23      0.24        13\n",
      "          59       0.18      0.24      0.20        55\n",
      "          60       0.45      0.56      0.50        70\n",
      "          61       0.14      0.22      0.17        18\n",
      "          62       0.37      0.46      0.41        28\n",
      "          63       0.53      0.59      0.56        56\n",
      "          64       0.40      0.52      0.45        62\n",
      "          65       0.45      0.59      0.51        79\n",
      "          66       0.07      0.12      0.09        25\n",
      "          67       0.39      0.60      0.47        65\n",
      "          68       0.21      0.38      0.27        42\n",
      "          69       0.49      0.44      0.46       143\n",
      "          70       0.48      0.63      0.55       122\n",
      "          71       0.12      0.17      0.14        35\n",
      "          72       0.41      0.37      0.39       161\n",
      "          73       0.40      0.52      0.45        33\n",
      "          74       0.81      0.67      0.73        39\n",
      "          75       0.70      0.63      0.66       658\n",
      "          76       0.26      0.33      0.29        57\n",
      "          77       0.58      0.55      0.57        38\n",
      "          78       0.29      0.29      0.29        14\n",
      "          79       0.38      0.40      0.39        95\n",
      "          80       0.11      0.20      0.15        20\n",
      "          81       0.12      0.09      0.10        78\n",
      "          82       0.26      0.30      0.28        20\n",
      "          83       0.21      0.33      0.26        39\n",
      "          84       0.25      0.28      0.27        53\n",
      "          85       0.58      0.60      0.59        25\n",
      "          86       0.44      0.56      0.49        61\n",
      "          87       0.16      0.23      0.19        44\n",
      "          88       0.60      0.49      0.54       328\n",
      "          89       0.44      0.37      0.40        19\n",
      "          90       0.27      0.29      0.28        34\n",
      "          91       0.20      0.19      0.20        83\n",
      "          92       0.34      0.39      0.36        64\n",
      "          93       0.28      0.44      0.34        25\n",
      "          94       0.09      0.13      0.11        30\n",
      "          95       0.57      0.53      0.55       129\n",
      "          96       0.30      0.21      0.25       142\n",
      "          97       0.79      0.62      0.70        37\n",
      "          98       0.65      0.54      0.59        24\n",
      "          99       0.32      0.39      0.35        70\n",
      "         100       0.27      0.20      0.23       298\n",
      "         101       0.35      0.47      0.40        17\n",
      "         102       0.89      0.85      0.87        20\n",
      "         103       0.62      0.58      0.60       191\n",
      "         104       0.44      0.22      0.30        18\n",
      "         105       0.23      0.35      0.28        23\n",
      "         106       0.20      0.26      0.23        34\n",
      "         107       0.44      0.37      0.40       209\n",
      "         108       0.43      0.38      0.40        16\n",
      "         109       0.41      0.35      0.38        26\n",
      "         110       0.59      0.43      0.50       297\n",
      "         111       0.30      0.36      0.33        25\n",
      "         112       0.50      0.40      0.44        20\n",
      "         113       0.18      0.23      0.20        53\n",
      "         114       0.18      0.22      0.20        41\n",
      "         115       0.67      0.32      0.43        25\n",
      "         116       0.14      0.24      0.17        25\n",
      "         117       0.29      0.32      0.30        19\n",
      "         118       0.56      0.67      0.61        75\n",
      "         119       0.26      0.17      0.20       137\n",
      "         120       0.38      0.34      0.35       155\n",
      "         121       0.39      0.63      0.48        54\n",
      "         122       0.52      0.52      0.52        61\n",
      "         123       0.82      0.71      0.76        38\n",
      "         124       0.65      0.57      0.61        61\n",
      "         125       0.29      0.40      0.34        25\n",
      "         126       0.15      0.31      0.20        16\n",
      "         127       0.53      0.48      0.50       194\n",
      "         128       0.08      0.15      0.10        26\n",
      "         129       0.59      0.63      0.61       144\n",
      "         130       0.62      0.56      0.59       334\n",
      "         131       0.57      0.64      0.60       116\n",
      "         132       0.29      0.38      0.33        53\n",
      "         133       0.36      0.38      0.37        79\n",
      "         134       0.64      0.62      0.63       365\n",
      "         135       0.18      0.34      0.24        35\n",
      "         136       0.52      0.67      0.58        95\n",
      "         137       0.32      0.44      0.37        41\n",
      "         138       0.16      0.26      0.20        34\n",
      "         139       0.27      0.40      0.32        20\n",
      "         140       0.67      0.29      0.40        21\n",
      "         141       0.56      0.74      0.64        19\n",
      "         142       0.25      0.41      0.31        34\n",
      "         143       0.25      0.35      0.29        37\n",
      "         144       0.71      0.74      0.72       945\n",
      "         145       0.69      0.67      0.68        30\n",
      "         146       0.34      0.44      0.38        45\n",
      "         147       0.62      0.33      0.43        15\n",
      "         148       0.42      0.52      0.46        61\n",
      "         149       0.31      0.30      0.31        92\n",
      "         150       0.82      0.76      0.78        41\n",
      "         151       0.14      0.19      0.16        21\n",
      "         152       0.52      0.48      0.50       240\n",
      "         153       0.57      0.57      0.57        47\n",
      "         154       0.31      0.31      0.31        16\n",
      "         155       0.22      0.22      0.22        23\n",
      "         156       0.58      0.44      0.50       236\n",
      "         157       0.54      0.62      0.58        34\n",
      "         158       0.78      0.78      0.78      1969\n",
      "         159       0.54      0.50      0.52       108\n",
      "         160       0.37      0.35      0.36        63\n",
      "         161       0.38      0.35      0.36        66\n",
      "         162       0.82      0.50      0.62        28\n",
      "         163       0.05      0.04      0.04        84\n",
      "         164       0.21      0.28      0.24        39\n",
      "         165       0.17      0.21      0.19        42\n",
      "         166       0.29      0.29      0.29        17\n",
      "         167       0.59      0.47      0.52       190\n",
      "         168       0.18      0.18      0.18        17\n",
      "         169       0.23      0.38      0.29        39\n",
      "         170       0.60      0.27      0.37        22\n",
      "         171       0.37      0.47      0.41        45\n",
      "         172       0.79      0.52      0.63        21\n",
      "         173       0.27      0.36      0.31        50\n",
      "         174       0.24      0.32      0.27        73\n",
      "         175       0.60      0.62      0.61       130\n",
      "         176       0.19      0.29      0.23        24\n",
      "         177       0.22      0.35      0.27        31\n",
      "         178       0.82      0.71      0.76        51\n",
      "         179       0.17      0.21      0.19        56\n",
      "         180       0.38      0.45      0.41        20\n",
      "         181       0.52      0.44      0.47        39\n",
      "         182       0.60      0.56      0.58       600\n",
      "         183       0.62      0.46      0.53       387\n",
      "         184       0.36      0.33      0.34        79\n",
      "         185       0.37      0.64      0.47        22\n",
      "         186       0.22      0.34      0.27        44\n",
      "         187       0.62      0.48      0.54        27\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     18552\n",
      "   macro avg       0.41      0.42      0.41     18552\n",
      "weighted avg       0.53      0.52      0.52     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Є невелике покращення в якості."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN+Doc2Vec\n",
    "\n",
    "Переходимо до Doc2Vec. Для цього використовуємо gensim. Спочатку конвертуємо наші документи в модель gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "def to_tagged_doc(doc):\n",
    "    words = tokenize_doc(doc)\n",
    "    return TaggedDocument(words, [doc.topic_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['звернення', 'призначення', 'субсидії', 'отримала', 'відповідповідь', 'заява', 'призначення', 'субсидії', 'розглянута', 'травня', 'місяця', 'звернення', 'го', 'червня', 'можливо', 'писати', 'розглянуто', 'травні', 'травень', 'закінчився', '??', 'вважаю', 'працівники', 'розглядають', 'звернень', 'дають', 'стандартні', 'відповіді', 'досі', 'маю', 'жодних', 'результатів', 'опрацювання', 'звернення'], tags=[0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tagged_doc(uk_documents[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab7d305953e4703bf8445ee4d1972a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tagged_docs = [to_tagged_doc(doc) for doc in tqdm_notebook(train_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f918e603862a480d9bb6cc2bfe8818ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_tagged_docs = [to_tagged_doc(doc) for doc in tqdm_notebook(test_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потім тренуємо PV-DBOW модель. Розмір вектору документа 300, як і в моделі з embeddins, яку ми використовували в бейзлайні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "dbow_model = Doc2Vec(dm=0, vector_size=300, min_count=5, window=10, workers=6, epochs=120)\n",
    "\n",
    "dbow_model.build_vocab(train_tagged_docs + test_tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 24s, sys: 1min 8s, total: 13min 32s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "%time dbow_model.train(train_tagged_docs, total_examples=dbow_model.corpus_count, epochs=dbow_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збираємо вектори документів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff349ec7a3e477285f8d13c399834b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43287), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_doc_vecs = np.array([dbow_model.infer_vector(doc.words) for doc in tqdm_notebook(train_tagged_docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fe26f420b5447183505d1941e58e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_doc_vecs = np.array([dbow_model.infer_vector(doc.words) for doc in tqdm_notebook(test_tagged_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знову намагаємося застосувати kNN та логістичну регресію на отриманних векторах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KnnModel(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 29s, sys: 3.79 s, total: 12min 32s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%time knn2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.89      0.82        28\n",
      "           1       0.37      0.36      0.36        87\n",
      "           2       0.40      0.65      0.50        83\n",
      "           3       0.69      0.78      0.73        40\n",
      "           4       0.51      0.35      0.42       185\n",
      "           5       0.42      0.56      0.48        25\n",
      "           6       0.53      0.62      0.57        50\n",
      "           7       0.52      0.57      0.54        44\n",
      "           8       0.42      0.23      0.29        22\n",
      "           9       0.83      0.86      0.84        22\n",
      "          10       0.33      0.16      0.22        25\n",
      "          11       0.85      0.55      0.67        20\n",
      "          12       0.51      0.55      0.53        53\n",
      "          13       0.27      0.38      0.32        16\n",
      "          14       0.35      0.25      0.29        24\n",
      "          15       0.70      0.37      0.48        19\n",
      "          16       0.49      0.59      0.54       137\n",
      "          17       0.33      0.50      0.40        18\n",
      "          18       0.34      0.33      0.34       105\n",
      "          19       0.44      0.58      0.50        19\n",
      "          20       0.55      0.33      0.42        33\n",
      "          21       0.58      0.44      0.50        48\n",
      "          22       0.70      0.58      0.63        45\n",
      "          23       0.57      0.70      0.63        23\n",
      "          24       0.57      0.64      0.60        96\n",
      "          25       0.32      0.26      0.29        47\n",
      "          26       0.49      0.79      0.60        43\n",
      "          27       0.42      0.52      0.46        29\n",
      "          28       0.59      0.43      0.50        23\n",
      "          29       0.37      0.83      0.51        30\n",
      "          30       0.65      0.69      0.67      1089\n",
      "          31       0.57      0.67      0.61       377\n",
      "          32       0.51      0.34      0.41        74\n",
      "          33       0.62      0.46      0.53       104\n",
      "          34       0.83      0.47      0.60        32\n",
      "          35       0.58      0.66      0.62        56\n",
      "          36       0.54      0.17      0.26        40\n",
      "          37       0.48      0.42      0.45        55\n",
      "          38       0.40      0.36      0.38        74\n",
      "          39       0.50      0.37      0.43        27\n",
      "          40       0.37      0.55      0.44        47\n",
      "          41       0.63      0.56      0.59        39\n",
      "          42       0.66      0.68      0.67       249\n",
      "          43       0.45      0.62      0.53        24\n",
      "          44       0.58      0.54      0.56        57\n",
      "          45       0.72      0.72      0.72       201\n",
      "          46       0.60      0.21      0.31        29\n",
      "          47       0.49      0.69      0.57       106\n",
      "          48       0.49      0.50      0.50        52\n",
      "          49       0.41      0.56      0.47        16\n",
      "          50       0.87      0.75      0.81        36\n",
      "          51       0.83      0.87      0.85       145\n",
      "          52       0.18      0.12      0.15        16\n",
      "          53       0.38      0.30      0.33        20\n",
      "          54       0.47      0.46      0.47       391\n",
      "          55       0.42      0.47      0.45       224\n",
      "          56       0.63      0.53      0.58       360\n",
      "          57       0.79      0.87      0.83        85\n",
      "          58       0.19      0.46      0.27        13\n",
      "          59       0.57      0.38      0.46        55\n",
      "          60       0.57      0.34      0.43        70\n",
      "          61       0.25      0.22      0.24        18\n",
      "          62       0.60      0.64      0.62        28\n",
      "          63       0.75      0.79      0.77        56\n",
      "          64       0.59      0.47      0.52        62\n",
      "          65       0.48      0.52      0.50        79\n",
      "          66       0.11      0.20      0.14        25\n",
      "          67       0.56      0.54      0.55        65\n",
      "          68       0.25      0.36      0.30        42\n",
      "          69       0.46      0.53      0.49       143\n",
      "          70       0.52      0.68      0.59       122\n",
      "          71       0.33      0.29      0.31        35\n",
      "          72       0.55      0.47      0.51       161\n",
      "          73       0.49      0.55      0.51        33\n",
      "          74       0.61      0.56      0.59        39\n",
      "          75       0.75      0.70      0.73       658\n",
      "          76       0.44      0.35      0.39        57\n",
      "          77       0.64      0.66      0.65        38\n",
      "          78       0.50      0.50      0.50        14\n",
      "          79       0.44      0.41      0.42        95\n",
      "          80       0.11      0.25      0.15        20\n",
      "          81       0.41      0.23      0.30        78\n",
      "          82       0.58      0.55      0.56        20\n",
      "          83       0.32      0.31      0.32        39\n",
      "          84       0.51      0.42      0.46        53\n",
      "          85       0.50      0.64      0.56        25\n",
      "          86       0.60      0.75      0.67        61\n",
      "          87       0.31      0.25      0.28        44\n",
      "          88       0.69      0.53      0.60       328\n",
      "          89       0.58      0.37      0.45        19\n",
      "          90       0.33      0.24      0.28        34\n",
      "          91       0.48      0.42      0.45        83\n",
      "          92       0.52      0.48      0.50        64\n",
      "          93       0.52      0.56      0.54        25\n",
      "          94       0.24      0.20      0.22        30\n",
      "          95       0.62      0.62      0.62       129\n",
      "          96       0.42      0.29      0.34       142\n",
      "          97       0.61      0.84      0.70        37\n",
      "          98       0.50      0.54      0.52        24\n",
      "          99       0.45      0.50      0.47        70\n",
      "         100       0.57      0.34      0.42       298\n",
      "         101       0.55      0.35      0.43        17\n",
      "         102       0.61      0.95      0.75        20\n",
      "         103       0.64      0.55      0.59       191\n",
      "         104       0.54      0.39      0.45        18\n",
      "         105       0.52      0.48      0.50        23\n",
      "         106       0.42      0.65      0.51        34\n",
      "         107       0.49      0.31      0.38       209\n",
      "         108       0.24      0.25      0.24        16\n",
      "         109       0.45      0.50      0.47        26\n",
      "         110       0.56      0.37      0.45       297\n",
      "         111       0.35      0.36      0.35        25\n",
      "         112       0.57      0.65      0.60        20\n",
      "         113       0.44      0.23      0.30        53\n",
      "         114       0.23      0.32      0.27        41\n",
      "         115       0.50      0.32      0.39        25\n",
      "         116       0.43      0.60      0.50        25\n",
      "         117       0.38      0.47      0.42        19\n",
      "         118       0.86      0.96      0.91        75\n",
      "         119       0.40      0.23      0.29       137\n",
      "         120       0.52      0.48      0.50       155\n",
      "         121       0.46      0.61      0.53        54\n",
      "         122       0.57      0.62      0.59        61\n",
      "         123       0.70      0.87      0.78        38\n",
      "         124       0.72      0.84      0.77        61\n",
      "         125       0.43      0.24      0.31        25\n",
      "         126       0.20      0.12      0.15        16\n",
      "         127       0.72      0.51      0.60       194\n",
      "         128       0.21      0.15      0.18        26\n",
      "         129       0.68      0.71      0.69       144\n",
      "         130       0.53      0.64      0.58       334\n",
      "         131       0.52      0.75      0.61       116\n",
      "         132       0.36      0.32      0.34        53\n",
      "         133       0.60      0.52      0.56        79\n",
      "         134       0.65      0.77      0.70       365\n",
      "         135       0.75      0.34      0.47        35\n",
      "         136       0.74      0.82      0.78        95\n",
      "         137       0.50      0.63      0.56        41\n",
      "         138       0.42      0.29      0.34        34\n",
      "         139       0.69      0.45      0.55        20\n",
      "         140       0.74      0.67      0.70        21\n",
      "         141       0.50      0.84      0.63        19\n",
      "         142       0.30      0.32      0.31        34\n",
      "         143       0.56      0.62      0.59        37\n",
      "         144       0.65      0.81      0.72       945\n",
      "         145       0.68      0.90      0.77        30\n",
      "         146       0.62      0.71      0.66        45\n",
      "         147       0.60      0.40      0.48        15\n",
      "         148       0.58      0.34      0.43        61\n",
      "         149       0.23      0.21      0.22        92\n",
      "         150       0.74      0.85      0.80        41\n",
      "         151       0.18      0.24      0.20        21\n",
      "         152       0.58      0.57      0.58       240\n",
      "         153       0.54      0.55      0.55        47\n",
      "         154       0.56      0.31      0.40        16\n",
      "         155       0.27      0.26      0.27        23\n",
      "         156       0.60      0.61      0.60       236\n",
      "         157       0.51      0.76      0.61        34\n",
      "         158       0.78      0.84      0.81      1969\n",
      "         159       0.64      0.60      0.62       108\n",
      "         160       0.59      0.37      0.45        63\n",
      "         161       0.54      0.30      0.39        66\n",
      "         162       0.87      0.71      0.78        28\n",
      "         163       0.35      0.15      0.21        84\n",
      "         164       0.47      0.38      0.42        39\n",
      "         165       0.15      0.21      0.18        42\n",
      "         166       0.35      0.41      0.38        17\n",
      "         167       0.55      0.52      0.53       190\n",
      "         168       0.42      0.29      0.34        17\n",
      "         169       0.67      0.26      0.37        39\n",
      "         170       0.60      0.55      0.57        22\n",
      "         171       0.45      0.49      0.47        45\n",
      "         172       0.50      0.52      0.51        21\n",
      "         173       0.44      0.32      0.37        50\n",
      "         174       0.47      0.27      0.34        73\n",
      "         175       0.71      0.65      0.68       130\n",
      "         176       0.57      0.33      0.42        24\n",
      "         177       0.31      0.35      0.33        31\n",
      "         178       0.62      0.88      0.73        51\n",
      "         179       0.27      0.27      0.27        56\n",
      "         180       0.89      0.40      0.55        20\n",
      "         181       0.43      0.49      0.46        39\n",
      "         182       0.55      0.68      0.61       600\n",
      "         183       0.65      0.49      0.55       387\n",
      "         184       0.30      0.32      0.31        79\n",
      "         185       0.44      0.82      0.57        22\n",
      "         186       0.44      0.27      0.34        44\n",
      "         187       0.47      0.33      0.39        27\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     18552\n",
      "   macro avg       0.51      0.50      0.49     18552\n",
      "weighted avg       0.59      0.59      0.58     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn2.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logreg+Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogregModel(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 423 ms, total: 827 ms\n",
      "Wall time: 5min 5s\n"
     ]
    }
   ],
   "source": [
    "%time logreg2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64        28\n",
      "           1       0.32      0.33      0.33        87\n",
      "           2       0.55      0.57      0.56        83\n",
      "           3       0.83      0.62      0.71        40\n",
      "           4       0.44      0.39      0.41       185\n",
      "           5       0.64      0.56      0.60        25\n",
      "           6       0.55      0.58      0.56        50\n",
      "           7       0.48      0.45      0.47        44\n",
      "           8       0.56      0.23      0.32        22\n",
      "           9       0.94      0.77      0.85        22\n",
      "          10       0.44      0.16      0.24        25\n",
      "          11       0.65      0.55      0.59        20\n",
      "          12       0.54      0.51      0.52        53\n",
      "          13       0.50      0.19      0.27        16\n",
      "          14       0.64      0.29      0.40        24\n",
      "          15       0.80      0.42      0.55        19\n",
      "          16       0.54      0.55      0.54       137\n",
      "          17       0.50      0.50      0.50        18\n",
      "          18       0.36      0.42      0.39       105\n",
      "          19       0.57      0.42      0.48        19\n",
      "          20       0.43      0.30      0.36        33\n",
      "          21       0.65      0.54      0.59        48\n",
      "          22       0.56      0.40      0.47        45\n",
      "          23       0.71      0.43      0.54        23\n",
      "          24       0.64      0.61      0.63        96\n",
      "          25       0.32      0.23      0.27        47\n",
      "          26       0.70      0.72      0.71        43\n",
      "          27       0.49      0.59      0.53        29\n",
      "          28       0.75      0.39      0.51        23\n",
      "          29       0.62      0.70      0.66        30\n",
      "          30       0.63      0.74      0.68      1089\n",
      "          31       0.58      0.62      0.59       377\n",
      "          32       0.50      0.45      0.47        74\n",
      "          33       0.59      0.49      0.54       104\n",
      "          34       0.82      0.44      0.57        32\n",
      "          35       0.65      0.57      0.61        56\n",
      "          36       0.35      0.17      0.23        40\n",
      "          37       0.50      0.36      0.42        55\n",
      "          38       0.50      0.49      0.49        74\n",
      "          39       0.62      0.37      0.47        27\n",
      "          40       0.50      0.60      0.54        47\n",
      "          41       0.59      0.56      0.58        39\n",
      "          42       0.71      0.64      0.67       249\n",
      "          43       0.41      0.62      0.49        24\n",
      "          44       0.62      0.46      0.53        57\n",
      "          45       0.70      0.73      0.71       201\n",
      "          46       0.83      0.17      0.29        29\n",
      "          47       0.70      0.61      0.65       106\n",
      "          48       0.40      0.40      0.40        52\n",
      "          49       0.53      0.56      0.55        16\n",
      "          50       0.83      0.53      0.64        36\n",
      "          51       0.83      0.88      0.85       145\n",
      "          52       0.29      0.12      0.17        16\n",
      "          53       0.38      0.25      0.30        20\n",
      "          54       0.47      0.49      0.48       391\n",
      "          55       0.38      0.42      0.40       224\n",
      "          56       0.54      0.61      0.57       360\n",
      "          57       0.88      0.84      0.86        85\n",
      "          58       0.50      0.15      0.24        13\n",
      "          59       0.44      0.38      0.41        55\n",
      "          60       0.52      0.44      0.48        70\n",
      "          61       0.14      0.11      0.12        18\n",
      "          62       0.60      0.54      0.57        28\n",
      "          63       0.95      0.70      0.80        56\n",
      "          64       0.78      0.45      0.57        62\n",
      "          65       0.60      0.61      0.60        79\n",
      "          66       0.17      0.16      0.17        25\n",
      "          67       0.64      0.57      0.60        65\n",
      "          68       0.28      0.31      0.30        42\n",
      "          69       0.47      0.52      0.49       143\n",
      "          70       0.57      0.70      0.62       122\n",
      "          71       0.35      0.20      0.25        35\n",
      "          72       0.49      0.50      0.49       161\n",
      "          73       0.57      0.52      0.54        33\n",
      "          74       0.73      0.69      0.71        39\n",
      "          75       0.67      0.78      0.72       658\n",
      "          76       0.40      0.44      0.42        57\n",
      "          77       0.82      0.61      0.70        38\n",
      "          78       0.71      0.36      0.48        14\n",
      "          79       0.51      0.53      0.52        95\n",
      "          80       0.12      0.15      0.14        20\n",
      "          81       0.32      0.17      0.22        78\n",
      "          82       0.65      0.55      0.59        20\n",
      "          83       0.38      0.28      0.32        39\n",
      "          84       0.44      0.21      0.28        53\n",
      "          85       0.67      0.40      0.50        25\n",
      "          86       0.65      0.77      0.71        61\n",
      "          87       0.30      0.25      0.27        44\n",
      "          88       0.52      0.66      0.58       328\n",
      "          89       0.67      0.32      0.43        19\n",
      "          90       0.54      0.21      0.30        34\n",
      "          91       0.43      0.43      0.43        83\n",
      "          92       0.50      0.52      0.51        64\n",
      "          93       0.60      0.48      0.53        25\n",
      "          94       0.26      0.17      0.20        30\n",
      "          95       0.68      0.61      0.64       129\n",
      "          96       0.35      0.37      0.36       142\n",
      "          97       0.60      0.81      0.69        37\n",
      "          98       0.50      0.46      0.48        24\n",
      "          99       0.48      0.39      0.43        70\n",
      "         100       0.29      0.44      0.35       298\n",
      "         101       0.67      0.24      0.35        17\n",
      "         102       0.89      0.80      0.84        20\n",
      "         103       0.66      0.66      0.66       191\n",
      "         104       1.00      0.44      0.62        18\n",
      "         105       0.56      0.39      0.46        23\n",
      "         106       0.51      0.62      0.56        34\n",
      "         107       0.47      0.52      0.49       209\n",
      "         108       0.17      0.06      0.09        16\n",
      "         109       0.56      0.38      0.45        26\n",
      "         110       0.51      0.51      0.51       297\n",
      "         111       0.47      0.28      0.35        25\n",
      "         112       0.71      0.60      0.65        20\n",
      "         113       0.37      0.21      0.27        53\n",
      "         114       0.20      0.22      0.21        41\n",
      "         115       0.73      0.32      0.44        25\n",
      "         116       0.50      0.40      0.44        25\n",
      "         117       0.53      0.53      0.53        19\n",
      "         118       0.90      0.95      0.92        75\n",
      "         119       0.31      0.26      0.28       137\n",
      "         120       0.49      0.55      0.52       155\n",
      "         121       0.49      0.70      0.58        54\n",
      "         122       0.55      0.61      0.58        61\n",
      "         123       0.94      0.82      0.87        38\n",
      "         124       0.76      0.74      0.75        61\n",
      "         125       0.58      0.28      0.38        25\n",
      "         126       0.50      0.12      0.20        16\n",
      "         127       0.67      0.56      0.61       194\n",
      "         128       0.44      0.15      0.23        26\n",
      "         129       0.68      0.67      0.68       144\n",
      "         130       0.55      0.58      0.56       334\n",
      "         131       0.60      0.66      0.63       116\n",
      "         132       0.44      0.38      0.41        53\n",
      "         133       0.59      0.63      0.61        79\n",
      "         134       0.72      0.73      0.72       365\n",
      "         135       0.69      0.31      0.43        35\n",
      "         136       0.76      0.84      0.80        95\n",
      "         137       0.55      0.56      0.55        41\n",
      "         138       0.88      0.21      0.33        34\n",
      "         139       0.83      0.25      0.38        20\n",
      "         140       0.68      0.62      0.65        21\n",
      "         141       0.71      0.53      0.61        19\n",
      "         142       0.33      0.38      0.35        34\n",
      "         143       0.78      0.68      0.72        37\n",
      "         144       0.69      0.71      0.70       945\n",
      "         145       0.74      0.87      0.80        30\n",
      "         146       0.74      0.56      0.63        45\n",
      "         147       0.62      0.33      0.43        15\n",
      "         148       0.65      0.46      0.54        61\n",
      "         149       0.24      0.35      0.28        92\n",
      "         150       0.93      0.68      0.79        41\n",
      "         151       0.46      0.29      0.35        21\n",
      "         152       0.59      0.56      0.57       240\n",
      "         153       0.55      0.57      0.56        47\n",
      "         154       0.83      0.31      0.45        16\n",
      "         155       0.29      0.17      0.22        23\n",
      "         156       0.63      0.53      0.58       236\n",
      "         157       0.62      0.82      0.71        34\n",
      "         158       0.77      0.86      0.81      1969\n",
      "         159       0.58      0.64      0.61       108\n",
      "         160       0.51      0.40      0.45        63\n",
      "         161       0.59      0.33      0.43        66\n",
      "         162       0.93      0.46      0.62        28\n",
      "         163       0.33      0.21      0.26        84\n",
      "         164       0.43      0.41      0.42        39\n",
      "         165       0.14      0.24      0.18        42\n",
      "         166       0.42      0.29      0.34        17\n",
      "         167       0.50      0.50      0.50       190\n",
      "         168       0.50      0.24      0.32        17\n",
      "         169       0.37      0.26      0.30        39\n",
      "         170       0.57      0.18      0.28        22\n",
      "         171       0.57      0.56      0.56        45\n",
      "         172       0.78      0.33      0.47        21\n",
      "         173       0.39      0.30      0.34        50\n",
      "         174       0.47      0.29      0.36        73\n",
      "         175       0.75      0.61      0.67       130\n",
      "         176       0.35      0.25      0.29        24\n",
      "         177       0.32      0.32      0.32        31\n",
      "         178       0.93      0.80      0.86        51\n",
      "         179       0.49      0.41      0.45        56\n",
      "         180       0.82      0.45      0.58        20\n",
      "         181       0.46      0.31      0.37        39\n",
      "         182       0.63      0.66      0.64       600\n",
      "         183       0.55      0.58      0.56       387\n",
      "         184       0.44      0.46      0.45        79\n",
      "         185       0.41      0.68      0.51        22\n",
      "         186       0.50      0.27      0.35        44\n",
      "         187       0.86      0.22      0.35        27\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     18552\n",
      "   macro avg       0.57      0.47      0.50     18552\n",
      "weighted avg       0.59      0.59      0.59     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg2.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо що Doc2Vec дав значне покращення у якості."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Approach\n",
    "\n",
    "Спробуємо використати нейронні мережі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN+Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' #CNN doesn't work on my PC on GPU due to libraries incompatibility, comment to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "class NnModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels):\n",
    "        super().__init__(train_vectors, \n",
    "                         to_categorical(train_labels), \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "    \n",
    "    def train(self, epochs = 10):\n",
    "        self.history = self.model.fit(self.train_vectors, self.train_labels,\n",
    "                                      epochs = epochs, batch_size=128, \n",
    "                                      verbose=1, validation_split=0.1)\n",
    "        self.topics_predicted = np.argmax(self.model.predict(self.test_vectors), axis=-1)\n",
    "    \n",
    "\n",
    "class FeedForwardNN(NnModel):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels,\n",
    "                 input_size, hidden_size):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)               \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, activation='relu', input_shape=(input_size,)))        \n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 500,924\n",
      "Trainable params: 500,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_model = FeedForwardNN(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38958 samples, validate on 4329 samples\n",
      "Epoch 1/20\n",
      "38958/38958 [==============================] - 3s 69us/step - loss: 3.3076 - acc: 0.3761 - val_loss: 2.1648 - val_acc: 0.6008\n",
      "Epoch 2/20\n",
      "38958/38958 [==============================] - 2s 51us/step - loss: 1.4691 - acc: 0.7314 - val_loss: 1.1153 - val_acc: 0.7886\n",
      "Epoch 3/20\n",
      "38958/38958 [==============================] - 2s 51us/step - loss: 0.8150 - acc: 0.8425 - val_loss: 0.7158 - val_acc: 0.8535\n",
      "Epoch 4/20\n",
      "38958/38958 [==============================] - 2s 51us/step - loss: 0.5584 - acc: 0.8831 - val_loss: 0.5424 - val_acc: 0.8783\n",
      "Epoch 5/20\n",
      "38958/38958 [==============================] - 2s 52us/step - loss: 0.4343 - acc: 0.9031 - val_loss: 0.4552 - val_acc: 0.8910\n",
      "Epoch 6/20\n",
      "38958/38958 [==============================] - 2s 51us/step - loss: 0.3625 - acc: 0.9145 - val_loss: 0.3997 - val_acc: 0.9021\n",
      "Epoch 7/20\n",
      "38958/38958 [==============================] - 2s 52us/step - loss: 0.3153 - acc: 0.9237 - val_loss: 0.3683 - val_acc: 0.9060\n",
      "Epoch 8/20\n",
      "38958/38958 [==============================] - 2s 52us/step - loss: 0.2820 - acc: 0.9295 - val_loss: 0.3390 - val_acc: 0.9088\n",
      "Epoch 9/20\n",
      "38958/38958 [==============================] - 2s 55us/step - loss: 0.2566 - acc: 0.9346 - val_loss: 0.3270 - val_acc: 0.9099\n",
      "Epoch 10/20\n",
      "38958/38958 [==============================] - 2s 52us/step - loss: 0.2370 - acc: 0.9387 - val_loss: 0.3142 - val_acc: 0.9106\n",
      "Epoch 11/20\n",
      "38958/38958 [==============================] - 2s 55us/step - loss: 0.2210 - acc: 0.9409 - val_loss: 0.3045 - val_acc: 0.9148\n",
      "Epoch 12/20\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.2077 - acc: 0.9444 - val_loss: 0.2935 - val_acc: 0.9171\n",
      "Epoch 13/20\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.1954 - acc: 0.9479 - val_loss: 0.2858 - val_acc: 0.9173\n",
      "Epoch 14/20\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.1864 - acc: 0.9498 - val_loss: 0.2834 - val_acc: 0.9168\n",
      "Epoch 15/20\n",
      "38958/38958 [==============================] - 2s 57us/step - loss: 0.1776 - acc: 0.9502 - val_loss: 0.2789 - val_acc: 0.9152\n",
      "Epoch 16/20\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.1692 - acc: 0.9526 - val_loss: 0.2755 - val_acc: 0.9141\n",
      "Epoch 17/20\n",
      "38958/38958 [==============================] - 2s 56us/step - loss: 0.1621 - acc: 0.9549 - val_loss: 0.2760 - val_acc: 0.9157\n",
      "Epoch 18/20\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.1563 - acc: 0.9563 - val_loss: 0.2696 - val_acc: 0.9168\n",
      "Epoch 19/20\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.1501 - acc: 0.9572 - val_loss: 0.2682 - val_acc: 0.9178\n",
      "Epoch 20/20\n",
      "38958/38958 [==============================] - 2s 52us/step - loss: 0.1453 - acc: 0.9588 - val_loss: 0.2730 - val_acc: 0.9141\n"
     ]
    }
   ],
   "source": [
    "ff_model.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.54      0.65        28\n",
      "           1       0.38      0.41      0.40        87\n",
      "           2       0.56      0.53      0.54        83\n",
      "           3       0.80      0.70      0.75        40\n",
      "           4       0.44      0.46      0.45       185\n",
      "           5       0.54      0.52      0.53        25\n",
      "           6       0.63      0.58      0.60        50\n",
      "           7       0.59      0.50      0.54        44\n",
      "           8       0.40      0.18      0.25        22\n",
      "           9       1.00      0.82      0.90        22\n",
      "          10       0.38      0.12      0.18        25\n",
      "          11       0.69      0.45      0.55        20\n",
      "          12       0.57      0.62      0.59        53\n",
      "          13       0.50      0.12      0.20        16\n",
      "          14       0.50      0.29      0.37        24\n",
      "          15       0.86      0.32      0.46        19\n",
      "          16       0.50      0.60      0.55       137\n",
      "          17       0.57      0.44      0.50        18\n",
      "          18       0.44      0.47      0.45       105\n",
      "          19       0.73      0.42      0.53        19\n",
      "          20       0.67      0.24      0.36        33\n",
      "          21       0.61      0.42      0.49        48\n",
      "          22       0.66      0.51      0.57        45\n",
      "          23       0.91      0.43      0.59        23\n",
      "          24       0.64      0.68      0.66        96\n",
      "          25       0.29      0.23      0.26        47\n",
      "          26       0.69      0.81      0.74        43\n",
      "          27       0.59      0.59      0.59        29\n",
      "          28       0.75      0.39      0.51        23\n",
      "          29       0.58      0.63      0.60        30\n",
      "          30       0.57      0.82      0.67      1089\n",
      "          31       0.60      0.66      0.63       377\n",
      "          32       0.56      0.45      0.50        74\n",
      "          33       0.65      0.48      0.55       104\n",
      "          34       0.87      0.41      0.55        32\n",
      "          35       0.71      0.62      0.67        56\n",
      "          36       0.44      0.17      0.25        40\n",
      "          37       0.59      0.49      0.53        55\n",
      "          38       0.51      0.54      0.52        74\n",
      "          39       0.67      0.37      0.48        27\n",
      "          40       0.60      0.60      0.60        47\n",
      "          41       0.57      0.54      0.55        39\n",
      "          42       0.71      0.65      0.68       249\n",
      "          43       0.75      0.75      0.75        24\n",
      "          44       0.75      0.47      0.58        57\n",
      "          45       0.74      0.78      0.76       201\n",
      "          46       0.83      0.17      0.29        29\n",
      "          47       0.59      0.69      0.64       106\n",
      "          48       0.56      0.44      0.49        52\n",
      "          49       0.67      0.38      0.48        16\n",
      "          50       0.91      0.58      0.71        36\n",
      "          51       0.84      0.90      0.87       145\n",
      "          52       0.33      0.12      0.18        16\n",
      "          53       0.80      0.20      0.32        20\n",
      "          54       0.52      0.39      0.45       391\n",
      "          55       0.44      0.45      0.44       224\n",
      "          56       0.59      0.66      0.62       360\n",
      "          57       0.87      0.89      0.88        85\n",
      "          58       0.50      0.15      0.24        13\n",
      "          59       0.64      0.42      0.51        55\n",
      "          60       0.64      0.46      0.53        70\n",
      "          61       0.29      0.11      0.16        18\n",
      "          62       0.73      0.57      0.64        28\n",
      "          63       0.92      0.79      0.85        56\n",
      "          64       0.72      0.45      0.55        62\n",
      "          65       0.65      0.61      0.63        79\n",
      "          66       0.26      0.20      0.23        25\n",
      "          67       0.70      0.66      0.68        65\n",
      "          68       0.48      0.29      0.36        42\n",
      "          69       0.56      0.45      0.50       143\n",
      "          70       0.62      0.66      0.64       122\n",
      "          71       0.41      0.26      0.32        35\n",
      "          72       0.55      0.51      0.53       161\n",
      "          73       0.59      0.48      0.53        33\n",
      "          74       0.76      0.64      0.69        39\n",
      "          75       0.68      0.81      0.74       658\n",
      "          76       0.36      0.51      0.42        57\n",
      "          77       0.65      0.68      0.67        38\n",
      "          78       0.86      0.43      0.57        14\n",
      "          79       0.49      0.47      0.48        95\n",
      "          80       0.12      0.10      0.11        20\n",
      "          81       0.34      0.18      0.24        78\n",
      "          82       0.83      0.50      0.62        20\n",
      "          83       0.39      0.28      0.33        39\n",
      "          84       0.50      0.42      0.45        53\n",
      "          85       0.77      0.40      0.53        25\n",
      "          86       0.64      0.80      0.71        61\n",
      "          87       0.42      0.25      0.31        44\n",
      "          88       0.56      0.70      0.63       328\n",
      "          89       0.88      0.37      0.52        19\n",
      "          90       0.53      0.24      0.33        34\n",
      "          91       0.46      0.48      0.47        83\n",
      "          92       0.55      0.56      0.55        64\n",
      "          93       0.42      0.56      0.48        25\n",
      "          94       0.40      0.13      0.20        30\n",
      "          95       0.70      0.67      0.69       129\n",
      "          96       0.45      0.38      0.41       142\n",
      "          97       0.79      0.81      0.80        37\n",
      "          98       0.61      0.46      0.52        24\n",
      "          99       0.52      0.49      0.50        70\n",
      "         100       0.30      0.45      0.36       298\n",
      "         101       0.75      0.18      0.29        17\n",
      "         102       0.74      0.85      0.79        20\n",
      "         103       0.66      0.77      0.71       191\n",
      "         104       1.00      0.33      0.50        18\n",
      "         105       0.69      0.39      0.50        23\n",
      "         106       0.74      0.59      0.66        34\n",
      "         107       0.46      0.58      0.51       209\n",
      "         108       0.30      0.19      0.23        16\n",
      "         109       0.48      0.38      0.43        26\n",
      "         110       0.57      0.53      0.55       297\n",
      "         111       0.53      0.32      0.40        25\n",
      "         112       0.75      0.75      0.75        20\n",
      "         113       0.43      0.19      0.26        53\n",
      "         114       0.39      0.17      0.24        41\n",
      "         115       0.67      0.32      0.43        25\n",
      "         116       0.75      0.36      0.49        25\n",
      "         117       0.77      0.53      0.62        19\n",
      "         118       0.95      0.96      0.95        75\n",
      "         119       0.35      0.31      0.33       137\n",
      "         120       0.48      0.62      0.54       155\n",
      "         121       0.63      0.61      0.62        54\n",
      "         122       0.73      0.59      0.65        61\n",
      "         123       0.94      0.84      0.89        38\n",
      "         124       0.69      0.85      0.76        61\n",
      "         125       0.53      0.36      0.43        25\n",
      "         126       0.50      0.12      0.20        16\n",
      "         127       0.70      0.59      0.64       194\n",
      "         128       0.38      0.12      0.18        26\n",
      "         129       0.83      0.65      0.73       144\n",
      "         130       0.62      0.61      0.61       334\n",
      "         131       0.66      0.65      0.66       116\n",
      "         132       0.46      0.32      0.38        53\n",
      "         133       0.74      0.63      0.68        79\n",
      "         134       0.71      0.73      0.72       365\n",
      "         135       0.67      0.34      0.45        35\n",
      "         136       0.80      0.84      0.82        95\n",
      "         137       0.73      0.54      0.62        41\n",
      "         138       0.89      0.24      0.37        34\n",
      "         139       0.86      0.30      0.44        20\n",
      "         140       0.83      0.71      0.77        21\n",
      "         141       0.61      0.58      0.59        19\n",
      "         142       0.37      0.29      0.33        34\n",
      "         143       0.76      0.70      0.73        37\n",
      "         144       0.70      0.72      0.71       945\n",
      "         145       0.90      0.87      0.88        30\n",
      "         146       0.73      0.67      0.70        45\n",
      "         147       0.67      0.53      0.59        15\n",
      "         148       0.65      0.59      0.62        61\n",
      "         149       0.26      0.35      0.30        92\n",
      "         150       0.79      0.73      0.76        41\n",
      "         151       0.42      0.24      0.30        21\n",
      "         152       0.61      0.58      0.60       240\n",
      "         153       0.57      0.62      0.59        47\n",
      "         154       0.57      0.50      0.53        16\n",
      "         155       0.50      0.13      0.21        23\n",
      "         156       0.70      0.51      0.59       236\n",
      "         157       0.65      0.88      0.75        34\n",
      "         158       0.74      0.89      0.81      1969\n",
      "         159       0.63      0.55      0.59       108\n",
      "         160       0.56      0.32      0.40        63\n",
      "         161       0.67      0.30      0.42        66\n",
      "         162       1.00      0.54      0.70        28\n",
      "         163       0.31      0.19      0.24        84\n",
      "         164       0.44      0.31      0.36        39\n",
      "         165       0.34      0.26      0.30        42\n",
      "         166       0.50      0.18      0.26        17\n",
      "         167       0.58      0.52      0.55       190\n",
      "         168       1.00      0.24      0.38        17\n",
      "         169       0.60      0.23      0.33        39\n",
      "         170       0.83      0.23      0.36        22\n",
      "         171       0.69      0.69      0.69        45\n",
      "         172       0.91      0.48      0.62        21\n",
      "         173       0.47      0.32      0.38        50\n",
      "         174       0.45      0.33      0.38        73\n",
      "         175       0.76      0.65      0.70       130\n",
      "         176       0.50      0.21      0.29        24\n",
      "         177       0.50      0.26      0.34        31\n",
      "         178       0.93      0.82      0.87        51\n",
      "         179       0.48      0.38      0.42        56\n",
      "         180       0.82      0.45      0.58        20\n",
      "         181       0.44      0.51      0.48        39\n",
      "         182       0.59      0.74      0.66       600\n",
      "         183       0.60      0.54      0.57       387\n",
      "         184       0.52      0.28      0.36        79\n",
      "         185       0.68      0.68      0.68        22\n",
      "         186       0.48      0.27      0.35        44\n",
      "         187       0.45      0.33      0.38        27\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     18552\n",
      "   macro avg       0.62      0.48      0.52     18552\n",
      "weighted avg       0.62      0.62      0.61     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поки що це найкращий результат згідно F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN+sum vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 500,924\n",
      "Trainable params: 500,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model = FeedForwardNN(train_doc_sum_vecs, train_topic_labels, \n",
    "                             test_doc_sum_vecs, test_topic_labels, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38958 samples, validate on 4329 samples\n",
      "Epoch 1/25\n",
      "38958/38958 [==============================] - 2s 56us/step - loss: 0.8902 - acc: 0.8825 - val_loss: 2.9439 - val_acc: 0.5560\n",
      "Epoch 2/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.8645 - acc: 0.8902 - val_loss: 2.9818 - val_acc: 0.5581\n",
      "Epoch 3/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.8391 - acc: 0.8980 - val_loss: 3.0162 - val_acc: 0.5537\n",
      "Epoch 4/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.8155 - acc: 0.9044 - val_loss: 3.0405 - val_acc: 0.5539\n",
      "Epoch 5/25\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.7956 - acc: 0.9105 - val_loss: 3.0856 - val_acc: 0.5539\n",
      "Epoch 6/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.7760 - acc: 0.9154 - val_loss: 3.0587 - val_acc: 0.5641\n",
      "Epoch 7/25\n",
      "38958/38958 [==============================] - 2s 58us/step - loss: 0.7595 - acc: 0.9204 - val_loss: 3.0962 - val_acc: 0.5648\n",
      "Epoch 8/25\n",
      "38958/38958 [==============================] - 2s 58us/step - loss: 0.7462 - acc: 0.9235 - val_loss: 3.1360 - val_acc: 0.5613\n",
      "Epoch 9/25\n",
      "38958/38958 [==============================] - 2s 58us/step - loss: 0.7310 - acc: 0.9278 - val_loss: 3.1489 - val_acc: 0.5579\n",
      "Epoch 10/25\n",
      "38958/38958 [==============================] - 2s 55us/step - loss: 0.7185 - acc: 0.9301 - val_loss: 3.1525 - val_acc: 0.5660\n",
      "Epoch 11/25\n",
      "38958/38958 [==============================] - 2s 57us/step - loss: 0.7082 - acc: 0.9329 - val_loss: 3.2191 - val_acc: 0.5606\n",
      "Epoch 12/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6988 - acc: 0.9356 - val_loss: 3.2243 - val_acc: 0.5641\n",
      "Epoch 13/25\n",
      "38958/38958 [==============================] - 2s 57us/step - loss: 0.6904 - acc: 0.9375 - val_loss: 3.2480 - val_acc: 0.5664\n",
      "Epoch 14/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6815 - acc: 0.9392 - val_loss: 3.2994 - val_acc: 0.5618\n",
      "Epoch 15/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6728 - acc: 0.9417 - val_loss: 3.2921 - val_acc: 0.5609\n",
      "Epoch 16/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6672 - acc: 0.9431 - val_loss: 3.3401 - val_acc: 0.5606\n",
      "Epoch 17/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6607 - acc: 0.9447 - val_loss: 3.3292 - val_acc: 0.5666\n",
      "Epoch 18/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6580 - acc: 0.9456 - val_loss: 3.3737 - val_acc: 0.5581\n",
      "Epoch 19/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6530 - acc: 0.9460 - val_loss: 3.4027 - val_acc: 0.5588\n",
      "Epoch 20/25\n",
      "38958/38958 [==============================] - 2s 53us/step - loss: 0.6482 - acc: 0.9481 - val_loss: 3.4190 - val_acc: 0.5669\n",
      "Epoch 21/25\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.6449 - acc: 0.9480 - val_loss: 3.4291 - val_acc: 0.5690\n",
      "Epoch 22/25\n",
      "38958/38958 [==============================] - 2s 56us/step - loss: 0.6390 - acc: 0.9486 - val_loss: 3.4569 - val_acc: 0.5641\n",
      "Epoch 23/25\n",
      "38958/38958 [==============================] - 2s 55us/step - loss: 0.6356 - acc: 0.9493 - val_loss: 3.4931 - val_acc: 0.5683\n",
      "Epoch 24/25\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.6349 - acc: 0.9500 - val_loss: 3.4776 - val_acc: 0.5722\n",
      "Epoch 25/25\n",
      "38958/38958 [==============================] - 2s 54us/step - loss: 0.6333 - acc: 0.9506 - val_loss: 3.5648 - val_acc: 0.5606\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.64      0.72        28\n",
      "           1       0.33      0.38      0.35        87\n",
      "           2       0.55      0.51      0.53        83\n",
      "           3       0.79      0.65      0.71        40\n",
      "           4       0.44      0.44      0.44       185\n",
      "           5       0.54      0.56      0.55        25\n",
      "           6       0.63      0.74      0.68        50\n",
      "           7       0.50      0.34      0.41        44\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.94      0.77      0.85        22\n",
      "          10       0.08      0.04      0.05        25\n",
      "          11       0.56      0.25      0.34        20\n",
      "          12       0.62      0.58      0.60        53\n",
      "          13       0.33      0.31      0.32        16\n",
      "          14       0.18      0.21      0.19        24\n",
      "          15       0.45      0.26      0.33        19\n",
      "          16       0.48      0.53      0.50       137\n",
      "          17       0.80      0.22      0.35        18\n",
      "          18       0.35      0.56      0.43       105\n",
      "          19       0.38      0.26      0.31        19\n",
      "          20       0.00      0.00      0.00        33\n",
      "          21       0.29      0.19      0.23        48\n",
      "          22       0.45      0.38      0.41        45\n",
      "          23       0.75      0.52      0.62        23\n",
      "          24       0.67      0.56      0.61        96\n",
      "          25       0.16      0.17      0.17        47\n",
      "          26       0.42      0.35      0.38        43\n",
      "          27       0.53      0.34      0.42        29\n",
      "          28       0.72      0.57      0.63        23\n",
      "          29       0.67      0.60      0.63        30\n",
      "          30       0.70      0.64      0.67      1089\n",
      "          31       0.51      0.66      0.57       377\n",
      "          32       0.39      0.41      0.40        74\n",
      "          33       0.54      0.54      0.54       104\n",
      "          34       0.58      0.47      0.52        32\n",
      "          35       0.45      0.46      0.46        56\n",
      "          36       0.14      0.17      0.15        40\n",
      "          37       0.44      0.31      0.36        55\n",
      "          38       0.32      0.32      0.32        74\n",
      "          39       0.50      0.41      0.45        27\n",
      "          40       0.53      0.49      0.51        47\n",
      "          41       0.61      0.36      0.45        39\n",
      "          42       0.74      0.71      0.72       249\n",
      "          43       0.62      0.62      0.62        24\n",
      "          44       0.54      0.49      0.51        57\n",
      "          45       0.71      0.70      0.71       201\n",
      "          46       0.00      0.00      0.00        29\n",
      "          47       0.52      0.60      0.56       106\n",
      "          48       0.42      0.40      0.41        52\n",
      "          49       0.75      0.38      0.50        16\n",
      "          50       0.77      0.64      0.70        36\n",
      "          51       0.74      0.71      0.72       145\n",
      "          52       0.67      0.12      0.21        16\n",
      "          53       0.00      0.00      0.00        20\n",
      "          54       0.41      0.66      0.51       391\n",
      "          55       0.37      0.37      0.37       224\n",
      "          56       0.56      0.63      0.59       360\n",
      "          57       0.78      0.71      0.74        85\n",
      "          58       0.00      0.00      0.00        13\n",
      "          59       0.36      0.29      0.32        55\n",
      "          60       0.57      0.51      0.54        70\n",
      "          61       0.60      0.17      0.26        18\n",
      "          62       0.44      0.39      0.42        28\n",
      "          63       0.61      0.55      0.58        56\n",
      "          64       0.52      0.63      0.57        62\n",
      "          65       0.60      0.57      0.58        79\n",
      "          66       0.38      0.12      0.18        25\n",
      "          67       0.55      0.62      0.58        65\n",
      "          68       0.35      0.17      0.23        42\n",
      "          69       0.52      0.57      0.54       143\n",
      "          70       0.68      0.66      0.67       122\n",
      "          71       0.00      0.00      0.00        35\n",
      "          72       0.46      0.43      0.45       161\n",
      "          73       0.68      0.45      0.55        33\n",
      "          74       0.72      0.59      0.65        39\n",
      "          75       0.61      0.75      0.67       658\n",
      "          76       0.25      0.21      0.23        57\n",
      "          77       0.75      0.63      0.69        38\n",
      "          78       0.57      0.29      0.38        14\n",
      "          79       0.39      0.39      0.39        95\n",
      "          80       0.12      0.10      0.11        20\n",
      "          81       0.15      0.14      0.14        78\n",
      "          82       0.36      0.20      0.26        20\n",
      "          83       0.35      0.44      0.39        39\n",
      "          84       0.29      0.42      0.34        53\n",
      "          85       0.00      0.00      0.00        25\n",
      "          86       0.52      0.43      0.47        61\n",
      "          87       0.29      0.23      0.26        44\n",
      "          88       0.53      0.70      0.61       328\n",
      "          89       0.67      0.21      0.32        19\n",
      "          90       0.00      0.00      0.00        34\n",
      "          91       0.38      0.29      0.33        83\n",
      "          92       0.34      0.41      0.37        64\n",
      "          93       0.41      0.44      0.42        25\n",
      "          94       0.32      0.20      0.24        30\n",
      "          95       0.61      0.67      0.64       129\n",
      "          96       0.29      0.38      0.33       142\n",
      "          97       0.68      0.68      0.68        37\n",
      "          98       0.62      0.42      0.50        24\n",
      "          99       0.45      0.54      0.49        70\n",
      "         100       0.29      0.45      0.36       298\n",
      "         101       0.00      0.00      0.00        17\n",
      "         102       0.88      0.75      0.81        20\n",
      "         103       0.67      0.71      0.69       191\n",
      "         104       0.33      0.22      0.27        18\n",
      "         105       0.46      0.26      0.33        23\n",
      "         106       0.50      0.38      0.43        34\n",
      "         107       0.42      0.54      0.47       209\n",
      "         108       0.44      0.25      0.32        16\n",
      "         109       0.48      0.46      0.47        26\n",
      "         110       0.50      0.45      0.47       297\n",
      "         111       0.38      0.24      0.29        25\n",
      "         112       0.00      0.00      0.00        20\n",
      "         113       0.38      0.21      0.27        53\n",
      "         114       0.38      0.24      0.30        41\n",
      "         115       0.53      0.36      0.43        25\n",
      "         116       0.39      0.28      0.33        25\n",
      "         117       0.38      0.26      0.31        19\n",
      "         118       0.77      0.75      0.76        75\n",
      "         119       0.26      0.23      0.24       137\n",
      "         120       0.38      0.45      0.41       155\n",
      "         121       0.49      0.44      0.47        54\n",
      "         122       0.57      0.46      0.51        61\n",
      "         123       0.86      0.82      0.84        38\n",
      "         124       0.87      0.66      0.75        61\n",
      "         125       0.31      0.36      0.33        25\n",
      "         126       0.00      0.00      0.00        16\n",
      "         127       0.61      0.63      0.62       194\n",
      "         128       0.33      0.15      0.21        26\n",
      "         129       0.68      0.69      0.69       144\n",
      "         130       0.62      0.49      0.54       334\n",
      "         131       0.61      0.67      0.64       116\n",
      "         132       0.39      0.32      0.35        53\n",
      "         133       0.42      0.39      0.41        79\n",
      "         134       0.64      0.68      0.66       365\n",
      "         135       0.43      0.26      0.32        35\n",
      "         136       0.71      0.71      0.71        95\n",
      "         137       0.52      0.37      0.43        41\n",
      "         138       0.00      0.00      0.00        34\n",
      "         139       0.00      0.00      0.00        20\n",
      "         140       0.65      0.62      0.63        21\n",
      "         141       0.59      0.53      0.56        19\n",
      "         142       0.44      0.47      0.46        34\n",
      "         143       0.42      0.38      0.40        37\n",
      "         144       0.72      0.79      0.76       945\n",
      "         145       0.95      0.63      0.76        30\n",
      "         146       0.48      0.36      0.41        45\n",
      "         147       0.50      0.40      0.44        15\n",
      "         148       0.50      0.49      0.50        61\n",
      "         149       0.34      0.29      0.32        92\n",
      "         150       0.00      0.00      0.00        41\n",
      "         151       0.36      0.24      0.29        21\n",
      "         152       0.52      0.65      0.58       240\n",
      "         153       0.69      0.66      0.67        47\n",
      "         154       0.00      0.00      0.00        16\n",
      "         155       0.20      0.09      0.12        23\n",
      "         156       0.59      0.53      0.55       236\n",
      "         157       0.81      0.65      0.72        34\n",
      "         158       0.77      0.84      0.81      1969\n",
      "         159       0.65      0.60      0.62       108\n",
      "         160       0.50      0.21      0.29        63\n",
      "         161       0.41      0.30      0.35        66\n",
      "         162       0.71      0.18      0.29        28\n",
      "         163       0.17      0.14      0.15        84\n",
      "         164       0.44      0.21      0.28        39\n",
      "         165       0.21      0.17      0.19        42\n",
      "         166       0.67      0.24      0.35        17\n",
      "         167       0.61      0.56      0.58       190\n",
      "         168       0.00      0.00      0.00        17\n",
      "         169       0.34      0.33      0.34        39\n",
      "         170       0.77      0.45      0.57        22\n",
      "         171       0.58      0.62      0.60        45\n",
      "         172       0.00      0.00      0.00        21\n",
      "         173       0.46      0.36      0.40        50\n",
      "         174       0.30      0.27      0.29        73\n",
      "         175       0.61      0.72      0.66       130\n",
      "         176       0.27      0.12      0.17        24\n",
      "         177       0.38      0.32      0.35        31\n",
      "         178       0.84      0.75      0.79        51\n",
      "         179       0.38      0.29      0.33        56\n",
      "         180       0.41      0.35      0.38        20\n",
      "         181       0.47      0.38      0.42        39\n",
      "         182       0.64      0.64      0.64       600\n",
      "         183       0.54      0.68      0.60       387\n",
      "         184       0.33      0.27      0.30        79\n",
      "         185       0.64      0.64      0.64        22\n",
      "         186       0.50      0.34      0.41        44\n",
      "         187       0.00      0.00      0.00        27\n",
      "\n",
      "   micro avg       0.57      0.57      0.57     18552\n",
      "   macro avg       0.46      0.40      0.42     18552\n",
      "weighted avg       0.56      0.57      0.56     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приблизно як логістична регресія на сумі векторів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тренуванням LSTM мережі, спочатку векторизуємо дані. Кожний документ представимо вектором цілих чисел де кожне слово замінимо його цілочисленною відповідністю. Для цього спочатку готуємо словник таких відповідностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134538"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "vocab = Dictionary([doc.words for doc in (train_tagged_docs + test_tagged_docs)])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відкидаємо слова з дуже великою або дуже малою частотою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48519\n"
     ]
    }
   ],
   "source": [
    "vocab.filter_extremes(no_below = 3, no_above = 0.9, keep_n = 50000)\n",
    "\n",
    "MAX_WORDS_NUM = len(vocab) + 1\n",
    "\n",
    "print(MAX_WORDS_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готуємо вектори документів для тренувальних і тестових данних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43287"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences = [text_to_sequence(doc.words) for doc in train_tagged_docs]\n",
    "\n",
    "len(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 3, 23, 18, 1, 32, 26, 7, 11, 18, 21, 19, 10, 28, 33, 4, 5, 16, 20, 22, 18, 30, 29, 2, 0, 9, 12, 0, 8, 27, 14, 31, 6, 15, 25, 15, 12, 18, 4, 17, 13]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['добрий', 'прошу', 'надати', 'роз’яснення', 'приводу', 'підвищення', 'рази', 'ціни', 'оплату', 'послуг', 'дитячого', 'садочка', 'оболонському', 'києва', 'ціни', 'районах', 'підвищені', 'оболонський', 'район', 'мав', 'нахабність', 'зробити', 'харчування', 'дітей', 'нічим', 'відрізняється', 'садочків', 'районів', 'дітей', 'годують', 'червоною', 'дають', 'свіжі', 'фрукти', 'повинні', 'давати', 'вихователі', 'більшу', 'зарплату', 'підвищення', 'оплати', 'отримувати', 'харчування', 'дітей', 'покращується', 'давали', 'пів', 'дають', 'синього', 'кольору', 'понеділок', 'макарони', 'наступний', 'прошу', 'розібратися', 'даній', 'ситуації']\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_sequences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18552"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = [text_to_sequence(doc.words) for doc in test_tagged_docs]\n",
    "\n",
    "len(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивимось розподілення векторів по розміру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length: 39.9\n",
      "Max Length: 235\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in (train_sequences + test_sequences)]\n",
    "print(\"Average Length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"Max Length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встановолюємо максимального вектору та вирівнюємо відносно цього розміру тренувальні і тестові данні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43287, 235)\n",
      "(18552, 235)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQ_LEN = 235\n",
    "\n",
    "train_padded_seqs = pad_sequences(train_sequences, maxlen = MAX_SEQ_LEN)\n",
    "test_padded_seqs = pad_sequences(test_sequences, maxlen = MAX_SEQ_LEN)\n",
    "\n",
    "print(train_padded_seqs.shape)\n",
    "print(test_padded_seqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будуємо та тренуємо модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Input, SpatialDropout1D, Dropout\n",
    "\n",
    "class LstmModel(NnModel):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels,\n",
    "                 embedding_dim, memory_units):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_WORDS_NUM, embedding_dim, input_length = MAX_SEQ_LEN))\n",
    "        #self.model.add(Dropout(0.2))        \n",
    "        self.model.add(LSTM(memory_units, dropout=0.2, recurrent_dropout=0.2))        \n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-3),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm_model = LstmModel(train_padded_seqs, train_topic_labels, test_padded_seqs, test_topic_labels, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38958 samples, validate on 4329 samples\n",
      "Epoch 1/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 4.1364 - acc: 0.1569 - val_loss: 3.7575 - val_acc: 0.2308\n",
      "Epoch 2/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 3.2838 - acc: 0.2901 - val_loss: 3.0263 - val_acc: 0.3246\n",
      "Epoch 3/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 2.7822 - acc: 0.3552 - val_loss: 2.6261 - val_acc: 0.3862\n",
      "Epoch 4/25\n",
      "38958/38958 [==============================] - 116s 3ms/step - loss: 2.4481 - acc: 0.4063 - val_loss: 2.4308 - val_acc: 0.4146\n",
      "Epoch 5/25\n",
      "38958/38958 [==============================] - 116s 3ms/step - loss: 2.1938 - acc: 0.4546 - val_loss: 2.2432 - val_acc: 0.4472\n",
      "Epoch 6/25\n",
      "38958/38958 [==============================] - 116s 3ms/step - loss: 1.9643 - acc: 0.4991 - val_loss: 2.0819 - val_acc: 0.4856\n",
      "Epoch 7/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 1.7748 - acc: 0.5374 - val_loss: 1.9876 - val_acc: 0.4973\n",
      "Epoch 8/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 1.6016 - acc: 0.5749 - val_loss: 1.8995 - val_acc: 0.5170\n",
      "Epoch 9/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 1.4513 - acc: 0.6118 - val_loss: 1.8073 - val_acc: 0.5456\n",
      "Epoch 10/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 1.3182 - acc: 0.6429 - val_loss: 1.7744 - val_acc: 0.5523\n",
      "Epoch 11/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 1.1905 - acc: 0.6761 - val_loss: 1.7265 - val_acc: 0.5588\n",
      "Epoch 12/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 1.0893 - acc: 0.7022 - val_loss: 1.6888 - val_acc: 0.5666\n",
      "Epoch 13/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.9894 - acc: 0.7289 - val_loss: 1.6684 - val_acc: 0.5694\n",
      "Epoch 14/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.8924 - acc: 0.7559 - val_loss: 1.6764 - val_acc: 0.5750\n",
      "Epoch 15/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.8156 - acc: 0.7750 - val_loss: 1.6552 - val_acc: 0.5879\n",
      "Epoch 16/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.7453 - acc: 0.7982 - val_loss: 1.6315 - val_acc: 0.5932\n",
      "Epoch 17/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.6843 - acc: 0.8147 - val_loss: 1.6799 - val_acc: 0.5937\n",
      "Epoch 18/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.6210 - acc: 0.8324 - val_loss: 1.6661 - val_acc: 0.5971\n",
      "Epoch 19/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.5688 - acc: 0.8460 - val_loss: 1.7032 - val_acc: 0.5976\n",
      "Epoch 20/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.5242 - acc: 0.8615 - val_loss: 1.7226 - val_acc: 0.6057\n",
      "Epoch 21/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.4730 - acc: 0.8751 - val_loss: 1.7174 - val_acc: 0.6048\n",
      "Epoch 22/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.4338 - acc: 0.8856 - val_loss: 1.7315 - val_acc: 0.6031\n",
      "Epoch 23/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.4007 - acc: 0.8928 - val_loss: 1.7407 - val_acc: 0.6061\n",
      "Epoch 24/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.3640 - acc: 0.9038 - val_loss: 1.7704 - val_acc: 0.6055\n",
      "Epoch 25/25\n",
      "38958/38958 [==============================] - 117s 3ms/step - loss: 0.3339 - acc: 0.9128 - val_loss: 1.8175 - val_acc: 0.5992\n"
     ]
    }
   ],
   "source": [
    "lstm_model.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.57      0.68        28\n",
      "           1       0.42      0.33      0.37        87\n",
      "           2       0.73      0.64      0.68        83\n",
      "           3       0.60      0.45      0.51        40\n",
      "           4       0.49      0.46      0.47       185\n",
      "           5       0.42      0.20      0.27        25\n",
      "           6       0.72      0.56      0.63        50\n",
      "           7       0.51      0.52      0.52        44\n",
      "           8       0.37      0.32      0.34        22\n",
      "           9       0.81      0.77      0.79        22\n",
      "          10       0.17      0.12      0.14        25\n",
      "          11       0.44      0.55      0.49        20\n",
      "          12       0.53      0.74      0.61        53\n",
      "          13       0.25      0.25      0.25        16\n",
      "          14       0.28      0.21      0.24        24\n",
      "          15       0.47      0.37      0.41        19\n",
      "          16       0.53      0.57      0.55       137\n",
      "          17       0.27      0.22      0.24        18\n",
      "          18       0.39      0.48      0.43       105\n",
      "          19       0.27      0.32      0.29        19\n",
      "          20       0.24      0.36      0.29        33\n",
      "          21       0.41      0.52      0.46        48\n",
      "          22       0.47      0.36      0.41        45\n",
      "          23       0.52      0.48      0.50        23\n",
      "          24       0.65      0.58      0.62        96\n",
      "          25       0.14      0.11      0.12        47\n",
      "          26       0.76      0.51      0.61        43\n",
      "          27       0.38      0.31      0.34        29\n",
      "          28       0.50      0.39      0.44        23\n",
      "          29       0.68      0.57      0.62        30\n",
      "          30       0.72      0.73      0.73      1089\n",
      "          31       0.59      0.68      0.63       377\n",
      "          32       0.57      0.45      0.50        74\n",
      "          33       0.55      0.60      0.57       104\n",
      "          34       0.41      0.47      0.43        32\n",
      "          35       0.77      0.66      0.71        56\n",
      "          36       0.09      0.07      0.08        40\n",
      "          37       0.46      0.55      0.50        55\n",
      "          38       0.52      0.36      0.43        74\n",
      "          39       0.23      0.22      0.23        27\n",
      "          40       0.67      0.62      0.64        47\n",
      "          41       0.60      0.54      0.57        39\n",
      "          42       0.73      0.80      0.76       249\n",
      "          43       0.36      0.33      0.35        24\n",
      "          44       0.63      0.68      0.66        57\n",
      "          45       0.82      0.84      0.83       201\n",
      "          46       0.11      0.07      0.09        29\n",
      "          47       0.79      0.71      0.75       106\n",
      "          48       0.48      0.40      0.44        52\n",
      "          49       0.89      0.50      0.64        16\n",
      "          50       0.95      0.53      0.68        36\n",
      "          51       0.86      0.81      0.84       145\n",
      "          52       0.29      0.12      0.17        16\n",
      "          53       0.29      0.10      0.15        20\n",
      "          54       0.53      0.56      0.54       391\n",
      "          55       0.46      0.42      0.44       224\n",
      "          56       0.64      0.70      0.67       360\n",
      "          57       0.92      0.81      0.86        85\n",
      "          58       0.11      0.08      0.09        13\n",
      "          59       0.47      0.33      0.39        55\n",
      "          60       0.55      0.60      0.58        70\n",
      "          61       0.29      0.11      0.16        18\n",
      "          62       0.58      0.54      0.56        28\n",
      "          63       0.52      0.64      0.58        56\n",
      "          64       0.48      0.63      0.54        62\n",
      "          65       0.66      0.66      0.66        79\n",
      "          66       0.12      0.08      0.10        25\n",
      "          67       0.59      0.69      0.64        65\n",
      "          68       0.32      0.31      0.31        42\n",
      "          69       0.49      0.61      0.54       143\n",
      "          70       0.66      0.74      0.70       122\n",
      "          71       0.59      0.29      0.38        35\n",
      "          72       0.37      0.25      0.30       161\n",
      "          73       0.45      0.55      0.49        33\n",
      "          74       0.86      0.64      0.74        39\n",
      "          75       0.82      0.82      0.82       658\n",
      "          76       0.29      0.32      0.30        57\n",
      "          77       0.78      0.47      0.59        38\n",
      "          78       0.67      0.43      0.52        14\n",
      "          79       0.45      0.46      0.46        95\n",
      "          80       0.08      0.20      0.11        20\n",
      "          81       0.17      0.14      0.15        78\n",
      "          82       0.22      0.20      0.21        20\n",
      "          83       0.52      0.33      0.41        39\n",
      "          84       0.42      0.43      0.43        53\n",
      "          85       0.42      0.44      0.43        25\n",
      "          86       0.59      0.69      0.64        61\n",
      "          87       0.25      0.20      0.23        44\n",
      "          88       0.62      0.62      0.62       328\n",
      "          89       0.29      0.26      0.28        19\n",
      "          90       0.31      0.24      0.27        34\n",
      "          91       0.33      0.46      0.38        83\n",
      "          92       0.38      0.48      0.42        64\n",
      "          93       0.32      0.24      0.27        25\n",
      "          94       0.26      0.30      0.28        30\n",
      "          95       0.78      0.69      0.73       129\n",
      "          96       0.34      0.36      0.35       142\n",
      "          97       0.73      0.73      0.73        37\n",
      "          98       0.40      0.33      0.36        24\n",
      "          99       0.57      0.46      0.51        70\n",
      "         100       0.28      0.37      0.32       298\n",
      "         101       0.13      0.12      0.12        17\n",
      "         102       0.50      0.60      0.55        20\n",
      "         103       0.60      0.77      0.67       191\n",
      "         104       0.40      0.11      0.17        18\n",
      "         105       0.73      0.35      0.47        23\n",
      "         106       0.61      0.50      0.55        34\n",
      "         107       0.47      0.57      0.51       209\n",
      "         108       0.36      0.25      0.30        16\n",
      "         109       0.38      0.23      0.29        26\n",
      "         110       0.58      0.58      0.58       297\n",
      "         111       0.14      0.08      0.10        25\n",
      "         112       0.65      0.75      0.70        20\n",
      "         113       0.38      0.21      0.27        53\n",
      "         114       0.23      0.12      0.16        41\n",
      "         115       0.62      0.32      0.42        25\n",
      "         116       0.29      0.36      0.32        25\n",
      "         117       0.60      0.16      0.25        19\n",
      "         118       0.87      0.89      0.88        75\n",
      "         119       0.30      0.36      0.33       137\n",
      "         120       0.44      0.60      0.51       155\n",
      "         121       0.58      0.65      0.61        54\n",
      "         122       0.59      0.59      0.59        61\n",
      "         123       0.89      0.82      0.85        38\n",
      "         124       0.87      0.87      0.87        61\n",
      "         125       0.27      0.16      0.20        25\n",
      "         126       0.60      0.19      0.29        16\n",
      "         127       0.58      0.49      0.53       194\n",
      "         128       0.12      0.08      0.10        26\n",
      "         129       0.79      0.81      0.80       144\n",
      "         130       0.52      0.76      0.62       334\n",
      "         131       0.65      0.53      0.58       116\n",
      "         132       0.45      0.32      0.37        53\n",
      "         133       0.80      0.65      0.71        79\n",
      "         134       0.75      0.80      0.78       365\n",
      "         135       0.23      0.31      0.27        35\n",
      "         136       0.83      0.77      0.80        95\n",
      "         137       0.74      0.49      0.59        41\n",
      "         138       0.47      0.24      0.31        34\n",
      "         139       0.31      0.25      0.28        20\n",
      "         140       0.65      0.62      0.63        21\n",
      "         141       0.40      0.21      0.28        19\n",
      "         142       0.46      0.47      0.46        34\n",
      "         143       0.62      0.49      0.55        37\n",
      "         144       0.78      0.77      0.77       945\n",
      "         145       0.81      0.70      0.75        30\n",
      "         146       0.49      0.40      0.44        45\n",
      "         147       0.33      0.07      0.11        15\n",
      "         148       0.38      0.41      0.40        61\n",
      "         149       0.33      0.37      0.35        92\n",
      "         150       0.64      0.51      0.57        41\n",
      "         151       0.29      0.29      0.29        21\n",
      "         152       0.64      0.68      0.66       240\n",
      "         153       0.46      0.36      0.40        47\n",
      "         154       0.00      0.00      0.00        16\n",
      "         155       0.17      0.13      0.15        23\n",
      "         156       0.76      0.67      0.71       236\n",
      "         157       0.83      0.71      0.76        34\n",
      "         158       0.87      0.85      0.86      1969\n",
      "         159       0.72      0.66      0.69       108\n",
      "         160       0.28      0.35      0.31        63\n",
      "         161       0.34      0.26      0.29        66\n",
      "         162       0.92      0.43      0.59        28\n",
      "         163       0.24      0.27      0.26        84\n",
      "         164       0.41      0.28      0.33        39\n",
      "         165       0.16      0.19      0.17        42\n",
      "         166       0.57      0.24      0.33        17\n",
      "         167       0.56      0.63      0.59       190\n",
      "         168       0.60      0.18      0.27        17\n",
      "         169       0.38      0.26      0.31        39\n",
      "         170       0.56      0.23      0.32        22\n",
      "         171       0.58      0.58      0.58        45\n",
      "         172       0.80      0.57      0.67        21\n",
      "         173       0.42      0.34      0.38        50\n",
      "         174       0.34      0.37      0.35        73\n",
      "         175       0.83      0.63      0.72       130\n",
      "         176       0.36      0.17      0.23        24\n",
      "         177       0.39      0.29      0.33        31\n",
      "         178       0.84      0.75      0.79        51\n",
      "         179       0.36      0.34      0.35        56\n",
      "         180       0.83      0.25      0.38        20\n",
      "         181       0.33      0.38      0.35        39\n",
      "         182       0.63      0.69      0.66       600\n",
      "         183       0.58      0.60      0.59       387\n",
      "         184       0.37      0.44      0.40        79\n",
      "         185       0.44      0.82      0.57        22\n",
      "         186       0.13      0.16      0.14        44\n",
      "         187       0.33      0.22      0.27        27\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     18552\n",
      "   macro avg       0.50      0.44      0.46     18552\n",
      "weighted avg       0.61      0.61      0.61     18552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якість приблизно така я у Logreg+Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "\n",
    "Спробуємо застосувати згорточну мережу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "class CnnModel(NnModel):\n",
    "     def __init__(self, train_vectors, train_labels, test_vectors, test_labels, \n",
    "                  embedding_dim, channels_num, conv_window):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_WORDS_NUM, embedding_dim, input_length = MAX_SEQ_LEN))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Conv1D(channels_num, conv_window, activation='relu'))\n",
    "        self.model.add(MaxPooling1D(conv_window))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(1024, activation='relu'))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-3),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 235, 100)          4855300   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 235, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 231, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5888)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              6030336   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 11,142,464\n",
      "Trainable params: 11,142,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CnnModel(train_padded_seqs, train_topic_labels, test_padded_seqs, test_topic_labels, 100, 128, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00d431549841fbb64958654335a3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=25, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 10', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 11', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 12', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 13', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 14', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 15', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 16', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 17', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 18', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 19', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 20', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 21', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 22', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 23', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 24', max=43280, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_model.train(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.49      0.57       335\n",
      "           1       0.48      0.56      0.52        41\n",
      "           2       0.45      0.43      0.44        79\n",
      "           3       0.39      0.44      0.41        39\n",
      "           4       0.35      0.27      0.31        22\n",
      "           5       0.50      0.68      0.58       106\n",
      "           6       0.21      0.38      0.27       142\n",
      "           7       0.39      0.60      0.47        62\n",
      "           8       0.71      0.50      0.59        30\n",
      "           9       0.55      0.55      0.55        22\n",
      "          10       0.81      0.69      0.75       144\n",
      "          11       0.33      0.14      0.20        29\n",
      "          12       0.46      0.55      0.50        22\n",
      "          13       0.26      0.51      0.35        45\n",
      "          14       0.20      0.18      0.19        84\n",
      "          15       0.38      0.56      0.45        16\n",
      "          16       0.66      0.68      0.67       122\n",
      "          17       0.29      0.12      0.17        16\n",
      "          18       0.32      0.49      0.39        53\n",
      "          19       0.31      0.21      0.25        19\n",
      "          20       0.31      0.25      0.28        16\n",
      "          21       0.27      0.33      0.30        24\n",
      "          22       0.63      0.48      0.54       194\n",
      "          23       0.73      0.71      0.72        51\n",
      "          24       0.29      0.36      0.32        47\n",
      "          25       0.20      0.39      0.26        28\n",
      "          26       0.50      0.28      0.36        25\n",
      "          27       0.74      0.67      0.71       943\n",
      "          28       0.55      0.50      0.52        56\n",
      "          29       0.31      0.33      0.32        63\n",
      "          30       0.20      0.32      0.24        44\n",
      "          31       0.03      0.04      0.03        25\n",
      "          32       0.61      0.68      0.64        95\n",
      "          33       0.74      0.77      0.76       129\n",
      "          34       0.35      0.38      0.37        45\n",
      "          35       0.23      0.40      0.29        35\n",
      "          36       0.46      0.54      0.50        50\n",
      "          37       0.19      0.20      0.20        40\n",
      "          38       0.87      0.51      0.65        39\n",
      "          39       0.37      0.39      0.38        44\n",
      "          40       0.60      0.63      0.62       115\n",
      "          41       0.52      0.50      0.51       143\n",
      "          42       0.30      0.68      0.42        62\n",
      "          43       0.29      0.46      0.36        41\n",
      "          44       0.18      0.28      0.22        29\n",
      "          45       0.44      0.52      0.47        71\n",
      "          46       0.41      0.45      0.43        56\n",
      "          47       0.29      0.10      0.14        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.41      0.37      0.39        19\n",
      "          50       0.69      0.41      0.51        44\n",
      "          51       0.09      0.25      0.14        20\n",
      "          52       0.15      0.26      0.19        27\n",
      "          53       0.53      0.47      0.50        53\n",
      "          54       0.26      0.33      0.29        18\n",
      "          55       0.07      0.19      0.10        26\n",
      "          56       0.52      0.50      0.51        28\n",
      "          57       0.57      0.66      0.61       108\n",
      "          58       0.79      0.69      0.74       366\n",
      "          59       0.32      0.32      0.32        74\n",
      "          60       0.19      0.26      0.22        34\n",
      "          61       0.15      0.33      0.21        24\n",
      "          62       0.30      0.41      0.35        95\n",
      "          63       0.23      0.17      0.19        18\n",
      "          64       0.61      0.62      0.61        37\n",
      "          65       0.26      0.47      0.33        15\n",
      "          66       0.31      0.33      0.32        33\n",
      "          67       0.74      0.56      0.64       602\n",
      "          68       0.64      0.61      0.62       190\n",
      "          69       0.29      0.29      0.29       105\n",
      "          70       0.42      0.61      0.49        36\n",
      "          71       0.45      0.35      0.40        48\n",
      "          72       0.15      0.32      0.20        22\n",
      "          73       0.38      0.45      0.42        33\n",
      "          74       0.87      0.84      0.85        85\n",
      "          75       0.64      0.60      0.62        30\n",
      "          76       0.73      0.60      0.66        50\n",
      "          77       0.78      0.49      0.60        43\n",
      "          78       0.41      0.58      0.48       104\n",
      "          79       0.48      0.43      0.46       391\n",
      "          80       0.08      0.06      0.07        17\n",
      "          81       0.64      0.64      0.64        28\n",
      "          82       0.26      0.30      0.28        20\n",
      "          83       0.13      0.17      0.15        24\n",
      "          84       0.24      0.30      0.26        53\n",
      "          85       0.18      0.22      0.20        23\n",
      "          86       0.30      0.34      0.32        74\n",
      "          87       0.18      0.44      0.26        34\n",
      "          88       0.51      0.64      0.57       297\n",
      "          89       0.46      0.47      0.47        40\n",
      "          90       0.08      0.12      0.10        16\n",
      "          91       0.65      0.28      0.39        39\n",
      "          92       0.28      0.27      0.27        26\n",
      "          93       0.95      0.86      0.90        22\n",
      "          94       0.17      0.20      0.18        20\n",
      "          95       0.44      0.26      0.33        42\n",
      "          96       0.59      0.48      0.53        21\n",
      "          97       0.59      0.67      0.63        24\n",
      "          98       0.21      0.36      0.26        25\n",
      "          99       0.45      0.53      0.49        34\n",
      "         100       0.36      0.38      0.37        55\n",
      "         101       0.69      0.45      0.54       329\n",
      "         102       0.87      0.79      0.83        75\n",
      "         103       0.32      0.32      0.32        19\n",
      "         104       0.77      0.61      0.68        38\n",
      "         105       0.19      0.21      0.20        87\n",
      "         106       0.13      0.34      0.19        47\n",
      "         107       0.16      0.19      0.17        32\n",
      "         108       0.43      0.43      0.43        21\n",
      "         109       0.46      0.58      0.51       155\n",
      "         110       0.17      0.27      0.21        66\n",
      "         111       0.42      0.65      0.51        20\n",
      "         112       0.43      0.66      0.52        80\n",
      "         113       0.67      0.54      0.59        56\n",
      "         114       0.54      0.64      0.58       137\n",
      "         115       0.21      0.24      0.22        17\n",
      "         116       0.66      0.66      0.66       129\n",
      "         117       0.14      0.21      0.17        78\n",
      "         118       0.29      0.26      0.27        39\n",
      "         119       0.52      0.57      0.55        47\n",
      "         120       0.38      0.36      0.37        14\n",
      "         121       0.65      0.54      0.59       377\n",
      "         122       0.34      0.55      0.42        38\n",
      "         123       0.33      0.33      0.33        78\n",
      "         124       0.18      0.16      0.17        25\n",
      "         125       0.36      0.32      0.34        73\n",
      "         126       0.52      0.70      0.60        20\n",
      "         127       0.87      0.73      0.80       660\n",
      "         128       0.22      0.41      0.29        82\n",
      "         129       0.27      0.20      0.23        20\n",
      "         130       0.11      0.11      0.11        18\n",
      "         131       0.30      0.26      0.28       137\n",
      "         132       0.31      0.37      0.34        43\n",
      "         133       0.40      0.25      0.31        55\n",
      "         134       0.44      0.59      0.50        64\n",
      "         135       0.12      0.26      0.16        57\n",
      "         136       0.61      0.60      0.61        96\n",
      "         137       0.48      0.75      0.59        61\n",
      "         138       0.88      0.80      0.84      1969\n",
      "         139       0.41      0.56      0.47        54\n",
      "         140       0.33      0.31      0.32        16\n",
      "         141       0.24      0.44      0.31        25\n",
      "         142       0.20      0.18      0.19        17\n",
      "         143       0.36      0.24      0.29       298\n",
      "         144       0.14      0.29      0.19        41\n",
      "         145       0.63      0.67      0.65        83\n",
      "         146       0.21      0.52      0.30        25\n",
      "         147       0.09      0.08      0.08        25\n",
      "         148       0.44      0.55      0.49       209\n",
      "         149       0.12      0.33      0.18        15\n",
      "         150       0.34      0.41      0.37        61\n",
      "         151       0.32      0.50      0.39        20\n",
      "         152       0.22      0.25      0.23        20\n",
      "         153       0.41      0.26      0.32        34\n",
      "         154       0.79      0.72      0.76       144\n",
      "         155       0.65      0.49      0.56       387\n",
      "         156       0.39      0.23      0.29        56\n",
      "         157       0.29      0.08      0.13        24\n",
      "         158       0.46      0.51      0.49        51\n",
      "         159       0.56      0.39      0.46        23\n",
      "         160       0.30      0.30      0.30        44\n",
      "         161       0.70      0.51      0.59       240\n",
      "         162       0.12      0.22      0.16        27\n",
      "         163       0.71      0.26      0.38        19\n",
      "         164       0.58      0.34      0.43       161\n",
      "         165       0.38      0.50      0.43        66\n",
      "         166       0.46      0.59      0.52        70\n",
      "         167       0.85      0.74      0.79        61\n",
      "         168       0.33      0.27      0.30        92\n",
      "         169       0.55      0.62      0.58        34\n",
      "         170       0.41      0.41      0.41        37\n",
      "         171       0.44      0.54      0.49       223\n",
      "         172       0.06      0.17      0.09        24\n",
      "         173       0.80      0.67      0.73       248\n",
      "         174       0.73      0.50      0.59       191\n",
      "         175       0.19      0.21      0.20        53\n",
      "         176       0.68      0.69      0.69       236\n",
      "         177       0.55      0.31      0.39        39\n",
      "         178       0.84      0.58      0.69       201\n",
      "         179       0.26      0.39      0.31        31\n",
      "         180       0.73      0.63      0.68      1088\n",
      "         181       0.36      0.48      0.41       185\n",
      "         182       0.46      0.46      0.46        28\n",
      "         183       0.67      0.58      0.62       359\n",
      "         184       0.37      0.53      0.43        19\n",
      "         185       0.19      0.20      0.19        35\n",
      "         186       0.50      0.36      0.42        39\n",
      "         187       0.30      0.36      0.33        25\n",
      "\n",
      "   micro avg       0.55      0.55      0.55     18549\n",
      "   macro avg       0.42      0.43      0.41     18549\n",
      "weighted avg       0.59      0.55      0.56     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>precision (macro avg)</th>\n",
       "      <th>recall (macro avg)</th>\n",
       "      <th>f1 (macro avg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kNN+vectors sum</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logreg+vectors sum</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kNN+doc2vec</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logreg+doc2vec</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FNN+vectors sum</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FNN+doc2vec</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model precision (macro avg) recall (macro avg) f1 (macro avg)\n",
       "0     kNN+vectors sum                  0.40               0.35           0.36\n",
       "1  logreg+vectors sum                  0.42               0.42           0.41\n",
       "2         kNN+doc2vec                  0.51               0.50           0.49\n",
       "3      logreg+doc2vec                  0.58               0.47           0.50\n",
       "4     FNN+vectors sum                  0.62               0.48           0.53\n",
       "5         FNN+doc2vec                  0.47               0.40           0.42\n",
       "6                LSTM                  0.54               0.47           0.49\n",
       "7                 CNN                  0.42               0.43           0.41"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_metrics(model, group = 'macro avg'): \n",
    "    metrics = [line.strip() for line in  model.test_report.split('\\n') if group in line][0]\n",
    "    return [metric.strip() for metric in metrics[len(group)-1:].split()][1:-1]\n",
    "\n",
    "pandas.DataFrame({\n",
    "    'model': ['kNN+vectors sum',\n",
    "              'logreg+vectors sum',\n",
    "              'kNN+doc2vec',\n",
    "              'logreg+doc2vec',\n",
    "              'FNN+vectors sum',\n",
    "              'FNN+doc2vec',\n",
    "              'LSTM',\n",
    "              'CNN'],\n",
    "    'precision (macro avg)': [extract_metrics(knn)[0],\n",
    "                              extract_metrics(logreg)[0],\n",
    "                              extract_metrics(knn2)[0],\n",
    "                              extract_metrics(logreg2)[0],\n",
    "                              extract_metrics(ff_sum_model)[0],\n",
    "                              extract_metrics(ff_model)[0],\n",
    "                              extract_metrics(lstm_model)[0],\n",
    "                              extract_metrics(cnn_model)[0]],\n",
    "    'recall (macro avg)': [extract_metrics(knn)[1],\n",
    "                              extract_metrics(logreg)[1],\n",
    "                              extract_metrics(knn2)[1],\n",
    "                              extract_metrics(logreg2)[1],\n",
    "                              extract_metrics(ff_sum_model)[1],\n",
    "                              extract_metrics(ff_model)[1],\n",
    "                              extract_metrics(lstm_model)[1],\n",
    "                              extract_metrics(cnn_model)[1]],\n",
    "    'f1 (macro avg)': [extract_metrics(knn)[2],\n",
    "                              extract_metrics(logreg)[2],\n",
    "                              extract_metrics(knn2)[2],\n",
    "                              extract_metrics(logreg2)[2],\n",
    "                              extract_metrics(ff_sum_model)[2],\n",
    "                              extract_metrics(ff_model)[2],\n",
    "                              extract_metrics(lstm_model)[2],\n",
    "                              extract_metrics(cnn_model)[2]]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бачимо що вдалося покращити якість у порівнянні з бейзланойм більш ніж на 10% згідно F1 в порівнянні з бейзлайном. Логістична регресія в порівнянні з kNN у всіх випадках працювала краще. Вектори документів також дали покращення у всіх випадках. Найкращий результат дало застосування feed forward мережі в комбінації с Doc2Vec, дуже близький результат дала LSTM мережа. Цікаво що аналогічні результати мают згорточна мереже і логістична мережа на сумі векторів. Виглядає так что згортка має той самий єфект що й сумма ембедінгів."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
