{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Спочатку завантажуємо word embeddings для української мови."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://lang.org.ua/static/downloads/models/news.lowercased.tokenized.word2vec.300d.bz2 --output news.lowercased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bunzip2 news.lowercased.tokenized.word2vec.300d.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 560 ms, total: 1min 12s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "%time wv = KeyedVectors.load_word2vec_format('news.lowercased.tokenized.word2vec.300d', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('дієслово', 0.6502863764762878),\n",
       " ('слівце', 0.6484909653663635),\n",
       " ('словосполучення', 0.6456568241119385),\n",
       " ('гасло', 0.5913079977035522),\n",
       " ('слово**', 0.555127739906311),\n",
       " (\"прислів'я\", 0.5407627820968628),\n",
       " ('письмо', 0.5235773324966431),\n",
       " ('прізвище', 0.52119380235672),\n",
       " ('пророцтво', 0.5125285983085632),\n",
       " ('ремесло', 0.5058826804161072)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('слово')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потім розпаковуємо та завантажуємо дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace 1551/Інші-Подяки.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip -q ../../../tasks/1551.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аварійний--травмонебезпечний-стан-утримання-об-єктів-благоустрою.txt\r\n",
      "Бажаючі-отримати--Картки-киянина--КК--.txt\r\n",
      "Будівництво-АЗС.txt\r\n",
      "Будівництво-в-нічний-час.txt\r\n",
      "Будівництво-дооблаштування-дитячого-майданчику.txt\r\n",
      "Будівництво--дооблаштування-спортивних-майданчиків.txt\r\n",
      "Будівництво-та-реконструкція-об-єктів-освіти.txt\r\n",
      "Взаємовідносини-з-сусідами.txt\r\n",
      "Вивезення--утилізація-твердих-та-негабаритних-відходів.txt\r\n",
      "Видалення-аварійних--пошкоджених-хворобами-дерев.txt\r\n",
      "Видача-розрахункових-книжок--квитанцій--довідок.txt\r\n",
      "Вилов-безпритульних-тварин.txt\r\n",
      "Вирізування--кронування--гілля-дерев.txt\r\n",
      "Виток-холодної-води-на-поверхню.txt\r\n",
      "Відновлення-благоустрою-після-вик--планових-аварійних-робіт-на-об-єктах-благоуст.txt\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!ls 1551 | head -n 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1551/Незадовільна-температура-ГВП.txt',\n",
       " '1551/Незадовільне-обслуговування-в-амбулаторно-поліклінічних-установах.txt',\n",
       " '1551/Відсутнє-електропостачання.txt',\n",
       " '1551/Порушення-правил-тиші--після-------.txt',\n",
       " '1551/Неякісне-ХВП.txt',\n",
       " '1551/Нанесення-дорожньої-розмітки.txt',\n",
       " '1551/Робота-циркуляційної-системи.txt',\n",
       " '1551/Встановлення-світлофора.txt',\n",
       " '1551/Завезення-піску-на-дитячий-майданчик.txt',\n",
       " '1551/Скошування-трави.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"1551/*\")\n",
    "\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "Document = namedtuple('Document', 'id topic_id tags content')\n",
    "\n",
    "def parse_tags(file):\n",
    "    return [tag for tag in os.path.basename(file.name)[:-4].split('-') if tag]\n",
    "\n",
    "def parse_topic_file(topic_id, filename):\n",
    "    documents = []    \n",
    "    with open(filename) as f:\n",
    "        tags = parse_tags(f)        \n",
    "        _id = None\n",
    "        idx = -1        \n",
    "        for line in f:            \n",
    "            if line.strip().isnumeric():\n",
    "                _id = int(line.strip())\n",
    "                documents.append(Document(_id, topic_id, tags, []))\n",
    "                idx +=1\n",
    "                continue\n",
    "            if not (_id is None) and line.strip():                \n",
    "                documents[idx].content.append(line.strip())                \n",
    "    \n",
    "    return [doc._replace(content = ''.join(doc.content)) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114348"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents = [doc for topic_id, file in enumerate(files) \\\n",
    "                 for doc in parse_topic_file(topic_id, file) if len(doc.content) > 0]\n",
    "\n",
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id=2697865, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Недогрев горячей воды (вода нормальной температуры подавалась неделю с15 по 23, до этого была частичная подача горячей воды (пару часов вечером и ночью горячая), остальное время теплой), сейчас опять температура порядка 40 градусов. Эта ситуация продолжается на фоне постоянного недогрева батарей в квартире, ДУ 12 ККЕУ  МОУкраины  не реагирует на ситуацию.'),\n",
       " Document(id=3170827, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Из горячего крана течет холодная вода, в вечернее и утреннее время купаться нет возможности. Необходимо или пересчитывать тарифы или включать горячую воду.'),\n",
       " Document(id=3165270, topic_id=0, tags=['Незадовільна', 'температура', 'ГВП'], content='Я поживаю на 6 этаже 9и - этажного дома на протяжении долгого промежутка времени у нас в квартире из крана горячей воды, особенно утром и в первой половине дня течёт если не холодная вода, то вода чуть тёплая. По утрам для того чтобы совершить утренний туалет приходится долго спускать воду (и эта проблема у большей части жильцов нашего дома). В свете того, что с мая месяца у нас очень выросли тарифы на горячую воду, меня интересует вопрос - почему я должна платить деньги за некачественную услугу. Огромная просьба посодействовать в решении данной проблемы. Местные сантехники подтверждают, что проблемы с горячей водой не только в нашей квартире, но решить эту проблему они не могут, так как это от них не зависит.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер фільтруємо документи з українською мовою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from langdetect.detector import LangDetectException\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def memoize(filename, compute):  \n",
    "    \n",
    "    fullname = filename + '.can'\n",
    "    \n",
    "    if os.path.isfile(fullname):\n",
    "        with open(fullname, 'rb') as f:                        \n",
    "            return pickle.load(f)\n",
    "    \n",
    "    result = compute()\n",
    "    with open(fullname, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def is_uk(text):\n",
    "    \n",
    "    if len(text):\n",
    "        try:\n",
    "            return detect(text[:1024]) == 'uk'\n",
    "        except LangDetectException as e:\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "uk_documents = memoize('uk_documents', \n",
    "                       lambda: [doc for doc in tqdm_notebook(all_documents) if is_uk(doc.content)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дивимся на дані."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "uk_doc_df = pandas.DataFrame([doc._replace(tags = \"-\".join(doc.tags)) for doc in uk_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3152784</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Відсутнітність горячого водопостачання належно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3143050</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Добрий вечір.Прошу розібратися з проблемою нев...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3142427</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>На моє звернення № Г-6623 відповідь написав ди...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3130991</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Доброго дня! Вже більше двох тижнів гаряче вод...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2405990</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>На звернення:Номер звернення:\\tГ-6478Зареєстро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3115494</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Звертаюсь до Вас стосовно вирішення питання, щ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3104107</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Доброго дня!!! Моє звернення від 02.12.14 #Г-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3091571</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Протягом останнього тижня гаряча вода недостат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2690156</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>Прошу прийняти необхідні заходи по покращенню ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2748419</td>\n",
       "      <td>0</td>\n",
       "      <td>Незадовільна-температура-ГВП</td>\n",
       "      <td>немає  температури гарячої води</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  topic_id                          tags  \\\n",
       "0  3152784         0  Незадовільна-температура-ГВП   \n",
       "1  3143050         0  Незадовільна-температура-ГВП   \n",
       "2  3142427         0  Незадовільна-температура-ГВП   \n",
       "3  3130991         0  Незадовільна-температура-ГВП   \n",
       "4  2405990         0  Незадовільна-температура-ГВП   \n",
       "5  3115494         0  Незадовільна-температура-ГВП   \n",
       "6  3104107         0  Незадовільна-температура-ГВП   \n",
       "7  3091571         0  Незадовільна-температура-ГВП   \n",
       "8  2690156         0  Незадовільна-температура-ГВП   \n",
       "9  2748419         0  Незадовільна-температура-ГВП   \n",
       "\n",
       "                                             content  \n",
       "0  Відсутнітність горячого водопостачання належно...  \n",
       "1  Добрий вечір.Прошу розібратися з проблемою нев...  \n",
       "2  На моє звернення № Г-6623 відповідь написав ди...  \n",
       "3  Доброго дня! Вже більше двох тижнів гаряче вод...  \n",
       "4  На звернення:Номер звернення:\\tГ-6478Зареєстро...  \n",
       "5  Звертаюсь до Вас стосовно вирішення питання, щ...  \n",
       "6  Доброго дня!!! Моє звернення від 02.12.14 #Г-1...  \n",
       "7  Протягом останнього тижня гаряча вода недостат...  \n",
       "8  Прошу прийняти необхідні заходи по покращенню ...  \n",
       "9                    немає  температури гарячої води  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.182900e+04</td>\n",
       "      <td>61829.000000</td>\n",
       "      <td>61829</td>\n",
       "      <td>61829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188</td>\n",
       "      <td>56061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Відсутність-ГВП</td>\n",
       "      <td>Відсутнє гаряче водопостачання</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6564</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.159625e+06</td>\n",
       "      <td>105.551731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.084360e+07</td>\n",
       "      <td>55.922291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.841555e+06</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.083712e+06</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.245460e+06</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.013102e+09</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id      topic_id             tags  \\\n",
       "count   6.182900e+04  61829.000000            61829   \n",
       "unique           NaN           NaN              188   \n",
       "top              NaN           NaN  Відсутність-ГВП   \n",
       "freq             NaN           NaN             6564   \n",
       "mean    3.159625e+06    105.551731              NaN   \n",
       "std     1.084360e+07     55.922291              NaN   \n",
       "min     1.000000e+01      0.000000              NaN   \n",
       "25%     2.841555e+06     58.000000              NaN   \n",
       "50%     3.083712e+06    121.000000              NaN   \n",
       "75%     3.245460e+06    150.000000              NaN   \n",
       "max     2.013102e+09    187.000000              NaN   \n",
       "\n",
       "                               content  \n",
       "count                            61829  \n",
       "unique                           56061  \n",
       "top     Відсутнє гаряче водопостачання  \n",
       "freq                                46  \n",
       "mean                               NaN  \n",
       "std                                NaN  \n",
       "min                                NaN  \n",
       "25%                                NaN  \n",
       "50%                                NaN  \n",
       "75%                                NaN  \n",
       "max                                NaN  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic_id</th>\n",
       "      <th>tags</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <th>Відсутність-ГВП</th>\n",
       "      <td>6564</td>\n",
       "      <td>6564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <th>Укладання-та-ремонт-асфальтного-покриття</th>\n",
       "      <td>3628</td>\n",
       "      <td>3628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <th>Відсутність-опалення</th>\n",
       "      <td>3142</td>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <th>Перевірка-дозвільної-документації-демонтаж-кіосків-ларків</th>\n",
       "      <td>2199</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <th>Прибирання-та-санітарний-стан-територій</th>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <th>Технічний-стан-проїжджих-частин-вулиць-та-тротуарів</th>\n",
       "      <td>1303</td>\n",
       "      <td>1303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <th>Відновлення-благоустрою-після-вик-планових-аварійних-робіт-на-об-єктах-благоуст</th>\n",
       "      <td>1289</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <th>Відсутність-освітлення-у-під-їзді-за-відсутності-несправності-лампочок</th>\n",
       "      <td>1256</td>\n",
       "      <td>1256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>Не-працює-пасажирський-ліфт</th>\n",
       "      <td>1220</td>\n",
       "      <td>1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <th>Ремонт-під-їзду</th>\n",
       "      <td>1198</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>Незадовільна-температура-ГВП</th>\n",
       "      <td>1116</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <th>Перерахунок-та-нарахування-плати-за-інші-види-житлово-комунальних-послуг</th>\n",
       "      <td>1097</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <th>Про-розгляд-звернень-громадян</th>\n",
       "      <td>993</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <th>Відсутність-опалення-по-стояку</th>\n",
       "      <td>989</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <th>ГЛ-Несанкціонована-торгівля</th>\n",
       "      <td>826</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <th>Прибирання-приміщень</th>\n",
       "      <td>800</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <th>Відсутнє-ХВП</th>\n",
       "      <td>785</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <th>Освітлення-в-приміщенні-й-при-вході-в-нього</th>\n",
       "      <td>743</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <th>Інші-технічні-недоліки-стану-ліфту</th>\n",
       "      <td>695</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <th>Ремонт-дахів</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <th>Перевірка-наявності-дозволів-на-виконання-будівельних-робіт</th>\n",
       "      <td>646</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <th>Незадовільна-температура-опалення</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <th>Будівництво-дооблаштування-дитячого-майданчику</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <th>Утримання-підвалів-колясочних-технічних-поверхів</th>\n",
       "      <td>617</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <th>Відсутність-освітлення-на-опорних-стовпах-за-відсутності-несправності-лампочок</th>\n",
       "      <td>538</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <th>Питання-освітлення-на-опорних-стовпах</th>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <th>Встановлення-та-експлуатація-лічильників-на-водопостачання</th>\n",
       "      <td>481</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>Робота-світлофора</th>\n",
       "      <td>480</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <th>Стихійне-сміттєзвалище</th>\n",
       "      <td>477</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>Робота-циркуляційної-системи</th>\n",
       "      <td>473</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <th>Технічне-обслуговування-систем-тепло-водопостачання-та-водовідведення-і-зливов</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <th>Демонтаж-рекламних-конструкцій-і-вивісок</th>\n",
       "      <td>458</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <th>Скління-та-ремонт-вікон-на-сходових-клітинах</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <th>Встановлення-лічильників-на-опалення</th>\n",
       "      <td>430</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>Вологе-прибирання-приміщень</th>\n",
       "      <td>407</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <th>Незадовільний-вивіз-сміття-з-контейнерів-та-урн-для-сміття</th>\n",
       "      <td>383</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <th>Встановлення-та-експлуатація-дорожніх-знаків</th>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Нанесення-дорожньої-розмітки</th>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <th>Паркування-авто-у-місцях-загального-користування</th>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <th>Встановлення-сміттєвих-контейнерів-та-урн-для-сміття</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <th>Видалення-аварійних-пошкоджених-хворобами-дерев</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <th>Контроль-за-станом-рекламних-засобів</th>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>Перевірка-дозвільної-документації-демонтаж-літніх-майданчиків-кафе-ресторанів</th>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <th>Інші-Подяки</th>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>Встановлення-огородження-зеленої-зони</th>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <th>Знищення-омели-амброзії-та-рослин-паразитів</th>\n",
       "      <td>283</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>Аварійний-травмонебезпечний-стан-утримання-об-єктів-благоустрою</th>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <th>Не-працює-вантажний-ліфт</th>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <th>Встановлення-сигнальних-стовпчиків-бар-єрних-огороджень-бордюрів</th>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <th>Ремонт-і-обслуговування-сміттєпроводів-та-сміттєзбірників-в-приміщенні</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               id  content\n",
       "topic_id tags                                                             \n",
       "138      Відсутність-ГВП                                     6564     6564\n",
       "180      Укладання-та-ремонт-асфальтного-покриття            3628     3628\n",
       "27       Відсутність-опалення                                3142     3142\n",
       "127      Перевірка-дозвільної-документації-демонтаж-кіос...  2199     2199\n",
       "67       Прибирання-та-санітарний-стан-територій             2005     2005\n",
       "79       Технічний-стан-проїжджих-частин-вулиць-та-троту...  1303     1303\n",
       "155      Відновлення-благоустрою-після-вик-планових-авар...  1289     1289\n",
       "121      Відсутність-освітлення-у-під-їзді-за-відсутност...  1256     1256\n",
       "58       Не-працює-пасажирський-ліфт                         1220     1220\n",
       "183      Ремонт-під-їзду                                     1198     1198\n",
       "0        Незадовільна-температура-ГВП                        1116     1116\n",
       "101      Перерахунок-та-нарахування-плати-за-інші-види-ж...  1097     1097\n",
       "143      Про-розгляд-звернень-громадян                        993      993\n",
       "88       Відсутність-опалення-по-стояку                       989      989\n",
       "173      ГЛ-Несанкціонована-торгівля                          826      826\n",
       "161      Прибирання-приміщень                                 800      800\n",
       "176      Відсутнє-ХВП                                         785      785\n",
       "171      Освітлення-в-приміщенні-й-при-вході-в-нього          743      743\n",
       "148      Інші-технічні-недоліки-стану-ліфту                   695      695\n",
       "178      Ремонт-дахів                                         671      671\n",
       "22       Перевірка-наявності-дозволів-на-виконання-будів...   646      646\n",
       "174      Незадовільна-температура-опалення                    635      635\n",
       "68       Будівництво-дооблаштування-дитячого-майданчику       633      633\n",
       "181      Утримання-підвалів-колясочних-технічних-поверхів     617      617\n",
       "164      Відсутність-освітлення-на-опорних-стовпах-за-ві...   538      538\n",
       "109      Питання-освітлення-на-опорних-стовпах                516      516\n",
       "154      Встановлення-та-експлуатація-лічильників-на-вод...   481      481\n",
       "10       Робота-світлофора                                    480      480\n",
       "41       Стихійне-сміттєзвалище                               477      477\n",
       "6        Робота-циркуляційної-системи                         473      473\n",
       "131      Технічне-обслуговування-систем-тепло-водопостач...   458      458\n",
       "114      Демонтаж-рекламних-конструкцій-і-вивісок             458      458\n",
       "116      Скління-та-ремонт-вікон-на-сходових-клітинах         431      431\n",
       "33       Встановлення-лічильників-на-опалення                 430      430\n",
       "16       Вологе-прибирання-приміщень                          407      407\n",
       "40       Незадовільний-вивіз-сміття-з-контейнерів-та-урн...   383      383\n",
       "57       Встановлення-та-експлуатація-дорожніх-знаків         359      359\n",
       "5        Нанесення-дорожньої-розмітки                         353      353\n",
       "69       Паркування-авто-у-місцях-загального-користування     351      351\n",
       "78       Встановлення-сміттєвих-контейнерів-та-урн-для-с...   348      348\n",
       "136      Видалення-аварійних-пошкоджених-хворобами-дерев      321      321\n",
       "62       Контроль-за-станом-рекламних-засобів                 318      318\n",
       "32       Перевірка-дозвільної-документації-демонтаж-літн...   316      316\n",
       "168      Інші-Подяки                                          307      307\n",
       "105      Встановлення-огородження-зеленої-зони                289      289\n",
       "74       Знищення-омели-амброзії-та-рослин-паразитів          283      283\n",
       "14       Аварійний-травмонебезпечний-стан-утримання-об-є...   280      280\n",
       "145      Не-працює-вантажний-ліфт                             275      275\n",
       "128      Встановлення-сигнальних-стовпчиків-бар-єрних-ог...   273      273\n",
       "112      Ремонт-і-обслуговування-сміттєпроводів-та-смітт...   265      265"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_doc_df.groupby(['topic_id', 'tags']).count().sort_values(['id'], ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виділяємо лейбли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61829"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_labels = np.array([doc.topic_id for doc in uk_documents])\n",
    "len(topic_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "І розбиваємо дані на тренувальні і тестові."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_documents, test_documents, train_topic_labels, test_topic_labels = \\\n",
    "    train_test_split(uk_documents, topic_labels, random_state = 26, test_size = 0.3, stratify = topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43280\n",
      "43280\n"
     ]
    }
   ],
   "source": [
    "print(len(train_documents))\n",
    "print(len(train_topic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18549\n",
      "18549\n"
     ]
    }
   ],
   "source": [
    "print(len(test_documents))\n",
    "print(len(test_topic_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Будуємо бейзлайн, знаходимо суму векторів слів по кожному документу і використовуємо kNN на знайденних векторах. Для порівняння векторів застосовуємо cosine similarity. Перед знаходженням суми векторів, документ токенізується та видаляються stop words. Знайдені вектори нормалізуються, в такому випадку eclidean distance для kNN має той самий ефект що й cosine distance, при цьому алгоритм дозволяє використовувати більш ефективні структури данних, такі як, наприклад, k-d tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize_uk\n",
    "import string\n",
    "import html\n",
    "import re\n",
    "\n",
    "with open('uk_stop_words.txt') as f:\n",
    "    STOP_WORDS = f.read().split()\n",
    "    \n",
    "EXT_PUNCTUATION = \"”...«»№\"\n",
    "\n",
    "def contain_numbers(s):\n",
    "    return bool(re.search(r'\\d', s))\n",
    "\n",
    "def non_stop_word(word):\n",
    "    return not (word in string.punctuation or word in EXT_PUNCTUATION \\\n",
    "                or word in STOP_WORDS or contain_numbers(word) or len(word) < 2)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if non_stop_word(token.lower())]\n",
    "\n",
    "def tokenize_doc(doc):\n",
    "    return [word.lower() for word in \\\n",
    "            remove_stop_words(tokenize_uk.tokenize_words(html.unescape(doc.content)))]\n",
    "\n",
    "def normalize_vec(x):\n",
    "    m = np.max(x)\n",
    "    if m > 0.0:\n",
    "        return x/np.sqrt(np.dot(x,x))\n",
    "    return x\n",
    "    \n",
    "def doc_to_sum_vec(doc):\n",
    "    words = tokenize_doc(doc)    \n",
    "    vec = np.zeros(300)\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec += wv[word]\n",
    "        except KeyError as e:            \n",
    "            pass\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рахуємо вектори для тренувальних і тестових документів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b0f947e37d40f3851b135f70134a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43280), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_doc_sum_vecs = np.array([doc_to_sum_vec(doc) for doc in tqdm_notebook(train_documents)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd37d2a3bff34de1a8494f7decdbd9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18549), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_doc_sum_vecs = np.array([doc_to_sum_vec(doc) for doc in tqdm_notebook(test_documents)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренуємо модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels):\n",
    "        self.train_vectors = train_vectors\n",
    "        self.train_labels = train_labels\n",
    "        self.test_vectors = test_vectors\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.fit(self.train_vectors, self.train_labels)\n",
    "        self.topics_predicted = self.model.predict(self.test_vectors)\n",
    "        \n",
    "    def test(self):\n",
    "        print(classification_report(self.test_labels, self.topics_predicted))  \n",
    "\n",
    "class KnnModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels, n = 1):\n",
    "        super().__init__(np.array([normalize_vec(doc) for doc in train_vectors]),\\\n",
    "                       train_labels,\\\n",
    "                       np.array([normalize_vec(doc) for doc in test_vectors]),\\\n",
    "                       test_labels)                \n",
    "        self.model = KNeighborsClassifier(n_neighbors = n, algorithm='kd_tree', metric = 'euclidean', n_jobs = 6)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KnnModel(train_doc_sum_vecs, train_topic_labels, test_doc_sum_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min, sys: 0 ns, total: 11min\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%time knn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.55      0.50       335\n",
      "           1       0.77      0.80      0.79        41\n",
      "           2       0.55      0.30      0.39        79\n",
      "           3       0.38      0.28      0.32        39\n",
      "           4       0.88      0.32      0.47        22\n",
      "           5       0.42      0.30      0.35       106\n",
      "           6       0.22      0.32      0.26       142\n",
      "           7       0.25      0.32      0.28        62\n",
      "           8       0.81      0.43      0.57        30\n",
      "           9       0.40      0.36      0.38        22\n",
      "          10       0.57      0.53      0.55       144\n",
      "          11       0.21      0.17      0.19        29\n",
      "          12       0.44      0.36      0.40        22\n",
      "          13       0.33      0.31      0.32        45\n",
      "          14       0.23      0.18      0.20        84\n",
      "          15       0.40      0.25      0.31        16\n",
      "          16       0.51      0.48      0.49       122\n",
      "          17       0.25      0.19      0.21        16\n",
      "          18       0.29      0.28      0.29        53\n",
      "          19       0.36      0.26      0.30        19\n",
      "          20       0.83      0.31      0.45        16\n",
      "          21       0.21      0.25      0.23        24\n",
      "          22       0.46      0.43      0.45       194\n",
      "          23       0.81      0.49      0.61        51\n",
      "          24       0.46      0.40      0.43        47\n",
      "          25       0.29      0.32      0.31        28\n",
      "          26       0.50      0.24      0.32        25\n",
      "          27       0.55      0.63      0.59       943\n",
      "          28       0.31      0.30      0.31        56\n",
      "          29       0.25      0.33      0.29        63\n",
      "          30       0.23      0.16      0.19        44\n",
      "          31       0.25      0.04      0.07        25\n",
      "          32       0.53      0.55      0.54        95\n",
      "          33       0.54      0.59      0.56       129\n",
      "          34       0.40      0.47      0.43        45\n",
      "          35       0.21      0.20      0.21        35\n",
      "          36       0.24      0.22      0.23        50\n",
      "          37       0.33      0.23      0.27        40\n",
      "          38       0.53      0.41      0.46        39\n",
      "          39       0.41      0.34      0.37        44\n",
      "          40       0.55      0.57      0.56       115\n",
      "          41       0.41      0.36      0.38       143\n",
      "          42       0.57      0.65      0.61        62\n",
      "          43       0.30      0.15      0.20        41\n",
      "          44       0.19      0.21      0.20        29\n",
      "          45       0.32      0.25      0.28        71\n",
      "          46       0.51      0.43      0.47        56\n",
      "          47       0.22      0.19      0.21        21\n",
      "          48       0.11      0.12      0.12        16\n",
      "          49       0.42      0.26      0.32        19\n",
      "          50       0.36      0.20      0.26        44\n",
      "          51       0.44      0.20      0.28        20\n",
      "          52       0.13      0.22      0.16        27\n",
      "          53       0.44      0.47      0.45        53\n",
      "          54       0.34      0.56      0.43        18\n",
      "          55       0.17      0.23      0.20        26\n",
      "          56       0.56      0.32      0.41        28\n",
      "          57       0.32      0.38      0.35       108\n",
      "          58       0.55      0.52      0.54       366\n",
      "          59       0.23      0.15      0.18        74\n",
      "          60       0.30      0.29      0.30        34\n",
      "          61       0.23      0.29      0.25        24\n",
      "          62       0.45      0.39      0.42        95\n",
      "          63       0.17      0.11      0.13        18\n",
      "          64       0.67      0.54      0.60        37\n",
      "          65       0.62      0.33      0.43        15\n",
      "          66       0.28      0.30      0.29        33\n",
      "          67       0.51      0.58      0.54       602\n",
      "          68       0.49      0.61      0.54       190\n",
      "          69       0.27      0.31      0.29       105\n",
      "          70       0.55      0.44      0.49        36\n",
      "          71       0.43      0.27      0.33        48\n",
      "          72       0.27      0.18      0.22        22\n",
      "          73       0.23      0.27      0.25        33\n",
      "          74       0.79      0.76      0.78        85\n",
      "          75       0.52      0.53      0.52        30\n",
      "          76       0.61      0.46      0.52        50\n",
      "          77       0.32      0.21      0.25        43\n",
      "          78       0.41      0.37      0.39       104\n",
      "          79       0.39      0.42      0.40       391\n",
      "          80       0.20      0.06      0.09        17\n",
      "          81       0.67      0.64      0.65        28\n",
      "          82       0.24      0.30      0.27        20\n",
      "          83       0.23      0.12      0.16        24\n",
      "          84       0.37      0.42      0.39        53\n",
      "          85       0.57      0.35      0.43        23\n",
      "          86       0.40      0.23      0.29        74\n",
      "          87       0.24      0.35      0.29        34\n",
      "          88       0.40      0.38      0.39       297\n",
      "          89       0.55      0.45      0.49        40\n",
      "          90       0.30      0.19      0.23        16\n",
      "          91       0.33      0.26      0.29        39\n",
      "          92       0.32      0.35      0.33        26\n",
      "          93       0.81      0.59      0.68        22\n",
      "          94       0.29      0.30      0.29        20\n",
      "          95       0.25      0.07      0.11        42\n",
      "          96       0.27      0.14      0.19        21\n",
      "          97       0.65      0.46      0.54        24\n",
      "          98       0.35      0.36      0.35        25\n",
      "          99       0.50      0.41      0.45        34\n",
      "         100       0.22      0.40      0.29        55\n",
      "         101       0.50      0.57      0.53       329\n",
      "         102       0.67      0.43      0.52        75\n",
      "         103       0.46      0.32      0.37        19\n",
      "         104       0.81      0.58      0.68        38\n",
      "         105       0.20      0.15      0.17        87\n",
      "         106       0.16      0.30      0.21        47\n",
      "         107       0.69      0.28      0.40        32\n",
      "         108       0.78      0.67      0.72        21\n",
      "         109       0.38      0.37      0.37       155\n",
      "         110       0.28      0.30      0.29        66\n",
      "         111       0.50      0.65      0.57        20\n",
      "         112       0.19      0.19      0.19        80\n",
      "         113       0.38      0.45      0.41        56\n",
      "         114       0.43      0.42      0.43       137\n",
      "         115       0.25      0.18      0.21        17\n",
      "         116       0.46      0.49      0.47       129\n",
      "         117       0.20      0.15      0.17        78\n",
      "         118       0.30      0.28      0.29        39\n",
      "         119       0.33      0.30      0.31        47\n",
      "         120       0.26      0.43      0.32        14\n",
      "         121       0.48      0.47      0.48       377\n",
      "         122       0.56      0.61      0.58        38\n",
      "         123       0.24      0.33      0.28        78\n",
      "         124       0.19      0.16      0.17        25\n",
      "         125       0.34      0.40      0.37        73\n",
      "         126       0.79      0.75      0.77        20\n",
      "         127       0.53      0.52      0.53       660\n",
      "         128       0.20      0.29      0.24        82\n",
      "         129       0.53      0.50      0.51        20\n",
      "         130       0.12      0.06      0.08        18\n",
      "         131       0.23      0.25      0.24       137\n",
      "         132       0.26      0.23      0.24        43\n",
      "         133       0.35      0.24      0.28        55\n",
      "         134       0.52      0.44      0.47        64\n",
      "         135       0.34      0.28      0.31        57\n",
      "         136       0.44      0.36      0.40        96\n",
      "         137       0.31      0.21      0.25        61\n",
      "         138       0.69      0.73      0.71      1969\n",
      "         139       0.44      0.35      0.39        54\n",
      "         140       0.25      0.19      0.21        16\n",
      "         141       0.38      0.40      0.39        25\n",
      "         142       0.18      0.24      0.21        17\n",
      "         143       0.37      0.35      0.36       298\n",
      "         144       0.14      0.10      0.11        41\n",
      "         145       0.46      0.52      0.49        83\n",
      "         146       0.56      0.40      0.47        25\n",
      "         147       0.30      0.12      0.17        25\n",
      "         148       0.34      0.39      0.36       209\n",
      "         149       0.46      0.40      0.43        15\n",
      "         150       0.30      0.36      0.33        61\n",
      "         151       0.27      0.30      0.29        20\n",
      "         152       0.42      0.25      0.31        20\n",
      "         153       0.47      0.24      0.31        34\n",
      "         154       0.52      0.52      0.52       144\n",
      "         155       0.42      0.43      0.42       387\n",
      "         156       0.24      0.21      0.23        56\n",
      "         157       0.18      0.12      0.15        24\n",
      "         158       0.40      0.37      0.39        51\n",
      "         159       0.57      0.35      0.43        23\n",
      "         160       0.27      0.27      0.27        44\n",
      "         161       0.48      0.43      0.45       240\n",
      "         162       0.44      0.26      0.33        27\n",
      "         163       0.29      0.26      0.28        19\n",
      "         164       0.33      0.34      0.34       161\n",
      "         165       0.56      0.30      0.39        66\n",
      "         166       0.40      0.33      0.36        70\n",
      "         167       0.55      0.43      0.48        61\n",
      "         168       0.46      0.17      0.25        92\n",
      "         169       0.39      0.53      0.45        34\n",
      "         170       0.19      0.16      0.18        37\n",
      "         171       0.36      0.39      0.37       223\n",
      "         172       0.20      0.08      0.12        24\n",
      "         173       0.62      0.58      0.60       248\n",
      "         174       0.45      0.46      0.45       191\n",
      "         175       0.20      0.21      0.20        53\n",
      "         176       0.42      0.32      0.36       236\n",
      "         177       0.32      0.28      0.30        39\n",
      "         178       0.37      0.48      0.42       201\n",
      "         179       0.12      0.16      0.14        31\n",
      "         180       0.51      0.57      0.54      1088\n",
      "         181       0.37      0.36      0.37       185\n",
      "         182       0.30      0.25      0.27        28\n",
      "         183       0.41      0.45      0.43       359\n",
      "         184       0.52      0.68      0.59        19\n",
      "         185       0.16      0.14      0.15        35\n",
      "         186       0.47      0.23      0.31        39\n",
      "         187       0.42      0.40      0.41        25\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     18549\n",
      "   macro avg       0.40      0.35      0.36     18549\n",
      "weighted avg       0.46      0.46      0.46     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imrovements\n",
    "\n",
    "Намагаємося покращити результат. Спочатку будемо використовувати логістичну регресію, потім проробимо все те саме але з векторами Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LogregModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels, iters = 3000):\n",
    "        super().__init__(train_vectors, train_labels, test_vectors, test_labels)\n",
    "        self.model = LogisticRegression(random_state=26, n_jobs = 6, solver=\"lbfgs\", \\\n",
    "                                        multi_class=\"multinomial\", max_iter = iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogregModel(train_doc_sum_vecs, train_topic_labels, test_doc_sum_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 0 ns, total: 1.44 s\n",
      "Wall time: 38min 23s\n"
     ]
    }
   ],
   "source": [
    "%time logreg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.53      0.59       335\n",
      "           1       0.81      0.83      0.82        41\n",
      "           2       0.46      0.52      0.49        79\n",
      "           3       0.41      0.36      0.38        39\n",
      "           4       0.37      0.32      0.34        22\n",
      "           5       0.62      0.47      0.53       106\n",
      "           6       0.31      0.19      0.24       142\n",
      "           7       0.35      0.58      0.43        62\n",
      "           8       0.75      0.50      0.60        30\n",
      "           9       0.48      0.55      0.51        22\n",
      "          10       0.62      0.69      0.65       144\n",
      "          11       0.20      0.31      0.24        29\n",
      "          12       0.83      0.45      0.59        22\n",
      "          13       0.28      0.33      0.31        45\n",
      "          14       0.14      0.12      0.13        84\n",
      "          15       0.38      0.50      0.43        16\n",
      "          16       0.56      0.55      0.56       122\n",
      "          17       0.80      0.25      0.38        16\n",
      "          18       0.26      0.23      0.24        53\n",
      "          19       0.35      0.32      0.33        19\n",
      "          20       0.31      0.25      0.28        16\n",
      "          21       0.13      0.25      0.17        24\n",
      "          22       0.53      0.49      0.51       194\n",
      "          23       0.90      0.71      0.79        51\n",
      "          24       0.54      0.53      0.54        47\n",
      "          25       0.35      0.39      0.37        28\n",
      "          26       0.48      0.52      0.50        25\n",
      "          27       0.71      0.71      0.71       943\n",
      "          28       0.31      0.39      0.34        56\n",
      "          29       0.35      0.51      0.42        63\n",
      "          30       0.24      0.36      0.29        44\n",
      "          31       0.06      0.08      0.07        25\n",
      "          32       0.54      0.72      0.62        95\n",
      "          33       0.63      0.67      0.65       129\n",
      "          34       0.34      0.42      0.38        45\n",
      "          35       0.22      0.29      0.25        35\n",
      "          36       0.25      0.30      0.28        50\n",
      "          37       0.09      0.12      0.10        40\n",
      "          38       0.69      0.64      0.67        39\n",
      "          39       0.18      0.27      0.22        44\n",
      "          40       0.55      0.59      0.57       115\n",
      "          41       0.53      0.43      0.48       143\n",
      "          42       0.54      0.69      0.61        62\n",
      "          43       0.33      0.44      0.37        41\n",
      "          44       0.17      0.28      0.21        29\n",
      "          45       0.41      0.48      0.44        71\n",
      "          46       0.59      0.61      0.60        56\n",
      "          47       0.14      0.29      0.18        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.38      0.42      0.40        19\n",
      "          50       0.31      0.34      0.33        44\n",
      "          51       0.56      0.50      0.53        20\n",
      "          52       0.17      0.19      0.18        27\n",
      "          53       0.43      0.57      0.49        53\n",
      "          54       0.17      0.33      0.23        18\n",
      "          55       0.17      0.31      0.22        26\n",
      "          56       0.79      0.68      0.73        28\n",
      "          57       0.56      0.60      0.58       108\n",
      "          58       0.66      0.64      0.65       366\n",
      "          59       0.23      0.22      0.22        74\n",
      "          60       0.43      0.53      0.47        34\n",
      "          61       0.20      0.38      0.26        24\n",
      "          62       0.38      0.41      0.39        95\n",
      "          63       0.17      0.28      0.21        18\n",
      "          64       0.68      0.68      0.68        37\n",
      "          65       0.38      0.40      0.39        15\n",
      "          66       0.26      0.30      0.28        33\n",
      "          67       0.60      0.61      0.60       602\n",
      "          68       0.60      0.65      0.62       190\n",
      "          69       0.33      0.30      0.31       105\n",
      "          70       0.68      0.75      0.71        36\n",
      "          71       0.33      0.40      0.36        48\n",
      "          72       0.20      0.18      0.19        22\n",
      "          73       0.40      0.52      0.45        33\n",
      "          74       0.73      0.71      0.72        85\n",
      "          75       0.54      0.67      0.60        30\n",
      "          76       0.69      0.54      0.61        50\n",
      "          77       0.34      0.37      0.36        43\n",
      "          78       0.46      0.58      0.51       104\n",
      "          79       0.57      0.38      0.46       391\n",
      "          80       0.25      0.29      0.27        17\n",
      "          81       0.85      0.79      0.81        28\n",
      "          82       0.20      0.30      0.24        20\n",
      "          83       0.18      0.21      0.19        24\n",
      "          84       0.38      0.40      0.39        53\n",
      "          85       0.40      0.52      0.45        23\n",
      "          86       0.24      0.32      0.28        74\n",
      "          87       0.19      0.32      0.24        34\n",
      "          88       0.55      0.43      0.48       297\n",
      "          89       0.67      0.55      0.60        40\n",
      "          90       0.17      0.12      0.14        16\n",
      "          91       0.24      0.31      0.27        39\n",
      "          92       0.43      0.38      0.41        26\n",
      "          93       0.86      0.82      0.84        22\n",
      "          94       0.14      0.30      0.19        20\n",
      "          95       0.33      0.38      0.35        42\n",
      "          96       0.56      0.48      0.51        21\n",
      "          97       0.73      0.67      0.70        24\n",
      "          98       0.16      0.20      0.18        25\n",
      "          99       0.24      0.26      0.25        34\n",
      "         100       0.28      0.33      0.30        55\n",
      "         101       0.57      0.43      0.49       329\n",
      "         102       0.59      0.63      0.61        75\n",
      "         103       0.26      0.32      0.29        19\n",
      "         104       0.90      0.68      0.78        38\n",
      "         105       0.27      0.30      0.28        87\n",
      "         106       0.07      0.09      0.08        47\n",
      "         107       0.40      0.31      0.35        32\n",
      "         108       0.71      0.48      0.57        21\n",
      "         109       0.41      0.39      0.40       155\n",
      "         110       0.33      0.32      0.32        66\n",
      "         111       0.57      0.65      0.60        20\n",
      "         112       0.34      0.31      0.32        80\n",
      "         113       0.39      0.43      0.41        56\n",
      "         114       0.51      0.45      0.48       137\n",
      "         115       0.08      0.06      0.07        17\n",
      "         116       0.59      0.62      0.61       129\n",
      "         117       0.14      0.10      0.12        78\n",
      "         118       0.15      0.28      0.19        39\n",
      "         119       0.40      0.49      0.44        47\n",
      "         120       0.50      0.50      0.50        14\n",
      "         121       0.53      0.51      0.52       377\n",
      "         122       0.62      0.55      0.58        38\n",
      "         123       0.24      0.26      0.25        78\n",
      "         124       0.29      0.40      0.33        25\n",
      "         125       0.32      0.40      0.36        73\n",
      "         126       0.87      0.65      0.74        20\n",
      "         127       0.69      0.62      0.66       660\n",
      "         128       0.22      0.22      0.22        82\n",
      "         129       0.33      0.40      0.36        20\n",
      "         130       0.56      0.28      0.37        18\n",
      "         131       0.26      0.21      0.23       137\n",
      "         132       0.21      0.33      0.25        43\n",
      "         133       0.22      0.29      0.25        55\n",
      "         134       0.42      0.47      0.44        64\n",
      "         135       0.29      0.28      0.28        57\n",
      "         136       0.49      0.48      0.49        96\n",
      "         137       0.57      0.64      0.60        61\n",
      "         138       0.77      0.80      0.79      1969\n",
      "         139       0.48      0.54      0.50        54\n",
      "         140       0.14      0.25      0.18        16\n",
      "         141       0.20      0.32      0.25        25\n",
      "         142       0.44      0.41      0.42        17\n",
      "         143       0.28      0.20      0.23       298\n",
      "         144       0.14      0.17      0.16        41\n",
      "         145       0.42      0.53      0.47        83\n",
      "         146       0.50      0.60      0.55        25\n",
      "         147       0.13      0.16      0.14        25\n",
      "         148       0.44      0.38      0.41       209\n",
      "         149       0.25      0.07      0.11        15\n",
      "         150       0.36      0.43      0.39        61\n",
      "         151       0.41      0.35      0.38        20\n",
      "         152       0.38      0.45      0.41        20\n",
      "         153       0.31      0.32      0.31        34\n",
      "         154       0.65      0.62      0.64       144\n",
      "         155       0.58      0.48      0.53       387\n",
      "         156       0.20      0.25      0.22        56\n",
      "         157       0.47      0.29      0.36        24\n",
      "         158       0.30      0.41      0.35        51\n",
      "         159       0.53      0.43      0.48        23\n",
      "         160       0.19      0.20      0.20        44\n",
      "         161       0.61      0.55      0.58       240\n",
      "         162       0.46      0.48      0.47        27\n",
      "         163       0.58      0.37      0.45        19\n",
      "         164       0.47      0.38      0.42       161\n",
      "         165       0.47      0.48      0.48        66\n",
      "         166       0.33      0.37      0.35        70\n",
      "         167       0.80      0.70      0.75        61\n",
      "         168       0.28      0.26      0.27        92\n",
      "         169       0.57      0.76      0.65        34\n",
      "         170       0.29      0.38      0.33        37\n",
      "         171       0.38      0.32      0.35       223\n",
      "         172       0.06      0.12      0.08        24\n",
      "         173       0.71      0.67      0.69       248\n",
      "         174       0.56      0.43      0.49       191\n",
      "         175       0.20      0.30      0.24        53\n",
      "         176       0.58      0.49      0.53       236\n",
      "         177       0.26      0.33      0.29        39\n",
      "         178       0.70      0.58      0.64       201\n",
      "         179       0.15      0.26      0.19        31\n",
      "         180       0.64      0.66      0.65      1088\n",
      "         181       0.45      0.37      0.40       185\n",
      "         182       0.28      0.39      0.32        28\n",
      "         183       0.56      0.51      0.53       359\n",
      "         184       0.71      0.63      0.67        19\n",
      "         185       0.13      0.20      0.16        35\n",
      "         186       0.22      0.38      0.28        39\n",
      "         187       0.65      0.52      0.58        25\n",
      "\n",
      "   micro avg       0.52      0.52      0.52     18549\n",
      "   macro avg       0.42      0.42      0.41     18549\n",
      "weighted avg       0.54      0.52      0.52     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Є невелике покращення в якості. Переходимо до Doc2Vec. Для цього використовуємо gensim. Спочатку конвертуємо наші документи в модель gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "def to_tagged_doc(doc):\n",
    "    words = tokenize_doc(doc)\n",
    "    return TaggedDocument(words, [doc.topic_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['заявляю', 'чергове', 'втручання', 'діяльність', 'інформаційної', 'системи', 'колцентру', 'метою', 'викривлення', 'відомостей', 'стосовно', 'результатів', 'вирішення', 'поданих', 'звернень', 'даний', 'відмітку', 'виконано', 'наводжу', 'витяг', 'березня', 'перегляд', 'інтерактивній', 'картівідсутність', 'гвп', 'відповідальний', 'кп', 'печерська', 'брама', 'мазурчак', 'олександр', 'володимирович', 'дата', 'контролю', 'березня', 'статус', 'виконано', 'відповідаю', 'дійсності', 'оскільки', 'годин', 'офіційно', 'отриманий', 'електронний', 'запит', 'виклав', 'текст', 'протилежного', 'змісту', 'наводжу', 'заявника', 'квітня', 'статус', 'виконано', 'квартира', 'розташована', 'му', 'під’їзді', 'відношення', 'перекриття', 'стояка', 'го', 'заміна', 'вентиля', 'сусідів', 'потребує', 'перекриття', 'водопостачання', 'будинку', 'викличе', 'появу', 'трубах', 'будинку', 'стільки', 'бруду', 'зливався', 'понад', 'хвилини', 'хвилину', 'вигаданих', 'нормативів', 'кму', 'скарги', 'взагалі', 'подавались', 'заради', 'отримання', 'пустих', 'роз’яснень', 'усунення', 'проблеми', 'рахунок', 'винуватців', 'поборами', 'див', 'договір', 'викликає', 'подив', 'подане', 'звернення', 'зареєстроване', 'оскільки', 'начебто', 'другу', 'добу', 'перевіряється', 'оператором', '???', 'наведене', 'криміналом', 'корупційними', 'діяннями', '!!!'], tags=[0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tagged_doc(uk_documents[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f74392091e1467e9417b1fe68cd9be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43280), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tagged_docs = [to_tagged_doc(doc) for doc in tqdm_notebook(train_documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61585092a4f64135a3d02aae0dafa544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18549), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_tagged_docs = [to_tagged_doc(doc) for doc in tqdm_notebook(test_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потім тренуємо PV-DBOW модель. Розмір вектору документа 300, як і в моделі з embeddins, яку ми використовували в бейзлайні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "dbow_model = Doc2Vec(dm=0, vector_size=300, min_count=5, window=10, workers=6, epochs=120)\n",
    "\n",
    "dbow_model.build_vocab(train_tagged_docs + test_tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 30s, sys: 47.8 s, total: 13min 17s\n",
      "Wall time: 4min 57s\n"
     ]
    }
   ],
   "source": [
    "%time dbow_model.train(train_tagged_docs, total_examples=dbow_model.corpus_count, epochs=dbow_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Збираємо вектори документів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41eb60e568004c8eb1db5e47aeea0e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43280), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_doc_vecs = np.array([dbow_model.infer_vector(doc.words) for doc in tqdm_notebook(train_tagged_docs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe9f692243f4b8cb558f830cce0b4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=18549), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_doc_vecs = np.array([dbow_model.infer_vector(doc.words) for doc in tqdm_notebook(test_tagged_docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Знову намагаємося застосувати kNN та логістичну регресію на отриманних векторах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KnnModel(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 13s, sys: 51 ms, total: 12min 13s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%time knn2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.59      0.60       335\n",
      "           1       0.83      0.83      0.83        41\n",
      "           2       0.57      0.52      0.54        79\n",
      "           3       0.48      0.54      0.51        39\n",
      "           4       0.57      0.55      0.56        22\n",
      "           5       0.55      0.77      0.64       106\n",
      "           6       0.43      0.32      0.36       142\n",
      "           7       0.53      0.52      0.52        62\n",
      "           8       0.37      0.93      0.53        30\n",
      "           9       0.51      0.82      0.63        22\n",
      "          10       0.78      0.85      0.81       144\n",
      "          11       0.67      0.21      0.32        29\n",
      "          12       0.94      0.73      0.82        22\n",
      "          13       0.66      0.51      0.57        45\n",
      "          14       0.32      0.17      0.22        84\n",
      "          15       0.44      0.44      0.44        16\n",
      "          16       0.50      0.63      0.56       122\n",
      "          17       0.40      0.25      0.31        16\n",
      "          18       0.39      0.36      0.37        53\n",
      "          19       0.31      0.21      0.25        19\n",
      "          20       0.45      0.31      0.37        16\n",
      "          21       0.62      0.33      0.43        24\n",
      "          22       0.66      0.53      0.59       194\n",
      "          23       0.69      0.84      0.76        51\n",
      "          24       0.58      0.47      0.52        47\n",
      "          25       0.63      0.61      0.62        28\n",
      "          26       0.41      0.56      0.47        25\n",
      "          27       0.62      0.81      0.71       943\n",
      "          28       0.60      0.55      0.57        56\n",
      "          29       0.47      0.38      0.42        63\n",
      "          30       0.22      0.20      0.21        44\n",
      "          31       0.28      0.20      0.23        25\n",
      "          32       0.77      0.71      0.74        95\n",
      "          33       0.75      0.78      0.76       129\n",
      "          34       0.40      0.60      0.48        45\n",
      "          35       0.41      0.26      0.32        35\n",
      "          36       0.39      0.34      0.36        50\n",
      "          37       0.41      0.23      0.29        40\n",
      "          38       0.57      0.67      0.61        39\n",
      "          39       0.53      0.52      0.53        44\n",
      "          40       0.47      0.74      0.58       115\n",
      "          41       0.58      0.59      0.59       143\n",
      "          42       0.66      0.71      0.68        62\n",
      "          43       0.54      0.61      0.57        41\n",
      "          44       0.28      0.38      0.32        29\n",
      "          45       0.63      0.34      0.44        71\n",
      "          46       0.73      0.71      0.72        56\n",
      "          47       0.26      0.33      0.29        21\n",
      "          48       0.17      0.19      0.18        16\n",
      "          49       0.59      0.68      0.63        19\n",
      "          50       0.67      0.70      0.69        44\n",
      "          51       0.43      0.45      0.44        20\n",
      "          52       0.50      0.26      0.34        27\n",
      "          53       0.48      0.57      0.52        53\n",
      "          54       0.35      0.44      0.39        18\n",
      "          55       0.42      0.19      0.26        26\n",
      "          56       0.82      0.82      0.82        28\n",
      "          57       0.58      0.56      0.57       108\n",
      "          58       0.67      0.79      0.73       366\n",
      "          59       0.39      0.39      0.39        74\n",
      "          60       0.52      0.35      0.42        34\n",
      "          61       0.43      0.38      0.40        24\n",
      "          62       0.52      0.63      0.57        95\n",
      "          63       0.21      0.39      0.27        18\n",
      "          64       0.54      0.86      0.67        37\n",
      "          65       0.54      0.47      0.50        15\n",
      "          66       0.47      0.24      0.32        33\n",
      "          67       0.57      0.65      0.61       602\n",
      "          68       0.61      0.54      0.58       190\n",
      "          69       0.38      0.34      0.36       105\n",
      "          70       0.74      0.78      0.76        36\n",
      "          71       0.73      0.40      0.51        48\n",
      "          72       0.29      0.23      0.26        22\n",
      "          73       0.35      0.52      0.42        33\n",
      "          74       0.76      0.92      0.83        85\n",
      "          75       0.54      0.73      0.62        30\n",
      "          76       0.60      0.56      0.58        50\n",
      "          77       0.53      0.86      0.65        43\n",
      "          78       0.63      0.57      0.60       104\n",
      "          79       0.41      0.43      0.42       391\n",
      "          80       0.25      0.12      0.16        17\n",
      "          81       0.62      0.89      0.74        28\n",
      "          82       0.24      0.40      0.30        20\n",
      "          83       0.16      0.17      0.16        24\n",
      "          84       0.48      0.28      0.36        53\n",
      "          85       0.65      0.48      0.55        23\n",
      "          86       0.49      0.34      0.40        74\n",
      "          87       0.57      0.35      0.44        34\n",
      "          88       0.54      0.40      0.46       297\n",
      "          89       0.67      0.78      0.72        40\n",
      "          90       0.60      0.19      0.29        16\n",
      "          91       0.44      0.28      0.34        39\n",
      "          92       0.42      0.31      0.36        26\n",
      "          93       0.63      1.00      0.77        22\n",
      "          94       0.16      0.35      0.22        20\n",
      "          95       0.15      0.19      0.17        42\n",
      "          96       0.50      0.43      0.46        21\n",
      "          97       0.49      0.79      0.60        24\n",
      "          98       0.38      0.36      0.37        25\n",
      "          99       0.41      0.44      0.42        34\n",
      "         100       0.42      0.31      0.36        55\n",
      "         101       0.69      0.53      0.60       329\n",
      "         102       0.87      0.89      0.88        75\n",
      "         103       0.52      0.63      0.57        19\n",
      "         104       0.74      0.89      0.81        38\n",
      "         105       0.34      0.26      0.30        87\n",
      "         106       0.38      0.32      0.34        47\n",
      "         107       0.58      0.44      0.50        32\n",
      "         108       0.64      0.67      0.65        21\n",
      "         109       0.47      0.52      0.49       155\n",
      "         110       0.38      0.23      0.28        66\n",
      "         111       0.58      0.70      0.64        20\n",
      "         112       0.66      0.61      0.64        80\n",
      "         113       0.66      0.48      0.56        56\n",
      "         114       0.56      0.57      0.56       137\n",
      "         115       0.20      0.18      0.19        17\n",
      "         116       0.64      0.65      0.64       129\n",
      "         117       0.29      0.21      0.24        78\n",
      "         118       0.75      0.31      0.44        39\n",
      "         119       0.37      0.40      0.38        47\n",
      "         120       0.28      0.57      0.37        14\n",
      "         121       0.56      0.62      0.59       377\n",
      "         122       0.55      0.68      0.61        38\n",
      "         123       0.39      0.45      0.42        78\n",
      "         124       0.24      0.16      0.19        25\n",
      "         125       0.40      0.25      0.31        73\n",
      "         126       0.58      0.75      0.65        20\n",
      "         127       0.74      0.74      0.74       660\n",
      "         128       0.35      0.32      0.33        82\n",
      "         129       0.53      0.40      0.46        20\n",
      "         130       0.88      0.39      0.54        18\n",
      "         131       0.40      0.27      0.32       137\n",
      "         132       0.28      0.42      0.33        43\n",
      "         133       0.53      0.36      0.43        55\n",
      "         134       0.55      0.55      0.55        64\n",
      "         135       0.39      0.32      0.35        57\n",
      "         136       0.71      0.62      0.67        96\n",
      "         137       0.61      0.74      0.67        61\n",
      "         138       0.79      0.87      0.83      1969\n",
      "         139       0.51      0.65      0.57        54\n",
      "         140       0.57      0.25      0.35        16\n",
      "         141       0.37      0.56      0.44        25\n",
      "         142       0.43      0.35      0.39        17\n",
      "         143       0.59      0.38      0.46       298\n",
      "         144       0.22      0.27      0.24        41\n",
      "         145       0.43      0.70      0.53        83\n",
      "         146       0.59      0.52      0.55        25\n",
      "         147       0.50      0.40      0.44        25\n",
      "         148       0.44      0.38      0.41       209\n",
      "         149       0.44      0.27      0.33        15\n",
      "         150       0.70      0.31      0.43        61\n",
      "         151       0.54      0.65      0.59        20\n",
      "         152       0.55      0.55      0.55        20\n",
      "         153       0.38      0.47      0.42        34\n",
      "         154       0.70      0.74      0.72       144\n",
      "         155       0.68      0.49      0.57       387\n",
      "         156       0.30      0.29      0.29        56\n",
      "         157       0.35      0.33      0.34        24\n",
      "         158       0.45      0.47      0.46        51\n",
      "         159       0.65      0.48      0.55        23\n",
      "         160       0.58      0.25      0.35        44\n",
      "         161       0.58      0.55      0.57       240\n",
      "         162       0.75      0.33      0.46        27\n",
      "         163       0.69      0.47      0.56        19\n",
      "         164       0.52      0.44      0.48       161\n",
      "         165       0.52      0.47      0.49        66\n",
      "         166       0.41      0.37      0.39        70\n",
      "         167       0.70      0.79      0.74        61\n",
      "         168       0.24      0.24      0.24        92\n",
      "         169       0.40      0.85      0.55        34\n",
      "         170       0.43      0.54      0.48        37\n",
      "         171       0.44      0.49      0.46       223\n",
      "         172       0.18      0.08      0.11        24\n",
      "         173       0.66      0.74      0.70       248\n",
      "         174       0.59      0.50      0.54       191\n",
      "         175       0.35      0.17      0.23        53\n",
      "         176       0.58      0.58      0.58       236\n",
      "         177       0.31      0.28      0.30        39\n",
      "         178       0.76      0.64      0.70       201\n",
      "         179       0.35      0.19      0.25        31\n",
      "         180       0.64      0.70      0.67      1088\n",
      "         181       0.53      0.42      0.47       185\n",
      "         182       0.36      0.46      0.41        28\n",
      "         183       0.67      0.52      0.58       359\n",
      "         184       0.50      0.79      0.61        19\n",
      "         185       0.43      0.17      0.24        35\n",
      "         186       0.49      0.44      0.46        39\n",
      "         187       0.56      0.36      0.44        25\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     18549\n",
      "   macro avg       0.51      0.49      0.49     18549\n",
      "weighted avg       0.59      0.59      0.58     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn2.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogregModel(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 334 ms, sys: 756 ms, total: 1.09 s\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%time logreg2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.50      0.56       335\n",
      "           1       1.00      0.63      0.78        41\n",
      "           2       0.51      0.48      0.50        79\n",
      "           3       0.56      0.46      0.51        39\n",
      "           4       0.90      0.41      0.56        22\n",
      "           5       0.64      0.65      0.64       106\n",
      "           6       0.39      0.37      0.38       142\n",
      "           7       0.76      0.55      0.64        62\n",
      "           8       0.65      0.67      0.66        30\n",
      "           9       0.61      0.50      0.55        22\n",
      "          10       0.76      0.86      0.81       144\n",
      "          11       0.71      0.17      0.28        29\n",
      "          12       1.00      0.41      0.58        22\n",
      "          13       0.59      0.36      0.44        45\n",
      "          14       0.30      0.24      0.26        84\n",
      "          15       0.62      0.50      0.55        16\n",
      "          16       0.50      0.65      0.57       122\n",
      "          17       0.71      0.31      0.43        16\n",
      "          18       0.35      0.36      0.36        53\n",
      "          19       0.57      0.21      0.31        19\n",
      "          20       0.80      0.25      0.38        16\n",
      "          21       0.44      0.17      0.24        24\n",
      "          22       0.62      0.57      0.59       194\n",
      "          23       0.83      0.84      0.83        51\n",
      "          24       0.67      0.43      0.52        47\n",
      "          25       0.68      0.54      0.60        28\n",
      "          26       0.61      0.44      0.51        25\n",
      "          27       0.68      0.71      0.70       943\n",
      "          28       0.66      0.66      0.66        56\n",
      "          29       0.38      0.41      0.39        63\n",
      "          30       0.20      0.23      0.21        44\n",
      "          31       0.18      0.08      0.11        25\n",
      "          32       0.77      0.72      0.74        95\n",
      "          33       0.79      0.74      0.77       129\n",
      "          34       0.52      0.62      0.57        45\n",
      "          35       0.40      0.17      0.24        35\n",
      "          36       0.31      0.28      0.29        50\n",
      "          37       0.41      0.23      0.29        40\n",
      "          38       0.67      0.77      0.71        39\n",
      "          39       0.59      0.50      0.54        44\n",
      "          40       0.61      0.68      0.64       115\n",
      "          41       0.56      0.56      0.56       143\n",
      "          42       0.61      0.68      0.64        62\n",
      "          43       0.59      0.73      0.65        41\n",
      "          44       0.37      0.24      0.29        29\n",
      "          45       0.56      0.41      0.47        71\n",
      "          46       0.72      0.73      0.73        56\n",
      "          47       0.38      0.24      0.29        21\n",
      "          48       0.12      0.06      0.08        16\n",
      "          49       0.90      0.47      0.62        19\n",
      "          50       0.67      0.59      0.63        44\n",
      "          51       0.58      0.35      0.44        20\n",
      "          52       0.33      0.15      0.21        27\n",
      "          53       0.61      0.62      0.62        53\n",
      "          54       0.27      0.39      0.32        18\n",
      "          55       0.33      0.23      0.27        26\n",
      "          56       0.93      0.50      0.65        28\n",
      "          57       0.59      0.59      0.59       108\n",
      "          58       0.71      0.74      0.72       366\n",
      "          59       0.47      0.38      0.42        74\n",
      "          60       0.82      0.26      0.40        34\n",
      "          61       0.38      0.33      0.36        24\n",
      "          62       0.47      0.53      0.50        95\n",
      "          63       0.36      0.22      0.28        18\n",
      "          64       0.69      0.84      0.76        37\n",
      "          65       1.00      0.33      0.50        15\n",
      "          66       0.60      0.18      0.28        33\n",
      "          67       0.64      0.70      0.67       602\n",
      "          68       0.67      0.67      0.67       190\n",
      "          69       0.41      0.40      0.41       105\n",
      "          70       0.84      0.75      0.79        36\n",
      "          71       0.62      0.38      0.47        48\n",
      "          72       0.50      0.23      0.31        22\n",
      "          73       0.56      0.45      0.50        33\n",
      "          74       0.82      0.92      0.87        85\n",
      "          75       0.71      0.67      0.69        30\n",
      "          76       0.59      0.66      0.62        50\n",
      "          77       0.73      0.63      0.68        43\n",
      "          78       0.62      0.67      0.65       104\n",
      "          79       0.41      0.42      0.42       391\n",
      "          80       0.25      0.18      0.21        17\n",
      "          81       0.83      0.54      0.65        28\n",
      "          82       0.36      0.40      0.38        20\n",
      "          83       0.31      0.21      0.25        24\n",
      "          84       0.53      0.34      0.41        53\n",
      "          85       0.64      0.39      0.49        23\n",
      "          86       0.54      0.46      0.50        74\n",
      "          87       0.56      0.29      0.38        34\n",
      "          88       0.49      0.52      0.50       297\n",
      "          89       0.64      0.72      0.68        40\n",
      "          90       0.67      0.12      0.21        16\n",
      "          91       0.36      0.31      0.33        39\n",
      "          92       0.47      0.31      0.37        26\n",
      "          93       0.95      0.91      0.93        22\n",
      "          94       0.29      0.30      0.29        20\n",
      "          95       0.15      0.21      0.17        42\n",
      "          96       0.89      0.38      0.53        21\n",
      "          97       0.48      0.62      0.55        24\n",
      "          98       0.80      0.32      0.46        25\n",
      "          99       0.48      0.47      0.48        34\n",
      "         100       0.50      0.45      0.48        55\n",
      "         101       0.50      0.63      0.56       329\n",
      "         102       0.86      0.88      0.87        75\n",
      "         103       0.69      0.58      0.63        19\n",
      "         104       0.93      0.74      0.82        38\n",
      "         105       0.36      0.26      0.30        87\n",
      "         106       0.26      0.21      0.23        47\n",
      "         107       0.73      0.50      0.59        32\n",
      "         108       0.85      0.52      0.65        21\n",
      "         109       0.54      0.57      0.56       155\n",
      "         110       0.42      0.26      0.32        66\n",
      "         111       0.93      0.65      0.76        20\n",
      "         112       0.63      0.74      0.68        80\n",
      "         113       0.66      0.45      0.53        56\n",
      "         114       0.54      0.54      0.54       137\n",
      "         115       0.20      0.12      0.15        17\n",
      "         116       0.72      0.73      0.73       129\n",
      "         117       0.36      0.26      0.30        78\n",
      "         118       0.53      0.26      0.34        39\n",
      "         119       0.54      0.47      0.50        47\n",
      "         120       0.86      0.43      0.57        14\n",
      "         121       0.58      0.66      0.62       377\n",
      "         122       0.82      0.61      0.70        38\n",
      "         123       0.39      0.44      0.41        78\n",
      "         124       0.42      0.20      0.27        25\n",
      "         125       0.44      0.29      0.35        73\n",
      "         126       0.94      0.75      0.83        20\n",
      "         127       0.64      0.80      0.71       660\n",
      "         128       0.40      0.29      0.34        82\n",
      "         129       0.75      0.30      0.43        20\n",
      "         130       0.60      0.17      0.26        18\n",
      "         131       0.27      0.33      0.30       137\n",
      "         132       0.26      0.26      0.26        43\n",
      "         133       0.53      0.35      0.42        55\n",
      "         134       0.66      0.61      0.63        64\n",
      "         135       0.47      0.44      0.45        57\n",
      "         136       0.75      0.68      0.71        96\n",
      "         137       0.72      0.75      0.74        61\n",
      "         138       0.76      0.86      0.81      1969\n",
      "         139       0.56      0.56      0.56        54\n",
      "         140       0.57      0.25      0.35        16\n",
      "         141       0.58      0.44      0.50        25\n",
      "         142       0.67      0.24      0.35        17\n",
      "         143       0.37      0.43      0.39       298\n",
      "         144       0.25      0.27      0.26        41\n",
      "         145       0.59      0.70      0.64        83\n",
      "         146       0.72      0.52      0.60        25\n",
      "         147       0.33      0.28      0.30        25\n",
      "         148       0.44      0.54      0.48       209\n",
      "         149       0.71      0.33      0.45        15\n",
      "         150       0.61      0.38      0.46        61\n",
      "         151       0.55      0.30      0.39        20\n",
      "         152       0.67      0.50      0.57        20\n",
      "         153       0.31      0.32      0.31        34\n",
      "         154       0.76      0.74      0.75       144\n",
      "         155       0.56      0.56      0.56       387\n",
      "         156       0.40      0.41      0.40        56\n",
      "         157       0.41      0.29      0.34        24\n",
      "         158       0.61      0.49      0.54        51\n",
      "         159       0.56      0.43      0.49        23\n",
      "         160       0.65      0.30      0.41        44\n",
      "         161       0.57      0.58      0.58       240\n",
      "         162       1.00      0.26      0.41        27\n",
      "         163       0.78      0.37      0.50        19\n",
      "         164       0.57      0.53      0.55       161\n",
      "         165       0.61      0.58      0.59        66\n",
      "         166       0.50      0.43      0.46        70\n",
      "         167       0.88      0.69      0.77        61\n",
      "         168       0.29      0.29      0.29        92\n",
      "         169       0.53      0.68      0.60        34\n",
      "         170       0.67      0.49      0.56        37\n",
      "         171       0.42      0.41      0.42       223\n",
      "         172       0.11      0.12      0.12        24\n",
      "         173       0.69      0.72      0.71       248\n",
      "         174       0.49      0.51      0.50       191\n",
      "         175       0.32      0.19      0.24        53\n",
      "         176       0.60      0.54      0.57       236\n",
      "         177       0.42      0.36      0.39        39\n",
      "         178       0.77      0.70      0.73       201\n",
      "         179       0.28      0.29      0.29        31\n",
      "         180       0.60      0.73      0.66      1088\n",
      "         181       0.54      0.43      0.48       185\n",
      "         182       0.36      0.46      0.41        28\n",
      "         183       0.56      0.63      0.59       359\n",
      "         184       0.69      0.58      0.63        19\n",
      "         185       0.62      0.14      0.23        35\n",
      "         186       0.50      0.36      0.42        39\n",
      "         187       0.67      0.40      0.50        25\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     18549\n",
      "   macro avg       0.57      0.46      0.50     18549\n",
      "weighted avg       0.60      0.60      0.59     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg2.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conslusion\n",
    "\n",
    "Бачимо що вдалося покращити якість у порівнянні з бейзланойм більш ніж на 10% згідно F1. Логістична регресія в порівнянні з kNN у всіх випадках працювала краще. Вектори документів також дали покращення у всіх випадках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "class NnModel(Model):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels):\n",
    "        super().__init__(train_vectors, \n",
    "                         to_categorical(train_labels), \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "    \n",
    "    def train(self, epochs = 10):\n",
    "        self.model.fit(self.train_vectors, self.train_labels, \n",
    "                       epochs = epochs, batch_size=128, \n",
    "                       verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "        self.topics_predicted = np.argmax(self.model.predict(self.test_vectors), axis=-1)\n",
    "    \n",
    "\n",
    "class FeedForwardNN(NnModel):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels,\n",
    "                 input_size, hidden_size):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)               \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(hidden_size, activation='relu', input_shape=(input_size,)))        \n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 500,924\n",
      "Trainable params: 500,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_model = FeedForwardNN(train_doc_vecs, train_topic_labels, test_doc_vecs, test_topic_labels, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9890a08650474546b83af1d4be3ca3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=10, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 5', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 6', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 7', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 8', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 9', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ff_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.62      0.61       335\n",
      "           1       0.78      0.76      0.77        41\n",
      "           2       0.56      0.54      0.55        79\n",
      "           3       0.53      0.46      0.49        39\n",
      "           4       0.90      0.41      0.56        22\n",
      "           5       0.68      0.67      0.67       106\n",
      "           6       0.44      0.34      0.38       142\n",
      "           7       0.67      0.58      0.62        62\n",
      "           8       0.70      0.70      0.70        30\n",
      "           9       0.62      0.59      0.60        22\n",
      "          10       0.77      0.88      0.82       144\n",
      "          11       0.83      0.17      0.29        29\n",
      "          12       0.93      0.64      0.76        22\n",
      "          13       0.69      0.60      0.64        45\n",
      "          14       0.33      0.23      0.27        84\n",
      "          15       0.67      0.25      0.36        16\n",
      "          16       0.53      0.64      0.58       122\n",
      "          17       0.75      0.38      0.50        16\n",
      "          18       0.43      0.40      0.41        53\n",
      "          19       0.45      0.26      0.33        19\n",
      "          20       0.67      0.25      0.36        16\n",
      "          21       0.73      0.46      0.56        24\n",
      "          22       0.63      0.61      0.62       194\n",
      "          23       0.88      0.88      0.88        51\n",
      "          24       0.59      0.62      0.60        47\n",
      "          25       0.74      0.61      0.67        28\n",
      "          26       0.50      0.40      0.44        25\n",
      "          27       0.71      0.78      0.74       943\n",
      "          28       0.65      0.66      0.65        56\n",
      "          29       0.40      0.46      0.43        63\n",
      "          30       0.39      0.27      0.32        44\n",
      "          31       0.25      0.08      0.12        25\n",
      "          32       0.76      0.77      0.76        95\n",
      "          33       0.81      0.78      0.80       129\n",
      "          34       0.54      0.60      0.57        45\n",
      "          35       0.36      0.37      0.37        35\n",
      "          36       0.50      0.30      0.37        50\n",
      "          37       0.35      0.17      0.23        40\n",
      "          38       0.71      0.62      0.66        39\n",
      "          39       0.67      0.66      0.67        44\n",
      "          40       0.65      0.67      0.66       115\n",
      "          41       0.66      0.50      0.57       143\n",
      "          42       0.52      0.68      0.59        62\n",
      "          43       0.64      0.71      0.67        41\n",
      "          44       0.40      0.21      0.27        29\n",
      "          45       0.69      0.38      0.49        71\n",
      "          46       0.93      0.71      0.81        56\n",
      "          47       0.57      0.19      0.29        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.55      0.63      0.59        19\n",
      "          50       0.76      0.64      0.69        44\n",
      "          51       0.58      0.35      0.44        20\n",
      "          52       0.60      0.22      0.32        27\n",
      "          53       0.62      0.64      0.63        53\n",
      "          54       0.50      0.17      0.25        18\n",
      "          55       0.83      0.19      0.31        26\n",
      "          56       0.91      0.71      0.80        28\n",
      "          57       0.56      0.55      0.55       108\n",
      "          58       0.71      0.78      0.74       366\n",
      "          59       0.45      0.53      0.49        74\n",
      "          60       0.92      0.35      0.51        34\n",
      "          61       0.60      0.38      0.46        24\n",
      "          62       0.47      0.60      0.53        95\n",
      "          63       0.67      0.22      0.33        18\n",
      "          64       0.73      0.86      0.79        37\n",
      "          65       0.45      0.33      0.38        15\n",
      "          66       0.62      0.24      0.35        33\n",
      "          67       0.60      0.75      0.67       602\n",
      "          68       0.62      0.77      0.69       190\n",
      "          69       0.42      0.47      0.44       105\n",
      "          70       0.82      0.75      0.78        36\n",
      "          71       0.74      0.42      0.53        48\n",
      "          72       0.38      0.27      0.32        22\n",
      "          73       0.53      0.55      0.54        33\n",
      "          74       0.82      0.95      0.88        85\n",
      "          75       0.71      0.57      0.63        30\n",
      "          76       0.70      0.62      0.66        50\n",
      "          77       0.68      0.63      0.65        43\n",
      "          78       0.59      0.68      0.63       104\n",
      "          79       0.45      0.49      0.47       391\n",
      "          80       1.00      0.18      0.30        17\n",
      "          81       0.75      0.75      0.75        28\n",
      "          82       0.41      0.45      0.43        20\n",
      "          83       0.45      0.21      0.29        24\n",
      "          84       0.44      0.34      0.38        53\n",
      "          85       0.78      0.30      0.44        23\n",
      "          86       0.52      0.55      0.54        74\n",
      "          87       0.47      0.26      0.34        34\n",
      "          88       0.55      0.56      0.55       297\n",
      "          89       0.64      0.80      0.71        40\n",
      "          90       0.75      0.19      0.30        16\n",
      "          91       0.34      0.33      0.34        39\n",
      "          92       0.42      0.38      0.40        26\n",
      "          93       0.94      0.77      0.85        22\n",
      "          94       0.27      0.20      0.23        20\n",
      "          95       0.24      0.19      0.21        42\n",
      "          96       0.73      0.52      0.61        21\n",
      "          97       0.68      0.62      0.65        24\n",
      "          98       0.48      0.44      0.46        25\n",
      "          99       0.59      0.38      0.46        34\n",
      "         100       0.40      0.53      0.46        55\n",
      "         101       0.54      0.67      0.59       329\n",
      "         102       0.93      0.85      0.89        75\n",
      "         103       0.83      0.53      0.65        19\n",
      "         104       0.91      0.76      0.83        38\n",
      "         105       0.41      0.25      0.31        87\n",
      "         106       0.27      0.17      0.21        47\n",
      "         107       0.67      0.44      0.53        32\n",
      "         108       0.68      0.62      0.65        21\n",
      "         109       0.49      0.52      0.51       155\n",
      "         110       0.50      0.29      0.37        66\n",
      "         111       0.88      0.75      0.81        20\n",
      "         112       0.67      0.78      0.72        80\n",
      "         113       0.72      0.50      0.59        56\n",
      "         114       0.55      0.53      0.54       137\n",
      "         115       0.30      0.18      0.22        17\n",
      "         116       0.74      0.74      0.74       129\n",
      "         117       0.66      0.24      0.36        78\n",
      "         118       0.50      0.21      0.29        39\n",
      "         119       0.66      0.45      0.53        47\n",
      "         120       0.86      0.43      0.57        14\n",
      "         121       0.62      0.59      0.60       377\n",
      "         122       0.71      0.66      0.68        38\n",
      "         123       0.47      0.47      0.47        78\n",
      "         124       0.62      0.20      0.30        25\n",
      "         125       0.44      0.33      0.38        73\n",
      "         126       1.00      0.70      0.82        20\n",
      "         127       0.68      0.86      0.76       660\n",
      "         128       0.50      0.29      0.37        82\n",
      "         129       0.70      0.35      0.47        20\n",
      "         130       1.00      0.11      0.20        18\n",
      "         131       0.33      0.34      0.34       137\n",
      "         132       0.35      0.28      0.31        43\n",
      "         133       0.46      0.33      0.38        55\n",
      "         134       0.70      0.55      0.61        64\n",
      "         135       0.52      0.47      0.50        57\n",
      "         136       0.71      0.73      0.72        96\n",
      "         137       0.62      0.72      0.67        61\n",
      "         138       0.79      0.87      0.83      1969\n",
      "         139       0.67      0.52      0.58        54\n",
      "         140       1.00      0.19      0.32        16\n",
      "         141       0.57      0.32      0.41        25\n",
      "         142       0.67      0.24      0.35        17\n",
      "         143       0.36      0.44      0.40       298\n",
      "         144       0.36      0.20      0.25        41\n",
      "         145       0.61      0.65      0.63        83\n",
      "         146       0.68      0.52      0.59        25\n",
      "         147       0.36      0.36      0.36        25\n",
      "         148       0.50      0.55      0.52       209\n",
      "         149       0.71      0.33      0.45        15\n",
      "         150       0.63      0.44      0.52        61\n",
      "         151       0.54      0.35      0.42        20\n",
      "         152       0.68      0.65      0.67        20\n",
      "         153       0.32      0.24      0.27        34\n",
      "         154       0.79      0.79      0.79       144\n",
      "         155       0.62      0.57      0.60       387\n",
      "         156       0.52      0.48      0.50        56\n",
      "         157       0.57      0.17      0.26        24\n",
      "         158       0.57      0.31      0.41        51\n",
      "         159       0.82      0.39      0.53        23\n",
      "         160       0.71      0.27      0.39        44\n",
      "         161       0.62      0.55      0.58       240\n",
      "         162       0.70      0.26      0.38        27\n",
      "         163       0.71      0.53      0.61        19\n",
      "         164       0.52      0.56      0.54       161\n",
      "         165       0.68      0.55      0.61        66\n",
      "         166       0.52      0.49      0.50        70\n",
      "         167       0.74      0.79      0.76        61\n",
      "         168       0.31      0.30      0.31        92\n",
      "         169       0.49      0.71      0.58        34\n",
      "         170       0.67      0.54      0.60        37\n",
      "         171       0.40      0.50      0.45       223\n",
      "         172       0.15      0.12      0.14        24\n",
      "         173       0.69      0.76      0.72       248\n",
      "         174       0.59      0.64      0.61       191\n",
      "         175       0.37      0.30      0.33        53\n",
      "         176       0.69      0.52      0.59       236\n",
      "         177       0.41      0.44      0.43        39\n",
      "         178       0.78      0.70      0.74       201\n",
      "         179       0.62      0.26      0.36        31\n",
      "         180       0.65      0.75      0.69      1088\n",
      "         181       0.58      0.41      0.48       185\n",
      "         182       0.37      0.36      0.36        28\n",
      "         183       0.56      0.71      0.62       359\n",
      "         184       0.47      0.47      0.47        19\n",
      "         185       0.36      0.14      0.20        35\n",
      "         186       0.56      0.46      0.51        39\n",
      "         187       0.59      0.52      0.55        25\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     18549\n",
      "   macro avg       0.60      0.48      0.52     18549\n",
      "weighted avg       0.62      0.62      0.61     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 1024)              308224    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 188)               192700    \n",
      "=================================================================\n",
      "Total params: 500,924\n",
      "Trainable params: 500,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model = FeedForwardNN(train_doc_sum_vecs, train_topic_labels, \n",
    "                             test_doc_sum_vecs, test_topic_labels, 300, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb5c2010e214831b4fe9b309f977ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=5, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 3', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 4', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ff_sum_model.train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.59       335\n",
      "           1       0.00      0.00      0.00        41\n",
      "           2       0.53      0.57      0.55        79\n",
      "           3       0.00      0.00      0.00        39\n",
      "           4       0.00      0.00      0.00        22\n",
      "           5       0.58      0.50      0.54       106\n",
      "           6       0.27      0.32      0.30       142\n",
      "           7       0.45      0.55      0.49        62\n",
      "           8       0.74      0.47      0.57        30\n",
      "           9       0.56      0.41      0.47        22\n",
      "          10       0.75      0.82      0.78       144\n",
      "          11       0.23      0.21      0.22        29\n",
      "          12       1.00      0.64      0.78        22\n",
      "          13       0.39      0.42      0.40        45\n",
      "          14       0.19      0.17      0.18        84\n",
      "          15       0.56      0.31      0.40        16\n",
      "          16       0.63      0.47      0.54       122\n",
      "          17       0.55      0.38      0.44        16\n",
      "          18       0.32      0.23      0.26        53\n",
      "          19       0.50      0.32      0.39        19\n",
      "          20       0.00      0.00      0.00        16\n",
      "          21       0.11      0.12      0.12        24\n",
      "          22       0.59      0.65      0.62       194\n",
      "          23       0.87      0.80      0.84        51\n",
      "          24       0.55      0.62      0.58        47\n",
      "          25       0.47      0.29      0.36        28\n",
      "          26       0.48      0.44      0.46        25\n",
      "          27       0.69      0.81      0.74       943\n",
      "          28       0.42      0.45      0.43        56\n",
      "          29       0.41      0.51      0.45        63\n",
      "          30       0.22      0.25      0.23        44\n",
      "          31       0.11      0.04      0.06        25\n",
      "          32       0.69      0.66      0.68        95\n",
      "          33       0.72      0.74      0.73       129\n",
      "          34       0.60      0.53      0.56        45\n",
      "          35       0.30      0.34      0.32        35\n",
      "          36       0.36      0.20      0.26        50\n",
      "          37       0.12      0.15      0.13        40\n",
      "          38       0.72      0.59      0.65        39\n",
      "          39       0.46      0.36      0.41        44\n",
      "          40       0.62      0.60      0.61       115\n",
      "          41       0.47      0.58      0.52       143\n",
      "          42       0.65      0.71      0.68        62\n",
      "          43       0.46      0.54      0.49        41\n",
      "          44       0.26      0.21      0.23        29\n",
      "          45       0.50      0.54      0.52        71\n",
      "          46       0.68      0.57      0.62        56\n",
      "          47       0.36      0.24      0.29        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.57      0.42      0.48        19\n",
      "          50       0.45      0.32      0.37        44\n",
      "          51       0.52      0.60      0.56        20\n",
      "          52       0.36      0.30      0.33        27\n",
      "          53       0.60      0.53      0.56        53\n",
      "          54       0.30      0.17      0.21        18\n",
      "          55       0.00      0.00      0.00        26\n",
      "          56       0.79      0.54      0.64        28\n",
      "          57       0.69      0.55      0.61       108\n",
      "          58       0.67      0.73      0.70       366\n",
      "          59       0.21      0.43      0.28        74\n",
      "          60       0.00      0.00      0.00        34\n",
      "          61       0.00      0.00      0.00        24\n",
      "          62       0.46      0.34      0.39        95\n",
      "          63       0.22      0.22      0.22        18\n",
      "          64       0.73      0.73      0.73        37\n",
      "          65       0.50      0.07      0.12        15\n",
      "          66       0.00      0.00      0.00        33\n",
      "          67       0.60      0.68      0.63       602\n",
      "          68       0.62      0.75      0.68       190\n",
      "          69       0.37      0.32      0.35       105\n",
      "          70       0.71      0.61      0.66        36\n",
      "          71       0.28      0.33      0.30        48\n",
      "          72       0.00      0.00      0.00        22\n",
      "          73       0.65      0.33      0.44        33\n",
      "          74       0.75      0.75      0.75        85\n",
      "          75       0.59      0.57      0.58        30\n",
      "          76       0.79      0.68      0.73        50\n",
      "          77       0.48      0.30      0.37        43\n",
      "          78       0.48      0.62      0.54       104\n",
      "          79       0.44      0.63      0.52       391\n",
      "          80       0.50      0.12      0.19        17\n",
      "          81       0.60      0.64      0.62        28\n",
      "          82       0.00      0.00      0.00        20\n",
      "          83       0.14      0.04      0.06        24\n",
      "          84       0.36      0.47      0.41        53\n",
      "          85       0.00      0.00      0.00        23\n",
      "          86       0.43      0.30      0.35        74\n",
      "          87       0.45      0.26      0.33        34\n",
      "          88       0.55      0.49      0.52       297\n",
      "          89       0.64      0.68      0.66        40\n",
      "          90       0.00      0.00      0.00        16\n",
      "          91       0.46      0.28      0.35        39\n",
      "          92       0.31      0.50      0.38        26\n",
      "          93       0.86      0.82      0.84        22\n",
      "          94       0.27      0.15      0.19        20\n",
      "          95       0.29      0.24      0.26        42\n",
      "          96       0.60      0.43      0.50        21\n",
      "          97       0.78      0.58      0.67        24\n",
      "          98       0.31      0.20      0.24        25\n",
      "          99       0.52      0.32      0.40        34\n",
      "         100       0.38      0.47      0.42        55\n",
      "         101       0.51      0.68      0.58       329\n",
      "         102       0.74      0.60      0.66        75\n",
      "         103       0.44      0.42      0.43        19\n",
      "         104       0.87      0.71      0.78        38\n",
      "         105       0.35      0.32      0.34        87\n",
      "         106       0.38      0.28      0.32        47\n",
      "         107       0.00      0.00      0.00        32\n",
      "         108       0.00      0.00      0.00        21\n",
      "         109       0.34      0.39      0.36       155\n",
      "         110       0.35      0.44      0.39        66\n",
      "         111       0.00      0.00      0.00        20\n",
      "         112       0.39      0.26      0.31        80\n",
      "         113       0.75      0.43      0.55        56\n",
      "         114       0.49      0.68      0.57       137\n",
      "         115       0.00      0.00      0.00        17\n",
      "         116       0.72      0.71      0.72       129\n",
      "         117       0.18      0.18      0.18        78\n",
      "         118       0.36      0.31      0.33        39\n",
      "         119       0.66      0.40      0.50        47\n",
      "         120       0.67      0.14      0.24        14\n",
      "         121       0.52      0.62      0.56       377\n",
      "         122       0.56      0.61      0.58        38\n",
      "         123       0.39      0.40      0.39        78\n",
      "         124       0.50      0.28      0.36        25\n",
      "         125       0.52      0.47      0.49        73\n",
      "         126       0.00      0.00      0.00        20\n",
      "         127       0.66      0.74      0.70       660\n",
      "         128       0.29      0.29      0.29        82\n",
      "         129       0.00      0.00      0.00        20\n",
      "         130       0.00      0.00      0.00        18\n",
      "         131       0.26      0.34      0.29       137\n",
      "         132       0.23      0.28      0.25        43\n",
      "         133       0.45      0.25      0.33        55\n",
      "         134       0.54      0.48      0.51        64\n",
      "         135       0.40      0.33      0.36        57\n",
      "         136       0.61      0.56      0.59        96\n",
      "         137       0.58      0.51      0.54        61\n",
      "         138       0.81      0.82      0.81      1969\n",
      "         139       0.60      0.48      0.54        54\n",
      "         140       0.00      0.00      0.00        16\n",
      "         141       0.32      0.32      0.32        25\n",
      "         142       0.00      0.00      0.00        17\n",
      "         143       0.24      0.39      0.30       298\n",
      "         144       0.14      0.12      0.13        41\n",
      "         145       0.68      0.51      0.58        83\n",
      "         146       0.00      0.00      0.00        25\n",
      "         147       0.20      0.12      0.15        25\n",
      "         148       0.39      0.55      0.46       209\n",
      "         149       0.00      0.00      0.00        15\n",
      "         150       0.57      0.43      0.49        61\n",
      "         151       0.00      0.00      0.00        20\n",
      "         152       0.86      0.30      0.44        20\n",
      "         153       0.33      0.29      0.31        34\n",
      "         154       0.66      0.73      0.69       144\n",
      "         155       0.58      0.63      0.60       387\n",
      "         156       0.42      0.32      0.36        56\n",
      "         157       0.56      0.21      0.30        24\n",
      "         158       0.75      0.41      0.53        51\n",
      "         159       0.67      0.43      0.53        23\n",
      "         160       0.29      0.25      0.27        44\n",
      "         161       0.51      0.59      0.55       240\n",
      "         162       0.71      0.37      0.49        27\n",
      "         163       0.62      0.42      0.50        19\n",
      "         164       0.48      0.41      0.44       161\n",
      "         165       0.55      0.45      0.50        66\n",
      "         166       0.44      0.44      0.44        70\n",
      "         167       0.78      0.70      0.74        61\n",
      "         168       0.44      0.39      0.41        92\n",
      "         169       0.65      0.76      0.70        34\n",
      "         170       0.42      0.35      0.38        37\n",
      "         171       0.41      0.40      0.40       223\n",
      "         172       0.15      0.08      0.11        24\n",
      "         173       0.67      0.78      0.72       248\n",
      "         174       0.56      0.50      0.53       191\n",
      "         175       0.22      0.21      0.21        53\n",
      "         176       0.50      0.61      0.55       236\n",
      "         177       0.41      0.31      0.35        39\n",
      "         178       0.69      0.66      0.67       201\n",
      "         179       0.56      0.29      0.38        31\n",
      "         180       0.68      0.60      0.64      1088\n",
      "         181       0.39      0.46      0.42       185\n",
      "         182       0.53      0.29      0.37        28\n",
      "         183       0.57      0.67      0.62       359\n",
      "         184       0.43      0.53      0.48        19\n",
      "         185       0.00      0.00      0.00        35\n",
      "         186       0.36      0.26      0.30        39\n",
      "         187       0.83      0.40      0.54        25\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     18549\n",
      "   macro avg       0.43      0.38      0.40     18549\n",
      "weighted avg       0.55      0.56      0.55     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ff_sum_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134451"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "vocab = Dictionary([doc.words for doc in (train_tagged_docs + test_tagged_docs)])\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48553\n"
     ]
    }
   ],
   "source": [
    "vocab.filter_extremes(no_below = 3, no_above = 0.9, keep_n = 50000)\n",
    "\n",
    "MAX_WORDS_NUM = len(vocab) + 1\n",
    "\n",
    "print(MAX_WORDS_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict([(i, token)for token, i in vocab.token2id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(words):\n",
    "    return [i + 1 for i in vocab.doc2idx(words)]\n",
    "\n",
    "def sequence_to_text(seq):\n",
    "    return [id2token[i - 1] for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43280"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences = [text_to_sequence(doc.words) for doc in train_tagged_docs]\n",
    "\n",
    "len(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 11, 20, 15, 2, 5, 16, 13, 10, 25, 22, 8, 1, 7, 14, 9, 3, 18, 24, 4, 12, 21, 17, 25, 19, 23]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['повідомляємо', 'протязі', 'тижня', 'прохання', 'мешканців', 'квартир', 'будинку', 'вул', 'жолудєва', 'поверх', 'замініти', 'лампочку', 'можливо', 'причина', 'світильники', 'загальному', 'коридорі', 'сотрудники', 'рєо', 'відмовляють', 'відсутністью', 'лампочек', 'просимо', 'допомогти', 'вирішенні', 'питання', 'освітлення', 'загального', 'коридору']\n"
     ]
    }
   ],
   "source": [
    "print(sequence_to_text(train_sequences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18549"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = [text_to_sequence(doc.words) for doc in test_tagged_docs]\n",
    "\n",
    "len(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = train_sequences + test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Length: 39.9\n",
      "Max Length: 235\n"
     ]
    }
   ],
   "source": [
    "seq_lens = [len(s) for s in sequences]\n",
    "print(\"Average Length: %0.1f\" % np.mean(seq_lens))\n",
    "print(\"Max Length: %d\" % max(seq_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(seq_lens, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43280, 235)\n",
      "(18549, 235)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQ_LEN = 235\n",
    "\n",
    "train_padded_seqs = pad_sequences(train_sequences, maxlen = MAX_SEQ_LEN)\n",
    "test_padded_seqs = pad_sequences(test_sequences, maxlen = MAX_SEQ_LEN)\n",
    "\n",
    "print(train_padded_seqs.shape)\n",
    "print(test_padded_seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Input, SpatialDropout1D, Dropout\n",
    "\n",
    "class LstmModel(NnModel):\n",
    "    def __init__(self, train_vectors, train_labels, test_vectors, test_labels,\n",
    "                 embedding_dim, memory_units):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_WORDS_NUM, embedding_dim, input_length = MAX_SEQ_LEN))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        #self.model.add(SpatialDropout1D(0.2))\n",
    "        self.model.add(LSTM(memory_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "        #self.model.add(LSTM(memory_units))\n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=\"adam\",\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/szubovych/.virtualenvs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 235, 100)          4855300   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 235, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 188)               18988     \n",
      "=================================================================\n",
      "Total params: 4,954,688\n",
      "Trainable params: 4,954,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = LstmModel(train_padded_seqs, train_topic_labels, test_padded_seqs, test_topic_labels, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13e960d5b634283a82bd5069602f449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=5, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740ff594e6c34fa6b985f94e898aa998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 2', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lstm_model.train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.62       335\n",
      "           1       0.30      0.68      0.41        41\n",
      "           2       0.62      0.57      0.59        79\n",
      "           3       0.26      0.23      0.25        39\n",
      "           4       0.57      0.18      0.28        22\n",
      "           5       0.63      0.74      0.68       106\n",
      "           6       0.20      0.15      0.17       142\n",
      "           7       0.59      0.35      0.44        62\n",
      "           8       0.50      0.23      0.32        30\n",
      "           9       0.12      0.27      0.17        22\n",
      "          10       0.76      0.84      0.80       144\n",
      "          11       0.25      0.03      0.06        29\n",
      "          12       1.00      0.18      0.31        22\n",
      "          13       0.43      0.13      0.20        45\n",
      "          14       0.15      0.11      0.12        84\n",
      "          15       0.33      0.06      0.11        16\n",
      "          16       0.68      0.66      0.67       122\n",
      "          17       0.57      0.25      0.35        16\n",
      "          18       0.36      0.26      0.30        53\n",
      "          19       0.40      0.11      0.17        19\n",
      "          20       0.00      0.00      0.00        16\n",
      "          21       0.00      0.00      0.00        24\n",
      "          22       0.52      0.68      0.59       194\n",
      "          23       0.83      0.76      0.80        51\n",
      "          24       0.20      0.40      0.27        47\n",
      "          25       0.00      0.00      0.00        28\n",
      "          26       0.22      0.08      0.12        25\n",
      "          27       0.74      0.85      0.79       943\n",
      "          28       0.54      0.55      0.55        56\n",
      "          29       0.22      0.48      0.30        63\n",
      "          30       0.20      0.14      0.16        44\n",
      "          31       1.00      0.04      0.08        25\n",
      "          32       0.71      0.72      0.71        95\n",
      "          33       0.77      0.77      0.77       129\n",
      "          34       0.53      0.40      0.46        45\n",
      "          35       0.00      0.00      0.00        35\n",
      "          36       0.40      0.38      0.39        50\n",
      "          37       0.33      0.10      0.15        40\n",
      "          38       0.49      0.54      0.51        39\n",
      "          39       0.42      0.43      0.43        44\n",
      "          40       0.54      0.70      0.61       115\n",
      "          41       0.52      0.29      0.37       143\n",
      "          42       0.67      0.60      0.63        62\n",
      "          43       0.50      0.49      0.49        41\n",
      "          44       0.43      0.21      0.28        29\n",
      "          45       0.51      0.34      0.41        71\n",
      "          46       0.56      0.45      0.50        56\n",
      "          47       0.00      0.00      0.00        21\n",
      "          48       0.00      0.00      0.00        16\n",
      "          49       0.42      0.26      0.32        19\n",
      "          50       0.58      0.34      0.43        44\n",
      "          51       0.00      0.00      0.00        20\n",
      "          52       0.30      0.11      0.16        27\n",
      "          53       0.56      0.53      0.54        53\n",
      "          54       0.00      0.00      0.00        18\n",
      "          55       0.00      0.00      0.00        26\n",
      "          56       0.56      0.32      0.41        28\n",
      "          57       0.69      0.61      0.65       108\n",
      "          58       0.73      0.81      0.77       366\n",
      "          59       0.35      0.38      0.37        74\n",
      "          60       0.00      0.00      0.00        34\n",
      "          61       0.00      0.00      0.00        24\n",
      "          62       0.44      0.40      0.42        95\n",
      "          63       0.00      0.00      0.00        18\n",
      "          64       0.71      0.68      0.69        37\n",
      "          65       0.11      0.07      0.08        15\n",
      "          66       0.50      0.09      0.15        33\n",
      "          67       0.63      0.76      0.69       602\n",
      "          68       0.58      0.86      0.69       190\n",
      "          69       0.27      0.50      0.35       105\n",
      "          70       0.45      0.42      0.43        36\n",
      "          71       0.37      0.23      0.28        48\n",
      "          72       0.00      0.00      0.00        22\n",
      "          73       0.56      0.27      0.37        33\n",
      "          74       0.88      0.82      0.85        85\n",
      "          75       0.61      0.37      0.46        30\n",
      "          76       0.87      0.40      0.55        50\n",
      "          77       0.57      0.30      0.39        43\n",
      "          78       0.44      0.56      0.49       104\n",
      "          79       0.52      0.47      0.50       391\n",
      "          80       0.40      0.12      0.18        17\n",
      "          81       0.52      0.39      0.45        28\n",
      "          82       0.00      0.00      0.00        20\n",
      "          83       0.00      0.00      0.00        24\n",
      "          84       0.18      0.13      0.15        53\n",
      "          85       0.43      0.39      0.41        23\n",
      "          86       0.41      0.28      0.34        74\n",
      "          87       0.56      0.15      0.23        34\n",
      "          88       0.58      0.59      0.58       297\n",
      "          89       0.38      0.60      0.47        40\n",
      "          90       0.00      0.00      0.00        16\n",
      "          91       0.33      0.05      0.09        39\n",
      "          92       0.50      0.08      0.13        26\n",
      "          93       0.41      0.55      0.47        22\n",
      "          94       0.20      0.05      0.08        20\n",
      "          95       0.17      0.17      0.17        42\n",
      "          96       0.50      0.38      0.43        21\n",
      "          97       0.32      0.25      0.28        24\n",
      "          98       0.40      0.08      0.13        25\n",
      "          99       0.47      0.26      0.34        34\n",
      "         100       0.40      0.38      0.39        55\n",
      "         101       0.51      0.74      0.60       329\n",
      "         102       0.83      0.77      0.80        75\n",
      "         103       0.24      0.32      0.27        19\n",
      "         104       0.51      0.55      0.53        38\n",
      "         105       0.31      0.41      0.35        87\n",
      "         106       0.10      0.02      0.04        47\n",
      "         107       0.11      0.09      0.10        32\n",
      "         108       0.58      0.33      0.42        21\n",
      "         109       0.37      0.51      0.43       155\n",
      "         110       0.21      0.23      0.22        66\n",
      "         111       0.40      0.20      0.27        20\n",
      "         112       0.43      0.35      0.39        80\n",
      "         113       0.58      0.55      0.57        56\n",
      "         114       0.45      0.55      0.50       137\n",
      "         115       0.00      0.00      0.00        17\n",
      "         116       0.65      0.79      0.71       129\n",
      "         117       0.12      0.03      0.04        78\n",
      "         118       0.31      0.13      0.18        39\n",
      "         119       0.46      0.26      0.33        47\n",
      "         120       1.00      0.07      0.13        14\n",
      "         121       0.59      0.58      0.58       377\n",
      "         122       0.53      0.47      0.50        38\n",
      "         123       0.39      0.47      0.43        78\n",
      "         124       0.60      0.12      0.20        25\n",
      "         125       0.30      0.27      0.29        73\n",
      "         126       0.38      0.15      0.21        20\n",
      "         127       0.74      0.90      0.81       660\n",
      "         128       0.21      0.18      0.20        82\n",
      "         129       0.33      0.05      0.09        20\n",
      "         130       0.44      0.22      0.30        18\n",
      "         131       0.28      0.25      0.26       137\n",
      "         132       0.25      0.02      0.04        43\n",
      "         133       0.30      0.31      0.31        55\n",
      "         134       0.35      0.23      0.28        64\n",
      "         135       0.15      0.09      0.11        57\n",
      "         136       0.51      0.75      0.61        96\n",
      "         137       0.50      0.74      0.60        61\n",
      "         138       0.87      0.90      0.88      1969\n",
      "         139       0.53      0.61      0.57        54\n",
      "         140       0.00      0.00      0.00        16\n",
      "         141       0.83      0.20      0.32        25\n",
      "         142       0.00      0.00      0.00        17\n",
      "         143       0.28      0.41      0.33       298\n",
      "         144       0.00      0.00      0.00        41\n",
      "         145       0.77      0.61      0.68        83\n",
      "         146       0.33      0.04      0.07        25\n",
      "         147       0.17      0.04      0.06        25\n",
      "         148       0.47      0.65      0.55       209\n",
      "         149       0.33      0.07      0.11        15\n",
      "         150       0.50      0.11      0.19        61\n",
      "         151       0.83      0.25      0.38        20\n",
      "         152       0.67      0.10      0.17        20\n",
      "         153       0.38      0.29      0.33        34\n",
      "         154       0.85      0.78      0.82       144\n",
      "         155       0.58      0.60      0.59       387\n",
      "         156       0.23      0.25      0.24        56\n",
      "         157       0.50      0.17      0.25        24\n",
      "         158       0.33      0.39      0.36        51\n",
      "         159       0.40      0.26      0.32        23\n",
      "         160       0.26      0.20      0.23        44\n",
      "         161       0.70      0.72      0.71       240\n",
      "         162       0.33      0.22      0.27        27\n",
      "         163       0.40      0.11      0.17        19\n",
      "         164       0.41      0.34      0.37       161\n",
      "         165       0.55      0.62      0.59        66\n",
      "         166       0.32      0.61      0.42        70\n",
      "         167       0.75      0.75      0.75        61\n",
      "         168       0.14      0.16      0.15        92\n",
      "         169       0.72      0.62      0.67        34\n",
      "         170       0.37      0.19      0.25        37\n",
      "         171       0.39      0.48      0.43       223\n",
      "         172       0.00      0.00      0.00        24\n",
      "         173       0.78      0.83      0.80       248\n",
      "         174       0.60      0.57      0.58       191\n",
      "         175       0.00      0.00      0.00        53\n",
      "         176       0.73      0.69      0.71       236\n",
      "         177       0.25      0.03      0.05        39\n",
      "         178       0.70      0.81      0.75       201\n",
      "         179       0.38      0.16      0.23        31\n",
      "         180       0.69      0.78      0.73      1088\n",
      "         181       0.47      0.54      0.50       185\n",
      "         182       0.27      0.11      0.15        28\n",
      "         183       0.66      0.72      0.69       359\n",
      "         184       1.00      0.16      0.27        19\n",
      "         185       0.09      0.03      0.04        35\n",
      "         186       0.48      0.28      0.35        39\n",
      "         187       0.44      0.28      0.34        25\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     18549\n",
      "   macro avg       0.42      0.34      0.35     18549\n",
      "weighted avg       0.57      0.59      0.57     18549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "class CnnModel(NnModel):\n",
    "     def __init__(self, train_vectors, train_labels, test_vectors, test_labels, \n",
    "                  embedding_dim, channels_num, conv_window):\n",
    "        super().__init__(train_vectors, \n",
    "                         train_labels, \n",
    "                         test_vectors, \n",
    "                         test_labels)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_WORDS_NUM, embedding_dim, input_length = MAX_SEQ_LEN))\n",
    "        self.model.add(Conv1D(channels_num, conv_window, activation='relu'))\n",
    "        self.model.add(MaxPooling1D(conv_window))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(188, activation='softmax'))\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['acc'])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 235, 100)          4855300   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 231, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5888)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 188)               1107132   \n",
      "=================================================================\n",
      "Total params: 6,026,560\n",
      "Trainable params: 6,026,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CnnModel(train_padded_seqs, train_topic_labels, test_padded_seqs, test_topic_labels, 100, 128, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72448c495b644e158600c707370977c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=10, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d313dafa3141fa868c567a5cebad0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=43280, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InternalError",
     "evalue": "cudnn PoolBackward launch failed\n\t [[{{node training_18/RMSprop/gradients/max_pooling1d_2/MaxPool_grad/MaxPoolGrad}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-302-7e9f8eeedc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-211-9eb03992550f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         self.model.fit(self.train_vectors, self.train_labels, \n\u001b[1;32m     16\u001b[0m                        \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                        verbose=0, callbacks=[TQDMNotebookCallback()])\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m             self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m   1438\u001b[0m                 session._session, options_ptr, status)\n\u001b[0;32m-> 1439\u001b[0;31m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m             self._handle = tf_session.TF_DeprecatedSessionMakeCallable(\n\u001b[1;32m   1441\u001b[0m                 session._session, options_ptr, status)\n",
      "\u001b[0;32m~/.virtualenvs/nlp/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: cudnn PoolBackward launch failed\n\t [[{{node training_18/RMSprop/gradients/max_pooling1d_2/MaxPool_grad/MaxPoolGrad}}]]"
     ]
    }
   ],
   "source": [
    "cnn_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_topic_labels),\n",
    "                                                 train_topic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87e5e8698634b98855fd28543544242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=111897), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "added 62930 words in the embedding matrix\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAX_WORDS_NUM = len(vocab) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 211, 64)           7164544   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 205, 32)           14368     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 41, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 35, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 188)               6204      \n",
      "=================================================================\n",
      "Total params: 7,192,316\n",
      "Trainable params: 7,192,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "#model.add(pretrained_embedding_layer)\n",
    "model.add(layers.Embedding(MAX_NB_WORDS, 64, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(Dense(188, activation='softmax'))\n",
    "#model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "nb_words_in_matrix = 0\n",
    "nb_words = MAX_NB_WORDS #min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in tqdm_notebook(vocab.token2id.items()):\n",
    "    #if i >= MAX_NB_WORDS:\n",
    "    #    continue\n",
    "    embedding_vector = None\n",
    "    try:\n",
    "        embedding_vector = wv[word]\n",
    "    except KeyError as e:\n",
    "        pass\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        nb_words_in_matrix = nb_words_in_matrix + 1\n",
    "        \n",
    "print(\"added %d words in the embedding matrix\" % nb_words_in_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
